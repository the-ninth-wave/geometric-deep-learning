{
  
    
        "post0": {
            "title": "notebook two",
            "content": "implementing basic GDL models . pip install torch --quiet --upgrade . |██████████████████████████████▎ | 834.1 MB 1.2 MB/s eta 0:00:40tcmalloc: large alloc 1147494400 bytes == 0x558e8f6fc000 @ 0x7f7ca0e76615 0x558e2a1404cc 0x558e2a22047a 0x558e2a1432ed 0x558e2a234e1d 0x558e2a1b6e99 0x558e2a1b19ee 0x558e2a144bda 0x558e2a1b6d00 0x558e2a1b19ee 0x558e2a144bda 0x558e2a1b3737 0x558e2a235c66 0x558e2a1b2daf 0x558e2a235c66 0x558e2a1b2daf 0x558e2a235c66 0x558e2a1b2daf 0x558e2a145039 0x558e2a188409 0x558e2a143c52 0x558e2a1b6c25 0x558e2a1b19ee 0x558e2a144bda 0x558e2a1b3737 0x558e2a1b19ee 0x558e2a144bda 0x558e2a1b2915 0x558e2a144afa 0x558e2a1b2c0d 0x558e2a1b19ee |████████████████████████████████| 881.9 MB 15 kB/s ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.10.1 which is incompatible. torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.10.1 which is incompatible. torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.10.1 which is incompatible. . imports . import matplotlib as mpl from matplotlib import pyplot as plt import numpy as np import torch import torchvision import torch.nn as nn import torch.utils.data as data from PIL import Image from torchvision import transforms . . #torch.manual_seed(42) print(&quot;Using torch&quot;, torch.__version__) . . Using torch 1.10.1+cu102 . helper classes and functions . !pip install --quiet tabletext . Building wheel for tabletext (setup.py) ... done . from tabletext import to_text . . info_1d . args . tens . | name . | . def info_1d(tens, name): # tensor info starts delim = &quot; &quot; mean = np.mean( np.array( tens ) ) std = np.std( np.array( tens ) ) display_tens = tens[None,:] print( &quot;tensor&quot;, delim, name) print(&quot;num. dims &quot;, delim, &quot; &quot;, tens.ndim, delim, delim, &quot;num. entries &quot;, delim, &quot; &quot;, np.array( tens ).size, delim, delim, &quot;shape &quot;, delim, &quot; &quot;, np.array( tens.size() ) ) print(&quot;mean &quot;, delim, &quot; &quot;, mean, &quot; n&quot;, &quot;std &quot;, delim,&quot; &quot;, std ) print(&quot; n&quot;) display_list = display_tens.tolist() J = len( display_list ) K = len( display_list[0]) outer_list = [] for j in range(J): inner_list = [] for k in range(K): inner_list += [ str( display_list[j][k] )[:4] ] outer_list += [ inner_list ] print( to_text( outer_list ) ) print(&quot; n n&quot;) . . . . info_2d . args . tens . | name . | . def info_2d(tens, name): # tensor info starts delim = &quot; &quot; mean = np.mean( np.array( tens ) ) std = np.std( np.array( tens ) ) display_tens = tens print( &quot;tensor&quot;, delim, name, &quot; n&quot; ) print( delim, &quot;num. dims &quot;, delim, &quot; &quot;, tens.ndim ) print( delim, &quot;num. entries &quot;, delim, &quot; &quot;, np.array( tens ).size ) print( delim, &quot;shape &quot;, delim, &quot; &quot;, np.array( tens.size() ) ) print(&quot; n&quot;) print( delim, &quot;mean &quot;, delim, &quot; &quot;, mean ) print( delim, &quot;std &quot;, delim, &quot; &quot;, std ) print(&quot; n&quot;) display_list = display_tens.tolist() J = len( display_list ) K = len( display_list[0]) outer_list = [] for j in range(J): inner_list = [] for k in range(K): inner_list += [ str( display_list[j][k] )[:4] ] outer_list += [ inner_list ] print( to_text( outer_list ) ) print(&quot; n n&quot;) . . . . mesh_helper . def mesh_helper(arr_out, mesh_size): M = mesh_size x = np.arange(0, len( arr_out ), 1) x_mesh = np.arange(0, len( arr_out), 1 / M) def y_custom(x_0): return arr_out[ np.int( np.floor( x_0 ) ) ] y_v = np.vectorize( y_custom ) return y_v( x_mesh ) . . . . height_helper . def height_helper(arr_out, mesh_size = 100): M = mesh_size L = len( arr_out ) x_list = [] y_list = [] for j in range(L): if j == 0 and arr_out[0] &gt; 0: a = 0 b = arr_out[0] elif j == 0 and arr_out[0] &lt;= 0: a = arr_out[j] b = 0 elif arr_out[j-1] &lt; arr_out[j]: a = arr_out[j-1] b = arr_out[j] elif arr_out[j-1] &gt;= arr_out[j]: a = arr_out[j] b = arr_out[j-1] y_temp_list = np.arange( a, b, 1 / M ) for i in range( len(y_temp_list) ): x_list += [ j ] y_list += [ y_temp_list[i] ] if arr_out[-1] &gt; 0: a = 0 b = arr_out[-1] else: a = arr_out[-1] b = 0 y_temp_list = np.arange( a, b, 1 / M ) for i in range( len(y_temp_list) ): x_list += [ L ] y_list += [ y_temp_list[i] ] return list(zip(x_list, y_list)) . . . . viz_1d . args . tens . | display_size = 8 . | . def viz_1d( tens, colorbar = True ): # convert to numpy arr = np.array( tens ) # boost dimension display_arr = np.array( tens[None,:] ) # array statistics mean = np.mean( arr ) std = np.std( arr ) L = len( arr ) # normalize cmap = mpl.cm.cool normalize = mpl.colors.Normalize(vmin = np.min( arr ) - std, vmax = np.max( arr ) + std) # mesh size mesh_size = 100 x_mesh = np.arange( 0, L, 1 / mesh_size ) y_mesh = mesh_helper( arr, mesh_size ) area = 1 plt.scatter(x_mesh, y_mesh, s = area, c = y_mesh, marker = &quot;x&quot;, cmap = cmap, norm = normalize, alpha = 0.8) zipped = height_helper( arr ) x_list = [ zipped[j][0] for j in range( len(zipped) ) ] y_list = [ zipped[j][1] for j in range( len(zipped) ) ] plt.scatter(x_list, y_list, s = area, c = y_list, marker = &quot;o&quot;, cmap = cmap, norm = normalize, alpha = 0.8) if colorbar == True: plt.colorbar() . . . viz_list_of_1d . def viz_list_of_1d( tens_list, colorbar = True ): # to list of numpy arrays arr_list = [ np.array( tens ) for tens in tens_list ] num_tens = len( arr_list ) # statistics mean_list = [ np.mean( arr ) for arr in arr_list ] std_list = [ np.std( arr ) for arr in arr_list ] ave_std = np.mean( np.array(std_list) ) max_list = [ np.max( arr ) for arr in arr_list ] min_list = [ np.min( arr ) for arr in arr_list ] mx = np.max( max_list ) mn = np.min( min_list ) L = len( arr_list[0] ) # normalize cmap = mpl.cm.cool normalize = mpl.colors.Normalize(vmin = mn - ave_std, vmax = mx + ave_std) # mesh size mesh_size = 100 # setting number and shape of subplots num_tens_sqrt = np.int( np.ceil( np.sqrt(num_tens) ) ) fig, axs = plt.subplots(num_tens_sqrt, num_tens_sqrt, figsize = (10,5)) x_mesh = np.arange( 0, L, 1 / mesh_size ) area = .5 for j in range(0, num_tens): y_mesh = mesh_helper( arr_list[j], mesh_size ) axs.flatten()[j].scatter(x_mesh, y_mesh, s = area, c = y_mesh, cmap = cmap, norm = normalize, alpha = 0.5) zipped = height_helper( np.array(tens_list[j]) ) x_list = [ zipped[j][0] for j in range( len(zipped) ) ] y_list = [ zipped[j][1] for j in range( len(zipped) ) ] axs.flatten()[j].scatter(x_list, y_list, s = area, c = y_list, marker = &quot;o&quot;, cmap = cmap, norm = normalize, alpha = 0.8) axs.flatten()[j].xlim = (-1, L) axs.flatten()[j].ylim = (mn-1, mx+1) # spacing fig.subplots_adjust( wspace = .4, hspace = .4 ) # colorbar cax = fig.add_axes([0.95, 0.2, 0.02, 0.6]) fig.colorbar(mpl.cm.ScalarMappable(norm = normalize, cmap = cmap), cax = cax, orientation = &#39;vertical&#39;) . . . initializing tensors in PyTorch . resources . lightning docs | . $ quad$ PyTorch&#39;s tensor objects are similar to NumPy&#39;s ndarrays, except that tensors can run on GPUs or other hardware accelerators. Tensors and NumPy arrays can often share the same underlying memory, eliminating the need to copy data. Tensors are also optimized for automatic differentiation. Here are some ways they can be initialized in PyTorch. . From lists, $ quad$ X_1 = torch.tensor( list_object ) . | From a numpy array, $ quad$ X_2 = torch.from_numpy( array_object ) . | With random or constant values, of a given shape. For example, . a. Entries all ones, $ quad$ X_3_a = torch.ones( shape ) . b. Entries all zeros, $ quad$ X_3_b = torch.zeros( shape ) . c. Entries are i.i.d. $ text{Unif}(0,1)$, $ quad$ X_3_c = torch.rand( shape ) . d. Entries are i.i.d. standard normal, $ quad$ X_3_d = torch.randn( shape ) . e. From values stored in memory, $ quad$ X_3_e = torch.Tensor( shape ) . f. As a list of consecutive integers between $N$ and $M$ inclusive, $ quad$ X_3_f = torch.arange(N,M) . | . . Example. ( from lists ) . X_1 = torch.tensor( [ 2, 3, 5, 7 ] ) info_1d( X_1, &quot;X_1&quot; ) #viz_1d( X_1 ) . . tensor X_1 num. dims 1 num. entries 4 shape [4] mean 4.25 std 1.920286436967152 ┌───┬───┬───┬───┐ │ 2 │ 3 │ 5 │ 7 │ └───┴───┴───┴───┘ . . . . . Example. ( from numpy ) . X_2 = torch.from_numpy( np.array( [ 2, 3, 4 ] ) ) info_1d(X_2, &quot;X_2&quot;) #viz_1d(X_2) . . tensor X_2 num. dims 1 num. entries 3 shape [3] mean 3.0 std 0.816496580927726 ┌───┬───┬───┐ │ 2 │ 3 │ 4 │ └───┴───┴───┘ . . . $ quad$ The next examples allow a shape to be provided as an argument. . shape_dict = {} shape_dict[&quot;i&quot;] = ( 10 ) # shape_dict[&quot;ii&quot;] = ( 2, 3 ) # shape_dict[&quot;iii&quot;] = ( 2, 3, ) . . . . Example. ( ones or zeros of given shape ) . ones . for key in shape_dict: s = key X_3_a = torch.ones(shape_dict[s]) info_1d(X_3_a, &quot;X_3 (a)&quot; + &quot; &quot; + s ) #viz_1d( X_3_a ) . . tensor X_3 (a) i num. dims 1 num. entries 10 shape [10] mean 1.0 std 0.0 ┌─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┐ │ 1.0 │ 1.0 │ 1.0 │ 1.0 │ 1.0 │ 1.0 │ 1.0 │ 1.0 │ 1.0 │ 1.0 │ └─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┘ . zeros . for key in shape_dict: s = key X_3_b = torch.zeros(shape_dict[s]) info_1d(X_3_b, &quot;X_3 (b)&quot; + &quot; &quot; + s ) #viz_1d( X_3_b ) . . tensor X_3 (b) i num. dims 1 num. entries 10 shape [10] mean 0.0 std 0.0 ┌─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┐ │ 0.0 │ 0.0 │ 0.0 │ 0.0 │ 0.0 │ 0.0 │ 0.0 │ 0.0 │ 0.0 │ 0.0 │ └─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┘ . . . . . Example. ( i.i.d. of given shape ) . $ textrm{Unif}([0,1])$ . for key in shape_dict: s = key X_3_c = torch.rand(shape_dict[s]) info_1d(X_3_c, &quot;X_3 (c) &quot; + &quot; &quot; + s ) #viz_1d( X_3_c ) . . tensor X_3 (c) i num. dims 1 num. entries 10 shape [10] mean 0.55636114 std 0.29058367 ┌──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┐ │ 0.37 │ 0.74 │ 0.86 │ 0.57 │ 0.34 │ 0.07 │ 0.13 │ 0.79 │ 0.72 │ 0.93 │ └──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┘ . standard normal . for key in shape_dict: s = key X_3_d = torch.randn(shape_dict[s]) info_1d(X_3_d, &quot;X 3 (d)&quot; + &quot; &quot; + s ) #viz_1d( X_3_d ) . . tensor X 3 (d) i num. dims 1 num. entries 10 shape [10] mean -0.23619552 std 1.2433954 ┌──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┐ │ -0.0 │ 1.01 │ -0.6 │ 1.31 │ 1.37 │ -0.7 │ -2.7 │ -1.1 │ -1.0 │ 0.39 │ └──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┘ . . . . . Example. ( list of consecutive integers ) . X_3_f = torch.arange(10,20) info_1d(X_3_f, &quot;X 3 (f)&quot;) #viz_1d( X_3_f ) . . tensor X 3 (f) num. dims 1 num. entries 10 shape [10] mean 14.5 std 2.8722813232690143 ┌────┬────┬────┬────┬────┬────┬────┬────┬────┬────┐ │ 10 │ 11 │ 12 │ 13 │ 14 │ 15 │ 16 │ 17 │ 18 │ 19 │ └────┴────┴────┴────┴────┴────┴────┴────┴────┴────┘ . . . tensor operations . $ quad$ We have seen above how to initialize a tensor from a numpy array. The next example illustrates how to go the other direction, and exhibits some memory properties. . . . Example. ( numpy objects and tensors ) . Tensors can be converted to numpy arrays, and numpy arrays back to tensors. To transform a numpy array into a tensor, we can use the function torch.from_numpy, and we use np.array for the other direction. . The conversion of tensors to numpy require the tensor to be on the CPU, and not the GPU. . In case you have a tensor on GPU, you need to call .cpu() on the tensor beforehand. Hence, you get a line like np_arr = tensor.cpu().numpy(). . Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other. . t = torch.ones(5) print(f&quot;t: {t}&quot;) n = t.numpy() print(f&quot;n: {n}&quot;) . . t: tensor([1., 1., 1., 1., 1.]) n: [1. 1. 1. 1. 1.] . t.add_(1) print(f&quot;t: {t}&quot;) print(f&quot;n: {n}&quot;) . . t: tensor([2., 2., 2., 2., 2.]) n: [2. 2. 2. 2. 2.] . . . $ quad$ Most operations existing in numpy also exist in PyTorch. A full list of operations can be found in the PyTorch documentation. Each Torch operation can be run on the GPU. By default, tensors are created on the CPU. Unless we are using a package like pytorch-lightning, we need to explicitly move tensors to the GPU using the .to method, after checking GPU availability. I&#39;ve read that copying large tensors across devices can be expensive in terms of time and memory. Now we go through some operations common between tensors and numpy arrays. . . . Example. ( adding tensors ) . X_1, X_2 = torch.rand(2,3), torch.rand(2,3) info_2d(X_1,&quot;X_1&quot;) info_2d(X_2,&quot;X_2&quot;) . . tensor X_1 num. dims 2 num. entries 6 shape [2 3] mean 0.46360818 std 0.21094887 ┌──────┬──────┬──────┐ │ 0.50 │ 0.10 │ 0.59 │ ├──────┼──────┼──────┤ │ 0.77 │ 0.47 │ 0.32 │ └──────┴──────┴──────┘ tensor X_2 num. dims 2 num. entries 6 shape [2 3] mean 0.5031305 std 0.27562448 ┌──────┬──────┬──────┐ │ 0.90 │ 0.15 │ 0.14 │ ├──────┼──────┼──────┤ │ 0.66 │ 0.61 │ 0.53 │ └──────┴──────┴──────┘ . Y = X_1 + X_2 info_2d(Y, &quot;Y = X_1 + X_2&quot;) . . tensor Y = X_1 + X_2 num. dims 2 num. entries 6 shape [2 3] mean 0.96673864 std 0.4079924 ┌──────┬──────┬──────┐ │ 1.41 │ 0.25 │ 0.73 │ ├──────┼──────┼──────┤ │ 1.43 │ 1.09 │ 0.86 │ └──────┴──────┴──────┘ . . . . . Example. ( stacking tensors ) . X_1, X_2 = torch.arange(5,10), torch.arange(10,15) info_1d(X_1, &quot;X_1&quot;) info_1d(X_2, &quot;X_2&quot;) . . tensor X_1 num. dims 1 num. entries 5 shape [5] mean 7.0 std 1.4142135623730951 ┌───┬───┬───┬───┬───┐ │ 5 │ 6 │ 7 │ 8 │ 9 │ └───┴───┴───┴───┴───┘ tensor X_2 num. dims 1 num. entries 5 shape [5] mean 12.0 std 1.4142135623730951 ┌────┬────┬────┬────┬────┐ │ 10 │ 11 │ 12 │ 13 │ 14 │ └────┴────┴────┴────┴────┘ . Y = torch.stack((X_1, X_2), dim = 0) info_2d(Y, &quot;Y = torch.stack([X_1, X_2], dim = 0)&quot;) . . tensor Y = torch.stack([X_1, X_2], dim = 0) num. dims 2 num. entries 10 shape [2 5] mean 9.5 std 2.8722813232690143 ┌────┬────┬────┬────┬────┐ │ 5 │ 6 │ 7 │ 8 │ 9 │ ├────┼────┼────┼────┼────┤ │ 10 │ 11 │ 12 │ 13 │ 14 │ └────┴────┴────┴────┴────┘ . . . $ quad$ Operations that store the result into the operand are called in-place. They are usually marked with a underscore postfix, e.g. &quot;add_&quot; instead of &quot;add&quot;. Thus, the operation X.copy_(Y) will change X. We give another example below. . . . Example. ( in-place operations ) . $ quad$ Given PyTorch tensors X_1 and X_2, calling X_1 + X_2 creates a new tensor containing the sum of the two inputs. One can also use in-place operations that are applied directly on the memory of a tensor. . before . X_1, X_2 = torch.rand(2, 3), torch.rand(2, 3) info_2d(X_1,&quot;X_1&quot;) info_2d(X_2,&quot;X_2 (old)&quot;) . . tensor X_1 num. dims 2 num. entries 6 shape [2 3] mean 0.4304365 std 0.3225695 ┌──────┬──────┬──────┐ │ 0.23 │ 0.06 │ 0.04 │ ├──────┼──────┼──────┤ │ 0.70 │ 0.76 │ 0.76 │ └──────┴──────┴──────┘ tensor X_2 (old) num. dims 2 num. entries 6 shape [2 3] mean 0.30377033 std 0.20098853 ┌──────┬──────┬──────┐ │ 0.70 │ 0.07 │ 0.35 │ ├──────┼──────┼──────┤ │ 0.31 │ 0.16 │ 0.20 │ └──────┴──────┴──────┘ . after calling X_2.add_(X_1) . X_2.add_(X_1) info_2d(X_1,&quot;X_1 (same as before)&quot;) info_2d(X_2,&quot;X_2 (new)&quot;) . . tensor X_1 (same as before) num. dims 2 num. entries 6 shape [2 3] mean 0.4304365 std 0.3225695 ┌──────┬──────┬──────┐ │ 0.23 │ 0.06 │ 0.04 │ ├──────┼──────┼──────┤ │ 0.70 │ 0.76 │ 0.76 │ └──────┴──────┴──────┘ tensor X_2 (new) num. dims 2 num. entries 6 shape [2 3] mean 0.73420686 std 0.33801785 ┌──────┬──────┬──────┐ │ 0.93 │ 0.13 │ 0.40 │ ├──────┼──────┼──────┤ │ 1.02 │ 0.93 │ 0.96 │ └──────┴──────┴──────┘ . Apparently, in-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss of history. This is a reason their use is discouraged. . . . $ quad$ Another common operation changes the shape of a tensor. A tensor of size (2,3) can be re-organized to any other shape with the same number of elements (e.g. a tensor of size (6), or (3,2), ...). In PyTorch, this reshaping operation is called view. . . . Example. ( reshaping tensors ) . X = torch.arange(6) info_1d(X,&quot;X&quot;) . . tensor X num. dims 1 num. entries 6 shape [6] mean 2.5 std 1.707825127659933 ┌───┬───┬───┬───┬───┬───┐ │ 0 │ 1 │ 2 │ 3 │ 4 │ 5 │ └───┴───┴───┴───┴───┴───┘ . X = X.view(2, 3) info_2d(X, &quot;X = X.view(2, 3)&quot;) . . tensor X = X.view(2, 3) num. dims 2 num. entries 6 shape [2 3] mean 2.5 std 1.707825127659933 ┌───┬───┬───┐ │ 0 │ 1 │ 2 │ ├───┼───┼───┤ │ 3 │ 4 │ 5 │ └───┴───┴───┘ . . . $ quad$ We can also permute the dimensions of a tensor. For tensors which are matrices, this operation coincides with the matrix transpose. . . . Example. ( transpose ) . before . info_2d(X,&quot;X&quot;) . . tensor X num. dims 2 num. entries 6 shape [2 3] mean 2.5 std 1.707825127659933 ┌───┬───┬───┐ │ 0 │ 1 │ 2 │ ├───┼───┼───┤ │ 3 │ 4 │ 5 │ └───┴───┴───┘ . after . X = X.permute(1,0) info_2d(X, &quot;X = X.permute(1,0)&quot;) . . tensor X = X.permute(1,0) num. dims 2 num. entries 6 shape [3 2] mean 2.5 std 1.707825127659933 ┌───┬───┐ │ 0 │ 3 │ ├───┼───┤ │ 1 │ 4 │ ├───┼───┤ │ 2 │ 5 │ └───┴───┘ . In the case that X is a matrix, one can call X.T to perform an equivalent operation. . X = X.T info_2d(X,&quot;X = X.T&quot;) . . tensor X = X.T num. dims 2 num. entries 6 shape [2 3] mean 2.5 std 1.707825127659933 ┌───┬───┬───┐ │ 0 │ 1 │ 2 │ ├───┼───┼───┤ │ 3 │ 4 │ 5 │ └───┴───┴───┘ . . . $ quad$ Torch also supports the same indexing and slicing as numpy. . . . Example. ( numpy-like indexing and slicing ) . $ quad$ We start by initializing a random $4 times 4$ matrix, which we call X. . X = torch.rand( 4,4 ) info_2d(X,&quot;X&quot;) . . tensor X num. dims 2 num. entries 16 shape [4 4] mean 0.6176396 std 0.2650224 ┌──────┬──────┬──────┬──────┐ │ 0.55 │ 0.65 │ 0.66 │ 0.32 │ ├──────┼──────┼──────┼──────┤ │ 0.52 │ 0.10 │ 0.38 │ 0.95 │ ├──────┼──────┼──────┼──────┤ │ 0.85 │ 0.76 │ 0.61 │ 0.15 │ ├──────┼──────┼──────┼──────┤ │ 0.98 │ 0.96 │ 0.54 │ 0.81 │ └──────┴──────┴──────┴──────┘ . The first row of X is given by X[0] . tens = X[0] info_1d(tens,&quot;X[0]&quot;) . . tensor X[0] num. dims 1 num. entries 4 shape [4] mean 0.5516057 std 0.1402314 ┌──────┬──────┬──────┬──────┐ │ 0.55 │ 0.65 │ 0.66 │ 0.32 │ └──────┴──────┴──────┴──────┘ . The first column of X is given by X[:,0] . tens = X[:,0] tens = tens[None,:] tens = tens.T info_2d(tens,&quot;X[:,0]&quot;) . . tensor X[:,0] num. dims 2 num. entries 4 shape [4 1] mean 0.7297974 std 0.19340122 ┌──────┐ │ 0.55 │ ├──────┤ │ 0.52 │ ├──────┤ │ 0.85 │ ├──────┤ │ 0.98 │ └──────┘ . The last column of X is given by X[...,-1] . tens = X[...,-1] tens = tens[None,:] tens = tens.T info_2d(tens,&quot;X[...,-1]&quot;) . . tensor X[...,-1] num. dims 2 num. entries 4 shape [4 1] mean 0.5609479 std 0.33263853 ┌──────┐ │ 0.32 │ ├──────┤ │ 0.95 │ ├──────┤ │ 0.15 │ ├──────┤ │ 0.81 │ └──────┘ . The first two rows of X, restricted to the last column, is given by X[:2,-1] . tens = X[:2,-1] tens = tens[None,:] tens = tens.T info_2d(tens,&quot;X[:2,-1]&quot;) . . tensor X[:2,-1] num. dims 2 num. entries 2 shape [2 1] mean 0.6380327 std 0.31738755 ┌──────┐ │ 0.32 │ ├──────┤ │ 0.95 │ └──────┘ . The middle two rows of X are given by X[1:3,:] . tens = X[1:3,:] info_2d(tens,&quot;X[1:3,:]&quot;) . . tensor X[1:3,:] num. dims 2 num. entries 8 shape [2 4] mean 0.5452546 std 0.29302517 ┌──────┬──────┬──────┬──────┐ │ 0.52 │ 0.10 │ 0.38 │ 0.95 │ ├──────┼──────┼──────┼──────┤ │ 0.85 │ 0.76 │ 0.61 │ 0.15 │ └──────┴──────┴──────┴──────┘ . $ quad$ The above slicing can be used to set a collection of entries of X to a certain value. Below, we modify X by calling X[:,1] = 0. This sets all entries in the second column of X to zero. . X[:,1] = 0 info_2d(X,&quot;after X[:,1] = 0&quot;) . . tensor after X[:,1] = 0 num. dims 2 num. entries 16 shape [4 4] mean 0.46152157 std 0.34013218 ┌──────┬─────┬──────┬──────┐ │ 0.55 │ 0.0 │ 0.66 │ 0.32 │ ├──────┼─────┼──────┼──────┤ │ 0.52 │ 0.0 │ 0.38 │ 0.95 │ ├──────┼─────┼──────┼──────┤ │ 0.85 │ 0.0 │ 0.61 │ 0.15 │ ├──────┼─────┼──────┼──────┤ │ 0.98 │ 0.0 │ 0.54 │ 0.81 │ └──────┴─────┴──────┴──────┘ . . . ... other operations . Here are some ways to perform matrix multiplication: . torch.matmul $ quad$ Performs the matrix product over two tensors, where the specific behavior depends on the dimensions. If both inputs are matrices (2-dimensional tensors), it performs the standard matrix product. For higher dimensional inputs, the function supports broadcasting (for details see the documentation). . It can also be written as a @ b, similar to numpy. . | torch.mm $ quad$ Performs the matrix product over two matrices, but doesn&#39;t support broadcasting (see documentation) . | torch.bmm $ quad$ Performs the matrix product with a support batch dimension. Let T be a tensor of shape (b, n, m), and R a tensor of shape (b, m, p), the output tensor is of shape (b, n , p), obtained by &quot;entry-wise&quot; matrix multiplication along the batch dimension. . | torch.einsum $ quad$ Performs matrix multiplications and more (i.e. sums of products) using the Einstein summation convention. . | . Usually, we use torch.matmul or torch.bmm. . X, Y = torch.arange(6).view(2, 3), torch.arange(9).view(3, 3) print(X, &quot; n&quot;, Y) . . tensor([[0, 1, 2], [3, 4, 5]]) tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]]) . Z = torch.matmul(X,Y) print(Z) . . tensor([[15, 18, 21], [42, 54, 66]]) . Given a tensor X, the tensors Y_1,Y_2, Y_3 computed below all have the same value: . Y_1 = X @ X.T Y_2 = X.matmul(X.T) Y_3 = torch.rand_like(X) torch.matmul(X, X.T, out = Y_3) . On the other hand, * denotes the entrywise product of two tensors. . X = torch.arange(6).view(2, 3) print(X) . . tensor([[0, 1, 2], [3, 4, 5]]) . print(X * X, &quot; n n&quot;, X.mul(X)) . . tensor([[ 0, 1, 4], [ 9, 16, 25]]) tensor([[ 0, 1, 4], [ 9, 16, 25]]) . You can use torch.cat to concatenate a sequence of tensors along a given dimension. See also torch.stack, another tensor joining op that is subtly different from torch.cat . Y = torch.cat( [X, X, X], dim =1) print(Y) . . tensor([[0, 1, 2, 0, 1, 2, 0, 1, 2], [3, 4, 5, 3, 4, 5, 3, 4, 5]]) . If you have a one-element tensor, for example obtained by aggregating all values of a given tensor into a single value, you can convert it to a Python numerical value using item(): . agg = X.sum() agg_item = agg.item() print(agg_item, &quot; t&quot;, type(agg_item)) . . 15 &lt;class &#39;int&#39;&gt; . 2.2 ... one-dimensional convolutions of scalar-valued signals . We now consider one of the simplest possible settings for learning, in which the underlying domain is a one-dimensional grid, and where signals over the domain have only a single channel. . In this case, the signal domain is a group itself, the cyclic group of order $n$, $$ C_n = langle , a : a^n = 1 , rangle equiv { , 1, a, a^2, dots, a^{n-1} , }. $$ It is convenient to parametrize the group, and hence the grid, through the exponent of the generator $$ C_n equiv { 0, 1, dots, n -1 } $$ as this indexing is consistent with the way most python indexes vectors. In this setting, the group operation may be reinterpreted as addition modulo $n$. . As the input domain is fixed, it feels natural to consider the GDL group $G$ to be $C_n$ as well. . We suppose that signals are scalar-valued, and are encoded in the natural basis, so that each $x in mathcal{X}(C_n, mathbb{R})$ may be expressed as . $$ mathcal{X}(C_n, mathbb{R}) = { x : C_n to mathbb{R} } , $$is finite dimensional, and each $x in mathcal{X}(C_n, mathbb{R})$ may be expressed as $$ x = left[ begin{matrix} x_0 vdots ,x_{n-1} , end{matrix} right] $$ . With this basis, we can describe the representation $ rho$ of $G equiv C_n$ concretely, as a matrix. . Given a vector $ theta = ( theta_0 , dots, theta_{n-1})$, recall the associated _circulant matrix_ is the $n times n$ matrix with entries $$ S( theta) := left( , theta_{ (u - v) mod n} right)_{ 0 , leq ,u, ,v , leq n-1 } $$ . In the specific case of $ theta_{+} := (0,1,0, dots, 0)^T$, the associated circulant matrix, $S_{+} := S( theta_{+})$ acts on vectors by shifting the entries of vectors to the right by one position, corresponding to addition by one, modulo $n$. . We call $S_+$ the (right) shift operator. . . Lemma $ quad$ A matrix is circulant if and only if it commutes with $S_+$. Moreover, given any two vectors $ theta, eta in mathbb{R}^n$, one has $S( theta) S( eta) = S( eta) S( theta)$. . . The importance of $S_+$ to the present discussion is that it generates a group isomorphic to the one-dimensional translation group $C_n$; the matrices $ { I, S_+, S_+^2, dots, S_+^{n-1} }$ constitute a faithful representation of $C_n$. . Circulant matrices are synonymous with discrete convolutions; given $x, theta in mathcal{X}( Omega, mathbb{R}) equiv mathbb{R}^n$, their convolution $x star theta$ is defined by $$ ( x star theta )_u := sum_{v = 0}^{n-1} x_{v mod n} , theta_{ (u-v) mod n} equiv S( theta) x $$ . Thus, the next corollary effectively follows from the much stronger theorem stated at the end of section 1.3. . . Corollary $ quad$ Any $f : mathcal{X}(C_n, mathbb{R}) to mathcal{X}(C_n, mathbb{R})$ which is linear and $C_n$-equivariant can be expressed (in the input coordinate system) as an $n times n$ circulant matrix $S( theta)$ for some vector $ theta$. . . . . ...example: local averaging as a circulant matrix . $ quad$ Recall a previous recipe for an equivariant function $F= Phi( X, A)$ using a local aggregation function $ varphi$. . In our present case of $ Omega equiv G equiv C_n$, we may write this local aggregation more concretely as $$ varphi ( x_u, X_{ textsf{nbhd}(u)} ) = varphi( x_{u-1}, , x_u, , x_{u+1} ), $$ with addition and subtraction in the indices above understood to be modulo $n$. . If in addition, we insist that $ varphi$ is linear, then it has the form $$ varphi( x_{u-1}, , x_u, , x_{u+1} ) = theta_{-1} x_{u-1} + theta_0 x_u + theta_1 x_{u+1}, $$ and in this case we can express $F = Phi (X, A )$ through the following matrix multiplication: $$ left[ begin{matrix} theta_0 &amp; theta_1 &amp; text{ } &amp; text{ } &amp; theta_{-1} theta_{-1} &amp; theta_0 &amp; theta_1 &amp; text{ } &amp; text{ } text{} &amp; ddots &amp; ddots &amp; ddots &amp; text{ } text{ } &amp; text{ } &amp; theta_{-1} &amp; theta_0 &amp; theta_1 theta_1 &amp; text{ } &amp; text{ } &amp; theta_{-1} &amp; theta_0 end{matrix} right] left[ begin{matrix} x_0 x_1 vdots ,x_{n-2} , x_{n-1} end{matrix} right] $$ This multi-diagonal structure is often synonymous with the concept of weight sharing in ML literature. . . . 2.3 ... one-dimensional convolutions in torch . The object in torch for executing convolutions of signals over $C_n$ is called torch.nn.Conv1d. We&#39;ll denote an instance of this object by $ tilde{B}$ . . class ... torch.nn.Conv1d . args . in_channels | out_channels | kernel_size | stride = 1 | . padding = 0 | dilation = 1 | groups = 1 | bias = True | . padding_mode = &#39;zeros&#39; | device = None | dtype = None | . Let us now relate the shapes of the input and output to the parameters . input parameter LaTeX symbol . in_channels | $ text{dim}( mathcal{C})$ | . out_channels | $ text{dim}( tilde{ mathcal{C}})$ | . kernel_size | $k$ | . stride | $ lambda$ | . padding | $ rho$ | . dilation | $ delta$ | . groups | $M$ | . This class specifies a one-dimensional convolution operation. Because of batching and the channel dimension, the input and output are $3$-tensors. We use $N$ to denote the batch size, and we will continue to use $n$ for the input length. . In general, $ mathcal{C}$ and $ mathcal{C}_1$ are the vector spaces of input channels and output channels respectively. In our current setting, assuming that signals $x$ are scalar signals over the cyclic group, one has $ textrm{dim}( mathcal{C}) = 1$. The shape of the input tensor is thus . $$ (N, 1, n) $$Even though we restrict ourselves to scalar-valued input signals, we will allow $ textrm{dim}( tilde{ mathcal{C}}) &gt; 1$, and we will write $ tilde{n}$ to denote the length of the output, so that the shape of the output tensor is . $$ (N, textrm{dim}( tilde{ mathcal{C}}), tilde{n}) $$ For the moment, we will also take $N=1$. Thus, inputs can be though of effectively as vectors, which we can easily visualize. We will think of the output as a length-$ textrm{dim}( tilde{ mathcal{C}})$ list of vectors (with scalar entries). . Our present goal is to try to understand how the number of learnable parameters depends on these shapes, as well as to visualize the effect of a convolutional layer in this simplest setting. . First, we remark that the relationship between $n_1$ and $n$, in terms of the input parameters, can be expressed as follows: . $$ tilde{n} = left lfloor frac{ n + 2 rho - delta (k-1) -1 }{ lambda} + 1 right rfloor $$ . method ... compute_out_size . def compute_out_size(in_size, padding, dilation, kernel_size, stride): numerator = in_size + 2 * padding - dilation * ( kernel_size - 1) - 1 arg_of_floor = (numerator / stride) + 1 return np.floor( arg_of_floor ) . . . . . ... example: concrete layer . For now, we will instantiate a parameters dictionary with dilation = 1 and padding = 0. Let us use $n = 32$, so that . $$ tilde{n} = left lfloor frac{n-k}{ lambda} + 1 right rfloor equiv left lfloor frac{32-k}{ lambda} + 1 right rfloor $$For our example, we specialize to kernel_size = 4 and stride = 2, in which case $ tilde{n} = 15$. . odcp = {} # one dimensional conv. parameters odcp[&quot;in_size&quot;] = 32 odcp[&quot;in_channels&quot;] = 1 odcp[&quot;padding&quot;] = 0 odcp[&quot;dilation&quot;] = 1 odcp[&quot;kernel_size&quot;] = 5 odcp[&quot;stride&quot;] = 2 odcp[&quot;groups&quot;] = 1 odcp[&quot;out_channels&quot;] = 16 . odcp[&quot;out_size&quot;] = compute_out_size( in_size = odcp[&quot;in_size&quot;], padding = odcp[&quot;padding&quot;], dilation = odcp[&quot;dilation&quot;], kernel_size = odcp[&quot;kernel_size&quot;], stride = odcp[&quot;stride&quot;] ) print( odcp[&quot;out_size&quot;] ) . . 14.0 . One of the important free parameters we can choose to initialize a one-dimensional convolutional layer is the number of out_channels. We choose out_channels = 16 in our first example. . In many vision related CNNs, the first layer takes in a grayscale or RGB image, outputting features with a much larger number of channels. . I am thinking of each output channel as a separate &quot;lint roller&quot; to run over the image. . From the docs, the parameter groups &quot;controls the connections between inputs and outputs.&quot; . Both in_channels and out_channels must be divisible by groups: . At groups = 1, all inputs are convolved to all outputs. . | At groups = 2, the operation becomes equivalent to having two convolutional layers side by side, each seeing half the input channels and producing half the output channels, and both subsequently concatenated. . | At groups = in_channels, each input channel is convolved with its own set of filters (of size out_channels / in_channels). . | . By taking in_channels = 1, we must have groups = 1 also. . The learned parameters of layer $ tilde{B}$ are of two types: those in the weight tensor $ tilde{ theta}$, and those in the bias vector $ tilde{b}$. At this point, for simplicity, we set $ tilde{b} equiv 0$ via bias = False, so that the only learned parameters are those in $ tilde{ theta}$. . from torch.nn import Conv1d . We now initialize the convolutional layer. . B_tilde = Conv1d(in_channels = odcp[&quot;in_channels&quot;], out_channels = odcp[&quot;out_channels&quot;], kernel_size = odcp[&quot;kernel_size&quot;], stride = odcp[&quot;stride&quot;], padding = odcp[&quot;padding&quot;], dilation = odcp[&quot;dilation&quot;], groups = odcp[&quot;groups&quot;], bias = False ) . Let us initialize a few possible input signals: X_1, X_2, X_3. . X_1 = torch.arange(0,32) info_1d(X_1, &quot;X 1&quot;) viz_1d( X_1 ) . . tensor X 1 num. dims 1 num. entries 32 shape [32] mean 15.5 std 9.233092656309694 ┌───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┐ │ 0 │ 1 │ 2 │ 3 │ 4 │ 5 │ 6 │ 7 │ 8 │ 9 │ 10 │ 11 │ 12 │ 13 │ 14 │ 15 │ 16 │ 17 │ 18 │ 19 │ 20 │ 21 │ 22 │ 23 │ 24 │ 25 │ 26 │ 27 │ 28 │ 29 │ 30 │ 31 │ └───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┘ . X_2 = torch.rand( (32) ) info_1d(X_2, &quot;X 2&quot;) viz_1d( X_2 ) . . tensor X 2 num. dims 1 num. entries 32 shape [32] mean 0.4745957 std 0.3013748 ┌──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┐ │ 0.72 │ 0.16 │ 0.67 │ 0.64 │ 0.62 │ 0.73 │ 0.84 │ 0.11 │ 0.45 │ 0.43 │ 0.16 │ 0.24 │ 0.28 │ 0.12 │ 0.00 │ 0.42 │ 0.24 │ 0.71 │ 0.16 │ 0.95 │ 0.48 │ 0.29 │ 0.68 │ 0.99 │ 0.05 │ 0.72 │ 0.41 │ 0.13 │ 0.06 │ 0.81 │ 0.96 │ 0.80 │ └──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┘ . X_3 = torch.randn( (32) ) info_1d(X_3, &quot;X 3&quot;) viz_1d( X_3 ) . . tensor X 3 num. dims 1 num. entries 32 shape [32] mean -0.0077615567 std 0.9509981 ┌──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┐ │ -0.6 │ 0.96 │ -0.8 │ 0.02 │ -1.3 │ 0.65 │ 0.12 │ 2.20 │ -0.5 │ 0.21 │ -0.1 │ -1.1 │ -0.1 │ 1.12 │ -0.6 │ 0.08 │ -1.3 │ -1.3 │ 0.13 │ 1.74 │ 0.55 │ -1.4 │ -0.1 │ -0.7 │ 1.12 │ 0.66 │ 0.11 │ 1.60 │ 0.31 │ -1.2 │ -0.9 │ 0.52 │ └──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┘ . To apply B_tilde to these signals, we must augment each signal with batch and channel dimensions: . X_1 = torch.as_tensor( X_1[ None, None, : ], dtype= torch.float32 ) X_2 = torch.as_tensor( X_2[ None, None, : ], dtype= torch.float32 ) X_3 = torch.as_tensor( X_3[ None, None, : ], dtype= torch.float32 ) . . Each output has shape . Y_1 = B_tilde( X_1 ).detach() Y_2 = B_tilde( X_2 ).detach() Y_3 = B_tilde( X_3 ).detach() print( np.array( Y_1.size() ) ) . . [ 1 16 14] . This is the output shape which we predicted: the last entry 14 (as of recent run) is the length $ tilde{n}$ of the output vector at each channel. The output has 16 channels, and our batch size $N$ remains 1. . The easiest way to visualize this output is to use torch.squeeze, which removes all dimensions of the input tensor of size one. The shape of each Y_1, Y_2, Y_3 after this squeezing will be [16,14]. . Y_1 = torch.squeeze(Y_1) Y_2 = torch.squeeze(Y_2) Y_3 = torch.squeeze(Y_3) print( np.array( Y_1.size() ) ) . . [16 14] . visualizing X_1: . viz_1d( torch.squeeze(X_1), colorbar = True ) . . visualizing Y_1 . Y_1_list = [ Y_1[j,:] for j in range(16) ] viz_list_of_1d( Y_1_list ) . . X_2 . viz_1d(torch.squeeze(X_2)) . . Y_2 . Y_2_list = [ Y_2[j,:] for j in range(16) ] list_Y_2 = [ tens[None,:] for tens in Y_2_list ] viz_list_of_1d( Y_2_list ) . . X_3 . viz_1d( torch.squeeze(X_3) ) . . Y_3 . Y_3_list = [ Y_3[j,:] for j in range(16) ] viz_list_of_1d( Y_3_list ) . . . . It is also of interest to say something about the size and shape of the weight tensor $ tilde{ theta}$ that effectively constitutes the layer B_tilde $ equiv tilde{B}$, having set the bias of this layer to be zero. . four_tuple_list = [ B_tilde.weight[j][0].detach() for j in range(16) ] four_tuple_list_prime = [ B_tilde.weight[j].detach() for j in range(16) ] weight_stack = torch.stack(four_tuple_list) . . We can view this weight matrix as a grid of &quot;images,&quot; each of which is effectively a $4$-tuple. . four_tuple_list_prime . . [tensor([[-0.4263, 0.0067, 0.1164, 0.1192, -0.4339]]), tensor([[ 0.0573, -0.0107, 0.3462, 0.1495, 0.1983]]), tensor([[-0.3103, 0.3386, 0.0122, -0.0715, -0.0886]]), tensor([[ 0.1754, -0.2693, -0.3207, -0.1704, 0.4245]]), tensor([[ 0.2760, -0.2987, -0.3486, 0.3718, -0.1541]]), tensor([[ 0.3042, -0.1554, 0.0681, -0.3196, -0.0570]]), tensor([[ 0.1059, -0.1253, 0.4401, -0.4115, 0.2772]]), tensor([[-0.1306, 0.1273, -0.4381, -0.0177, 0.4324]]), tensor([[-0.3801, -0.4468, -0.0702, -0.2962, 0.2563]]), tensor([[-0.3563, 0.2352, 0.0296, 0.2618, 0.2206]]), tensor([[ 0.2666, 0.3872, -0.1945, 0.4311, -0.4195]]), tensor([[ 0.3487, -0.1798, 0.0364, 0.3259, -0.0139]]), tensor([[-0.2289, 0.2245, -0.3838, 0.0922, 0.0169]]), tensor([[-0.2160, -0.1962, 0.3556, 0.0952, -0.0446]]), tensor([[-0.2914, -0.0188, 0.1127, -0.4291, -0.0389]]), tensor([[-0.0729, -0.3516, -0.1568, 0.1510, -0.2799]])] . or as a single block, though we remark that this matrix should be regarded as a stack of length $5$ vectors, each of which corresponds to its own circulant matrix with shape is appropriate to the input and output size. . viz_tens(weight_stack, display_size = 5) . . . Rmk $ quad$ Notice that what we are seeing is essentially noise. Yes, two of the input signals were initialized randomly. Importantly, the weights in $ hat{ theta}$ have been initialized randomly as well. Referencing the docs again, one sees that the weights are initialized as i.i.d. Uniform over the interval $[- sqrt{ M / (n k) }, sqrt{ M /(n k) }]$. The filters have not yet had a chance to have the patterns of signals imprint themselves through learning dynamics. . . In light of the above remark, let us design a length $5$ filter by hand. . filter = torch.tensor( [0, -1, 2, -1, 0] ) viz_1d(filter) . The point of this filter is to do some kind of edge detection, following examples given in pages like this, here we implement a simpler version in one dimension. Responses to this SO post were also helpful to look at. . Let us re-initialize B_tilde, now with only one output channel. This output will result from convolving our designed filter with input signals. . odcp = {} # one dimensional conv. parameters odcp[&quot;in_size&quot;] = 32 odcp[&quot;in_channels&quot;] = 1 odcp[&quot;padding&quot;] = 0 odcp[&quot;dilation&quot;] = 1 odcp[&quot;kernel_size&quot;] = 5 odcp[&quot;stride&quot;] = 2 odcp[&quot;groups&quot;] = 1 odcp[&quot;out_channels&quot;] = 1 . B_tilde = Conv1d(in_channels = odcp[&quot;in_channels&quot;], out_channels = odcp[&quot;out_channels&quot;], kernel_size = odcp[&quot;kernel_size&quot;], stride = odcp[&quot;stride&quot;], padding = odcp[&quot;padding&quot;], dilation = odcp[&quot;dilation&quot;], groups = odcp[&quot;groups&quot;], bias = False ) B_tilde.weight . Parameter containing: tensor([[[-0.2238, -0.1461, 0.0222, -0.1150, 0.3026]]], requires_grad=True) . To set these weights to filter in a direct fashion, we must first set the requires_grad attribute of the tensor to False. . B_tilde.weight.requires_grad = False for j in range(5): B_tilde.weight[0][0][j] = filter[j] B_tilde.weight . . Parameter containing: tensor([[[ 0., -1., 2., -1., 0.]]]) . Let us redefine Y_1, Y_2, Y_3 as the output of this new B_tilde acting on our three input signals. . Y_1 = B_tilde( X_1 ).detach() Y_2 = B_tilde( X_2 ).detach() Y_3 = B_tilde( X_3 ).detach() print( np.array( Y_1.size() ) ) Y_1 = torch.squeeze(Y_1) Y_2 = torch.squeeze(Y_2) Y_3 = torch.squeeze(Y_3) print( np.array( Y_1.size() ) ) . . [ 1 1 14] [14] . X_1 . X_1 . tensor([[[ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27., 28., 29., 30., 31.]]]) . info_1d(torch.squeeze(X_1), &quot;X 1&quot;) viz_1d(torch.squeeze(X_1) ) . . tensor X 1 num. dims 1 num. entries 32 shape [32] mean 15.5 std 9.233092 ┌─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┐ │ 0.0 │ 1.0 │ 2.0 │ 3.0 │ 4.0 │ 5.0 │ 6.0 │ 7.0 │ 8.0 │ 9.0 │ 10.0 │ 11.0 │ 12.0 │ 13.0 │ 14.0 │ 15.0 │ 16.0 │ 17.0 │ 18.0 │ 19.0 │ 20.0 │ 21.0 │ 22.0 │ 23.0 │ 24.0 │ 25.0 │ 26.0 │ 27.0 │ 28.0 │ 29.0 │ 30.0 │ 31.0 │ └─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┘ . Y_1 . viz_1d( Y_1 ) . . This is to be expected: the input signal is effectively a linear function, and our kernel effectively performs a discrete differentiation (and this is the motivation for edge detection -- derivatives detect edges). . X_2 . info_1d(torch.squeeze(X_2), &quot;X 2&quot;) viz_1d(torch.squeeze(X_2)) . . tensor X 2 num. dims 1 num. entries 32 shape [32] mean 0.4745957 std 0.3013748 ┌──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┐ │ 0.72 │ 0.16 │ 0.67 │ 0.64 │ 0.62 │ 0.73 │ 0.84 │ 0.11 │ 0.45 │ 0.43 │ 0.16 │ 0.24 │ 0.28 │ 0.12 │ 0.00 │ 0.42 │ 0.24 │ 0.71 │ 0.16 │ 0.95 │ 0.48 │ 0.29 │ 0.68 │ 0.99 │ 0.05 │ 0.72 │ 0.41 │ 0.13 │ 0.06 │ 0.81 │ 0.96 │ 0.80 │ └──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┘ . Y_2 . viz_1d( Y_2 ) . . . Rmk $ quad$ The next thing to link up with is the description of generalized convolution from the first notebook. Recall that, in general, one has . $$ tilde{ theta} in mathcal{X} ( H backslash G / tilde{H}, mathcal{C} otimes tilde{ mathcal{C}} ), $$but let us reduce this further based on the assumptions made. Because $G equiv Omega equiv C_n$, subgroup $H$ must be the trivial subgroup $ { e }$. Moreover, having assumed that $ dim{ mathcal{C}} = 1$, one also has $ mathcal{C} otimes tilde{ mathcal{C}} cong tilde{ mathcal{C}}$, so we may write . $$ tilde{ theta} in mathcal{X} ( G / tilde{H} , tilde{ mathcal{C}} ), $$with $C_{ tilde{n}} cong G / tilde{H}$. Of course, as $14$ does not divide $32$, we cannot interpret this map as a group homomorphism, for instance, at least not in a direct fashion. . . 2.4 ... two-dimensional convolutions in torch . . torch.nn.Conv2d . The arguments: . in_channels . | out_channels . | kernel_size . | stride $ quad$ controls the stride for the cross-correlation, a single number or a tuple. . | padding $ quad$ controls amount of padding applied to the input. It can either be a string, &quot;valid&quot; or &quot;same&quot; or a tuple of ints giving the amount of implicit padding applied on both sides. . | dilation $ quad$ controls the spacing between kernel points; &quot;also known as the a trous algorithm . | groups $ quad$ controls connections between inputs and outputs. The in_channels and out_channels must be divisible by groups. For example, . At groups = 1, all inputs are convolved to all outputs . | At groups = 2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated. . | At groups = in_channels, each input channel is convolved with its own set of filters (of size out_channels // in_channels) . | . | bias . | padding_mode, . | device, . | dtype . | . Let us now relate the shapes of the input and output to the parameters . input parameter LaTeX symbol . in_channels | $ text{dim}( mathcal{C})$ | . out_channels | $ text{dim}( mathcal{C}_1)$ | . kernel_size | $k$ | . stride | $ lambda$ | . padding | $ rho$ | . dilation | $ delta$ | . groups | $M$ | . Additionally, we use $N$ for the batch size of the input, N. We also let $(h,w)$ denote the height-width pair describing the shape of the input signal domain. . Correspondingly, we write $(h_1, w_1)$ for the height-width pair describing the shape of the output signal domain. . We remark that the stride can be either integer or a $2$-tuple, whose coordinates describe the vertical and horizontal stride respectively. We still write $ lambda$ for the stride when it is a tuple, and use $ lambda_h equiv lambda[0]$ and $ lambda_w equiv lambda[1]$ to denote its first and second coordinate, in this case. Likewise, the padding and kernel size may be $2$-tuples as well, and we use similar notation to denote their entries. . The full shape of the input to the layer includes the batch dimension, and is thus . $$ (N, text{dim}( mathcal{C}), H, W) ,, $$while the shape of the output is . $$ (N , text{dim}( mathcal{C}_1), H_1, W_1 ) $$These shapes, in particular the spatial dimensions of each, are related as follows: . $ begin{align} H_1 &amp;= left lfloor frac{ H + 2 rho_h - delta_h ( k_h -1) -1 }{ lambda_h} right rfloor W_1 &amp;= left lfloor frac{ W + 2 rho_w - delta_w ( k_w -1) -1 }{ lambda_w} right rfloor end{align}$, . in particular, the batch size does not have any bearing on how the shapes of tensors transform. . The parameters to be learned are the weights $w^1$ and biases $b^1$. These are both Tensor objects, accessed from the layer as Conv2d.weight and Conv2d.bias. The shape of the weight tensor is . $$ textrm{shape}(w^1) = left( , text{dim}( mathcal{C}_1), , text{dim}( mathcal{C}) big/ M , k_h, k_w right) $$The tensor $w^1$ thus has . $$ textrm{size}(w^1) = textrm{dim}( mathcal{C}_1) textrm{dim} ( mathcal{C}) k_h k_w big/ M $$scalar entries. . There is always the question of how to initialize weights. In the case of the Conv2d class, the weights are initialized to be i.i.d. $ text{Unif}( - sqrt{ alpha_1}, sqrt{ alpha_1} )$ random variables, where . $$ alpha_1 := frac{ textrm{dim}( mathcal{C}_1) }{ textrm{size}(w^1)} $$ The bias tensor is a much smaller object, we have . $ begin{align} textrm{shape}(b^1) = ( , textrm{dim}( mathcal{C}_1 ) ,) , , quad textrm{size}(b^1) = textrm{dim}( mathcal{C}_1) end{align} $ . Despite this, we use the same initialization (with mutual independence of all random variables in discussion) for the bias entries as we did for the weights. . ... A simple CNN . We consider possibly the simplest neural network that we can construct through the above blueprint. Suppose we have a binary classification problem, with the following hypothesis space. Let $ textsf{H}_1$ denote the hypothesis space of functions $f : mathcal{X}( C_n, mathbb{R}) to {0,1 }$ of the form . $$ f = A circ P circ mathbf{a} circ B ,, $$where the components of $f$ are . where the components of $f$ are . $B$ : $ quad$ A $C_n$-equivariant function, to be learned. It is represented as a circulant matrix $ mathbf{C}( theta)$, where $ theta$ is a vector $ theta equiv ( theta_0, dots, theta_{n-1})$ whose entries $ theta_j$ are parameters to be learned. . | $ mathbf{a} $ : $ quad$ We consider the ReLU activation function, $a : mathbb{R} to mathbb{R}_{ geq , 0}$ defined by $a(w) = max(0,w)$, for $w in mathbb{R}$. The bold-face $ mathbf{a}$ denotes the entry-wise action of this function on a given vector;for $y equiv ( ,y_1, , dots, , y_n , ) in mathcal{X}(C_n, mathbb{R})$, which we imagine as the output of $B(x)$ for some input signal $x$, we have $ mathbf{a} (y ) = ( , max(0,y_1), , dots, , max(0,y_n) )$. There are no learned parameters in this layer. . | $P$ : $ quad$ A coarsening operator. In this case, let us say it is a zero-padded group homomorphism. . $P : C_n to C_{n / d }$ for some divisor $d mid n$ footnote{zero-padding} , and let us say that it operates through max-pooling on the signal, over the pre-images of each element of $C_{n / d}$. . | $A$ : $ quad$ A global-pooling layer. We assume this has the form of a fully-connected layer, followed by a softmax. Specifically, . | .",
            "url": "https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2021/10/24/GDL2.html",
            "relUrl": "/jupyter/2021/10/24/GDL2.html",
            "date": " • Oct 24, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "notebook one",
            "content": "$ quad$ Modern neural network (NN) design is built on two algorithmic principles: hierarchical feature learning (concerning the NN architecture), and learning by local gradient-descent driven by backpropagation (concerning the dynamics undergone by the NN during its training). An instance of training data is modeled as an element of some high-dimensional vector space, making a generic learning problem subject to the curse of dimensionality. Most tasks of interest are not generic; the data comes from the physical world, and so it inherits low-dimensional structure and symmetries. Requiring an NN to respect the symmetries of the data upon which it acts amounts to a prior. The notes [BBCV21] construct a blueprint for neural network architecture incorporating these priors, termed geometric priors throughout the notes. Importantly, this blueprint provides a unified perspective of the most successful neural network architectures, at least at the level of the building blocks chosen for the network. . $ quad$ I am choosing to start the notes with the definition of a category. This is in part because it leads to a definition of a group of symmetries acting on some general &#39;object&#39;, and the goal of GDL is to incorporate into the NN arhitecture the symmetries of (and which act on) input data. I am also trusting that it will come in handy, at some point, following Bartosz Milewski&#39;s book on category theory for programmers: &quot;...category theory is a treasure trove of extremely useful programming ideas... Composition is at the very root of category theory — it’s part of the definition of the category itself. And I will argue strongly that composition is the essence of programming.&quot; . $ quad$ Moreover, the &#39;geometric&#39; constraints in GDL are summed up in a property called equivariance. Functors are mathematical objects central to category theory, in a sense are characterized by equivariance. They could become a useful perspective for describing some things in the notes, eventually. . algebra . References . Bronstein-Bruna-Cohen-Velicovic 2021, Geometric deep learning . | A first impression of group representations . | . graphs, categories, groups . . Definition. ( graph ) . $ quad$ A graph is a pair $G = ( textrm{V}, textrm{E})$, where $ textrm{V}$ is a set whose elements are called vertices. The set $ textrm{E}$ consists of edges, defined to be a multi-set of exactly two vertices in $ textrm{V}$, not necessarily distinct. . . . Definition. ( directed graph ) . $ quad$ A directed graph is a pair of sets $G = ( textrm{V}, textrm{A})$ of vertices and arrows (aka, directed edges). An arrow is an ordered pair of vertices. Self-edges have no distinct orientations. . . . Definition. ( domain and codomain operations ) . $ quad$ Consider an arrow $f$ of a directed graph $G = ( textrm{V}, textrm{A})$, specifically $f equiv (a,b) in textrm{A}$, with $a,b in textrm{V}$. The operations $ textsf{dom}$ and $ textsf{cod}$ act on the arrows $f in textrm{A}$ via $$ textsf{dom}f = a, , textsf{cod} f = b ,, $$ and are called the domain operation and codomain operation, respectively. . . $ quad$ Given arrows, $f$ and $g$ of some directed graph, say that the ordered pair $(g,f)$ is a composable pair if $ textsf{dom} g = textsf{cod} f$. Going forward, we express the relations $a = textsf{dom} f$ and $b = textsf{cod} f$ equivalently as $$ f : a to b , quad text { or } , quad a xrightarrow[ ,]{ f} b $$ . The next definition formalizes the behavior of a collection of structure-respecting maps between mathematical objects. . . Definition. ( category ) . $ quad$ A category is a directed graph $ textsf{C} = ( textrm{O}, textrm{A})$, whose vertices $ textrm{O}$ we call objects, such that . For each object $a in textrm{O}$, there is a unique identity arrow $ textrm{A} ni textrm{id}_a : a to a$, defined by the following property: for all arrows $f : b to a$ and $g : a to c$, composition with the identity arrow $ textrm{id}_a $ gives $$ textrm{id}_a circ f = f quad text{ and } quad g circ textrm{id}_a = g $$ . | For each composable pair $(g, f)$ of arrows, there is a unique arrow $g circ f$ called their composite, with $g circ f : textsf{dom} f to textsf{cod} g$, such that the composition operation is associative. Namely, for given objects and arrows in the configuration $$a xrightarrow[ ,]{ f} b xrightarrow[ ,]{ g} c xrightarrow[ ,]{ k} d ,, $$ one always has the equality $k circ (g circ f) = (k circ g ) circ f$. . | . $ quad$ Given a category $ mathcal{C} = ( textrm{O}, textrm{A})$, let $$ textsf{hom} (b,c) := { , f : f in textrm{A}, , textsf{dom} f = b in textrm{O}, , textsf{cod} f = c in textrm{O} , } $$ denote the set of arrows from $b$ to $c$. We use the terms morphism and arrow interchangeably. . $ quad$ Groups are collections of symmetries. A group $ textsf{G}$ is a category $ textsf{C} = ( textrm{O}, textrm{A})$ with $ textrm{O} = { o }$ ( so that we may identify $ textsf{G}$ with the collection of arrows $ textrm{A}$ ) such that each arrow has a unique inverse: for $g in textrm{A}$, there is an arrow $h$ such that $g circ h = h circ g = textrm{id}_o$. Each arrow $g in textrm{A}$ thus has $ textsf{dom} g = textsf{cod} g = o$. As remarked, the arrows $g in textrm{A}$ correspond to group elements $g in textsf{G}$. The categorical interpretation suggests that the group acts on some abstract object $o in textrm{O}$. In the present context, we care how groups act on data, and how this action is represented to a computer. . group representations . $ quad$ Linear representation theory uses linear algebra to study groups, by &#39;representing&#39; group elements as self-maps on a vector space $V$. When $V$ is finite dimensional, and is given a coordinate basis, group elements correspond to square matrices. Consider a function $ varphi : textsf{G} times V to V$, where $ textsf{G}$ is a group, and where $V$ is a vector space over $ mathbb{R}$. This function will encode how group elements $g$ are identified with self-maps of $V$. Filling in the first coordinate with $g$ produces the corresponding self-map $ varphi(g, cdot) : V to V$. When the map $ varphi$ is understood, or left general, we write $g.v$ in place of $ varphi(g,v)$, and we write $(g.)$ in place of $ varphi(g, cdot)$. . $ quad$ The &#39;representatives&#39; $(g.)$ of these group elements $g$ can always be composed. If this compositional structure is compatible with the original group operation, we say $ varphi$ is a group action on $V$. Specifically, $ varphi$ should satisfy $e.v = v$ for all $v in V$, where $e$ denotes the identity element of $ textsf{G}$, and in general one has $(gh).v = g.(h.v)$. The group action $ varphi$ is $ mathbb{R}$-linear if it is compatible with the $ mathbb{R}$-vector space structure on $V$, i.e. additive and homogeneous. Specifically, if for all $v,w in V$ and all scalars $ lambda in mathbb{R}$, one has $g.(v+w) = g.v + g.w$ and $g.( lambda v) = lambda , g.v$. . . Definition. ( linear representation ) . $ quad$ An $ mathbb{R}$-linear representation of group $ textsf{G}$ over $ mathbb{R}$-vector space $V$ is an $ mathbb{R}$-linear group action on $V$. . . $ quad$ The next example illustrates how linear group representations arise naturally when considering group actions on data. Input datum are modeled as members of some vector space $ mathcal{X}$, defined with respect to a domain $ Omega$ and a smaller channel vector space $ mathcal{C}$. We may assume all vector spaces discussed to be finite dimensional. Specifically, we consider the vector space of $ mathcal{C}$-valued signals over some finite, discrete domain $ Omega$, which perhaps has some graph structure. This vector space is denoted $ mathcal{X}( Omega, mathcal{C})$. . . . Example. ( shifts of RGB images ) . $ quad$ Consider, for some $n in mathbb{N}$, a signal domain $ Omega = mathbb{T}_n^2$, where $ mathbb{T}_n^2$ denotes the two-dimensional discrete torus of side-length $n$, namely $( mathbb{Z} / n mathbb{Z} )^2$. This domain has natural graph as well as group structures. If we imagine each vertex of $ Omega$ to be a pixel, we can express an $n times n$-pixel color (RGB) image as a signal $x : Omega to mathbb{R}^3$, with the first, second and third coordinates of $ mathbb{R}^3$ reporting Red, Green and Blue values of a given pixel. We make two observations: . As a vector space, $ mathcal{X}( Omega)$ is isomorphic to $ mathbb{R}^d$, with $d$ typically very large. In the above example, $d = 3n^2$, which is thirty-thousand for a $n times n equiv 100 times 100$ pixel image. . | Any group action on $ Omega$ induces a group action on $ mathcal{X}( Omega)$. . | $ quad$ Expanding on the latter, consider a group action of $ textsf{G}$ on domain $ Omega$. As the torus $ Omega$ already has group structure, it is natural to think of it acting on itself through translations, i.e. we now additionally consider $ textsf{G} equiv Omega$. The action of $ textsf{G}$ on itself $ Omega equiv mathbb{T}_n^2$ induces a $ textsf{G}$-action on $ mathcal{X}( Omega)$ as follows: for $g in G$ signal $x in mathcal{X}( Omega)$, the action $(g, x) mapsto mathbf{g}.x in mathcal{X}( Omega)$ is defined pointwise at each $u in Omega$: $$ ( mathbf{g}.x)(u) := x(g.u), $$ where the bold $( mathbf{g}.)$ is used to distinguish the action on signals from the action on the domain. . $ vdots$ . To summarize:any $ textsf{G}$-action on the domain $ Omega$ induces an $ mathbb{R}$-linear representation of $ textsf{G}$ over the vector space of signals on $ Omega$. . . . . . Example. ( one-hot encoding ) . $ quad$ It seems like standard practice to encode the collection of classes associated to some ML classification problem as an orthonormal basis. These are given ( to the computer ) in the usual coordinate basis $$ e_1 equiv (1, 0, dots, 0), , e_2 equiv (0,1, dots, 0), , dots, , e_n equiv (0, dots, 0,1) ,, $$ hence the nomenclature one-hot. In the preceding example, if one considers a one-hot encoding of the vertices of $ mathbb{T}_n^2$, we see that each signal is expressed with respect to this coordinate system, in the sense that $x = sum_{j=1}^n x_j e_j$. This kind of encoding is useful for considering general symmetries of the domain. For instance, if permuting node labels is a relevant symmetry, the action of the symmetric group $ frak{S}_n$ is naturally represented by $n times n$ permutation matrices. . . . $ quad$ The following definition reformulates the notion of a signal. . . Definition. ( features ) . $ quad$ A graph $G = ( textrm{V}, textrm{E} )$ has features if for each $v in textrm{V}$, one has the additional data of an $s$-dimensional vector $x(v) in mathbb{R}^s$, called the features of node $v$. Here, $ mathbb{R}^s$ is understood as the channel vector space $ mathcal{C}$ equipped with a coordinate basis. . . $ quad$ Above useage of the term features is compatible with its usage in ML. We can think of a neural network as a sequence of node-layers built on top of the signal domain graph $ Omega$, which forms the input layer of the network. An input signal endows the first node layer of a neural network with features, and the weights of the neural network propagate these through to node features on the nodes of successive layers of the network. The features on the last layer of the network can be read off as the output of the neural network. . equivariance . references . Bronstein--Bruna--Cohen--Velicovic 2021, Geometric deep learning . | Maron--Ben-Hamu--Shamir--Lipman 2018, Invariant and equivariant graph networks . | . definitions . $ quad$ We henceforth consider groups $G$ acting on some space of signals $ mathcal{X}( Omega)$ through a given action on some fixed signal domain $ Omega$. The group action is encoded in a linear representation $ rho$, assumed to be described in a given input coordinate system, just as we would need to specify to a computer. Thus, if $ dim ( mathcal{X} ) = n$, for each $g in G$, the object $ rho(g)$ is an $n times n$ matrix with real entries. . $ quad$ The training dynamics of the neural network take place in the hypothesis space $ mathcal{H}$, a collection of functions $$ mathcal{H} subset { , F : mathcal{X}( Omega) to mathcal{Y} , } , $$ where $ mathcal{Y}$ is a vector space of output channels. The point of the learning blueprint is to be able to choose $ textsf{H}$ compatible with the symmetries of input data. The blueprint describes $ textsf{H}$ explicitly, up to hyperparameters such as depth and layer widths. A key aspect of this blueprint is that $F in textsf{H}$ should be expressed as a composition of functions, most of which are $G$-equivariant. The requirement of $G$-equivariance constitutes a geometric prior. From this prior, one can derive the architecture of a generic CNN when $G$ corresponds to translations, and a family of generalizations for other domains and group actions. . . Definition. ( group invariance, equivariance ) . $ quad$ Let $ rho$ be a representation of group $ textsf{G}$ over $ mathcal{X}( Omega)$, and let $ rho&#39;$ be a representation of the same group over the $ mathbb{R}$-vector space $ mathcal{Y}$. A function $F: mathcal{X}( Omega) to mathcal{Y}$ is $ textsf{G}$-equivariant if for all $g in textsf{G}$ and all $x in mathcal{X}( Omega)$, we have $ rho&#39;(g) F(x) = F ( rho(g) x )$. Say $F$ is $ textsf{G}$-invariant if this holds when $ rho&#39;$ is the trivial representation, i.e. $F ( rho(g) x) = F(x)$ for all $g in textsf{G}$ and $x in mathcal{X}( Omega)$. . . examples . . . Example. ( permutation invariance and equivariance in data ) . $ quad$ Suppose we are given either a set $ textrm{V}$, or more generally a graph $G = ( textrm{V}, textrm{E} )$, with $ # textrm{V} = n$ in either case. As discussed, a signal $x$ over $ textrm{V} = { v_1, dots, v_n }$ can be thought of as a collection of node features $ { , x(v_1), dots, x(v_n) , }$, with $x(v_j) in mathbb{R}^s$. Let us stack the node features as rows of an $n times s$ matrix called the design matrix, which we also refer to as $x$: . $$ x = left[ begin{matrix} x(v_1) vdots x(v_n) end{matrix} right] , $$which is effectively the same object as signal $x$, provided the vertices are labeled as described. The action of $g in mathfrak{S}_n$ on this input data is naturally represented as an $n times n$ permutation matrix, $P equiv rho(g)$. . $ quad$ One standard way to construct a permutation invariant function $F$ in this setting is through the following recipe: a function $ psi$ is independently applied to every node&#39;s features, and $ varphi$ is applied on its sum-aggregated outputs. $$ F( X) = varphi left( , sum_{j , = , 1}^n psi(x(v_j)) , right) . $$ Such a function can be thought of as reporting some &#39;global statistic&#39; of signal $x$. . $ quad$ Equivariance manifests even more naturally. Suppose we want to apply a function $F$ to the signal to &#39;update&#39; the node features to a set of latent node features. This is the case when a neural network outputs an image segmentation mask; the underlying domain does not change, but the features at each node are updated to the extent that they may not even agree on the number of channels. We can stack these latent features into another design matrix, $F(x)$. The order of the rows of $F(x)$ should clearly be tied to the order of the rows of $x$, i.e. permutation equivariant: for any permutation matrix $P$, it holds that $F(P x ) = P F(x)$. . $ quad$ As a concrete example of a permutation equivariant function, consider a weight matrix $ theta in mathbb{R}^{s , times , s&#39;}$. This matrix can be used to map a length-$s$ feature vector at a given node to some new, updated feature vector with $s&#39;$ channels. Applying this matrix to each row of the design matrix is an example of a shared node-wise linear transform, and constitutes a linear, $ mathfrak{S}_n$-equivariant map. . . $ quad$ The above example considered signals over the nodes of a graph only, thus label permutation symmetry applies equally well, regardless of the graph structure ( or lack of structure ) underlying such functions. In the case that signals $x$ have a domain with graph structure, encoded by adjacency matrix $ textrm{A}$, it feels right to work with a hypothesis space recognizing this structure. . This is to say that we wish to consider functions $F in textsf{H}$ with $F equiv F( x, textrm{A} )$. Such a function is (label) permutation invariant if $F( Px, , P textrm{A} P^{ textrm{T}} ) = F ( x, textrm{A})$, and is permutation equivariant if $$ F( P x, P textrm{A} P^T ) = PF( x, textrm{A} ) $$ for any permutation matrix $P$. . . Rmk $ quad$ On a characterization of linear $ mathfrak{S}_n$-equivariant functions on nodes. BBCV 21: &quot;One can verify any such map can be written as a linear combination of two generators, the identity and the average. As observed by Maron et al. 2018, any linear permutation equivariant $F$ can be expressed as a linear combination of fifteen linear generators; remarkably, this family is independent of $n equiv # mathcal{V}$.&quot; . . $ quad$ Among the generators described in the above remark, the geometric learning blueprint &#39;specifically advocates&#39; for those that are also local, in the sense that the output on node $u$ directly depends on its neighboring nodes in the graph. We can formalize this constraint explicitly, by defining the $1$-hop neighborhood of node $u$ as $$ textsf{nbhd}(u) equiv textsf{nbhd}_1(u) := { v : { u,v } in mathcal{E} } , $$ as well as the corresponding neighborhood features, $$ x({ textsf{nbhd}(u)}) := { ! { , x(v) : v in textsf{nbhd}(u) , } ! } , $$ which is a multiset, as indicated by double-brackets, as distinct nodes may be decorated with the same feature vector. . . . Example. ( permutations and local averaging ) . $ quad$ The node-wise linear transformation described in the previous example can be thought of as operating on $0$-hop neighborhoods of nodes. We generalize this with an example of a function operating on $1$-hop neighborhoods. Instead of a fixed map between feature spaces $ theta : mathbb{R}^s to mathbb{R}^{s&#39;}$, cloned to a pointwise map, we instead specify a local function $$ varphi equiv varphi( , x(u), , x( textsf{nbhd}(u)) , ) $$ operating on the features of a node as well as those of its $1$-hop neighborhood. . $ quad$ We may construct a permutation equivariant function $ mathbf{ phi}$ by applying $ varphi$ to each $1$-hop neighborhood in isolation, and then collecting these into a new feature matrix. . As above, the vertices of the signal domain graph $ textrm{V} = { v_1, dots, v_n }$. . $$ phi ( x, textrm{A} ) = left[ begin{matrix} varphi( , x(v_1) , , x( ,{ textsf{nbhd}(v_1)} , ) varphi( , x(v_2) , , x( , textsf{nbhd}(v_2) , ) vdots varphi( , x(v_n) , , x( , textsf{nbhd}(v_n) , ) end{matrix} right] $$The permutation equivariance of $ phi$ rests on the output of $ varphi$ being independent of the ordering of the nodes in each $ textsf{nbhd}v)$. Thus, if $ varphi$ is permutation invariant, as in a local averaging, this property is satisfied by $ phi$ as well. BBCV 21: &quot;The choice of $ varphi$ plays a crucial role in the expressive power of the learning scheme.&quot; . . . $ quad$ For signals $x$ over graphs, it is natural to consider a hypothesis space whose functions operate on the pair $( x, textrm{A})$, where $ textrm{A}$ is the adjacency matrix of the signal domain. Thus, for such signals the domain naturally becomes part of the input. The GDL blueprint distinguishes between two contexts: . one in which the input domain is fixed, and . | another in which the input domain varies from signal to signal. . | The preceding example demonstrates that, even in the former context, it can be essential that elements of $ textsf{H}$ treat the fixed domain as an argument. . $ quad$ When the signal domain has geometric structure, it can often be leveraged to construct a coarsening operator, one of the components of a GDL block in the learning blueprint. Such an operator passes a signal $x in mathcal{X}( Omega)$ to a signal $y in mathcal{X}( Omega&#39;)$, where $ Omega&#39;$ is a coarse-grained version of $ Omega$. The domain may be fixed for each input, but this domain changes as the signal passes through the layers of the NN, and it is natural that the functions the NN is built out of should pass this data forward. BBCV 21: &quot;Due to their additional structure, graphs and grids, unlike sets, can be coarsened in a non-trivial way, giving rise to a variety of pooling operations... more precisely, we cannot define a non-trivial coarsening assuming set structure alone. There exist established approaches that infer topological structure from unordered sets, and those can admit non-trivial coarsening.&quot; . equivariance in neural networks . references . Kondor-Trivedi 2018 | . feed-forward neural networks . $ quad$ The building blocks of neural networks are tensor objects. It is convenient for now to present a definition of a neural network without introducing the notion of a tensor. . . Definition. ( feed-forward neural network ) . $ quad$ A feed-forward neural network $$ ( textsf{N}, textsf{S}, b, w, a ) $$ is a directed graph $( textsf{N}, textsf{S})$, called the computation graph, whose vertices $ textsf{N}$ and edges $ textsf{S}$ are equipped with weights, respectively $b$ and $w$, along with an activation function $a$. For simplicity, we assume a single activation function $a$ is used across the network. The objects should satisfy the following: . The vertices $ textsf{N}$ of the computation graph, also called neurons, are partitioned into $L+1$ distinct layers, for some $L in mathbb{N}$. The collection of vertices in layer $ ell$ is denoted $ textsf{N}_ ell$. Layer $ textsf{N}_0$ is called the input layer, while $ textsf{N}_L$ is the output layer. . | We write $ mathfrak{n}_i^ ell$ for the $i$th neuron in layer $ ell$. Each neuron is equipped with a bias parameter, $b_i^ ell$. The vector of biases for neurons in a given layer $ ell$ is denoted $b^ ell$, and the collection of all bias parameters across all layers is denoted $b$. . | The edges $ textsf{S}$ of the computation graph, called synapses, only join neurons in consecutive layers, and importantly, the orientation of each edge always points towards the larger layer. This is why the network is &quot;feed-forward.&quot; . | This structure effectively partitions the synapses into layers, indexed by the largest layer among the two endpoints of the edges. We write $ textsf{S}_ ell$ for the $ ell$th layer of synapses, though we could also write $ textsf{S}_{ ell-1, ell}$ to emphasize that the edges in $ textsf{S}_ ell$ join neurons in $ textsf{N}_{ ell-1}$ to those in $ textsf{N}_ ell$. The synapse in $ textsf{S}_ ell$ joining neuron $ mathfrak{n}_i^{ ell-1}$ to $ mathfrak{n}_j^ ell$ is denoted $ mathfrak{s}_{ij}^{ ell}$. . | Each synapse $ mathfrak{s}_{ij}^{ ell}$ is equipped with a weight parameter, $w_{ij}^{ ell}$. The collection of weights associated to neurons in layer $ ell$ is denoted $w^ ell$, and the collection of all weights across all layers, $w$. . | For all layer indices $ ell &gt;0$, and given an input vector $x$, let $a^ ell$ denote the activation at layer $ ell$. The activation at layer $ ell$ is a vector $a^ ell = ( a_1^ ell, dots, a_{ # textsf{N}_ ell }^ ell)$, the $j$th coordinate of which is called the activation at neuron $ mathfrak{n}_j^ ell$. These activations are defined inductively by $$ a_j^ ell = a left( left( sum_{i} w_{ij}^ ell a_i^{ ell-1} right) + b_j^{ ell} right) , $$ with the activation at the $0$th layer, $a^0$ is defined as some input signal $x$, without any transformation applied to it. . | . . $ quad$ The parameter $L$, deteremining the number of layers, is the depth of the network, while the width of layer $ ell$ is the number of neurons in the layer. We will often abuse notation slightly, and write $ textsf{N}$ in place of the quintuple $( textsf{N}, textsf{S}, b, w, a )$. Each feed-forward neural network $ textsf{N}$ of depth $L$ is associated to a function $$x mapsto F_{ textsf{N}}(x) equiv a^L(x, textsf{N})$$ . So, $F_{ textsf{N}}$ can be expressed as the the alternating composition of nonlinearities and transformations between layers . $$ alpha circ B_L circ alpha circ B_{L-1} circ alpha dots circ alpha circ B_1, $$where $ alpha$ denotes the nonlinearity $a$ applied entrywise to its vector argument. In the present subsection, it is convenient to consider each layer of neurons as embedded in $ mathbb{N}$ through some enumeration. Signals in the input layer are thus compactly supported functions $x : mathbb{N} to mathcal{C}$, where $ mathcal{C}$ is the input channel vector space. . $ quad$ As signal $x$ is propagated through the network, its codomain may change to a different channel vector space, so that in general, . $$ B_ ell : mathcal{X}_c( mathbb{N}, mathcal{C}_{ ell-1} ) to mathcal{X}_c ( mathbb{N}, mathcal{C}_ ell ), quad z mapsto w^ ell z + b^ ell $$so that $F_{ textsf{N}}$ may also be expressed as . $$ mathcal{X}_c( mathbb{N}, mathcal{C} ) xrightarrow[ ,]{ B_1} mathcal{X}_c( mathbb{N}, mathcal{C}_1 ) xrightarrow[ ,]{ alpha } mathcal{X}_c( mathbb{N}, mathcal{C}_1 ) xrightarrow[ ,]{ B_2} mathcal{X}_c( mathbb{N}, mathcal{C}_2) xrightarrow[ ,]{ alpha } , dots , xrightarrow[ ,]{ B_L} mathcal{X}_c( mathbb{N}, mathcal{C}_L) xrightarrow[ ,]{ alpha } mathcal{X}_c( mathbb{N}, mathcal{C}_L) $$ This view obscures the width of each layer, as well as the shape of the signal as it propagates through the layers. Nonetheless, it suffices to discuss equivariance in the context of such functions. Henceforth, we use the term neural network to indicate a feed forward neural network. . equivariant neural networks . $ quad$ In the present context, we suppose there is a group $ textsf{G}$ acting on each of the signal domains. As discussed, the signal domains have been obscured by embedding each domain into $ mathbb{N}$. We should keep in mind that these signal domains are changing, a priori, with each new layer. As such, the group $ textsf{G}$ may act differently on the signal domain at each layer, leading to different representations of $ textsf{G}$, which we wish to distinguish in our notation. For $ ell = 0, dots, L$, let $ rho^ ell$ denote the representation of $ textsf{G}$ at the $ ell$th layer, so that for each $g in textsf{G}$, $ rho_g^ ell$ is an invertible linear map, . $$ rho_g^ ell : mathcal{X}_c( mathbb{N} , mathcal{C}_ ell) to mathcal{X}_c( mathbb{N} , mathcal{C}_ ell). $$ . Definition. ( group equivariant neural network ) . $ quad$ Let $ textsf{N}$ be a feed-forward neural network of depth $L$. Let $ textsf{G}$ be a group acting on the signal domain of each layer of $ mathbb{N}$. The associated representation of this action on signals, at layer $ ell$, is denoted $ rho^ ell$ as above. The network $ textsf{N}$ is said to be $ textsf{G}$-equivariant if each $B_ ell$ is $ textsf{G}$-equivariant, in the sense that for all $g in textsf{G}$, and all appropriately supported $z in mathcal{X}_c( mathbb{N}, mathcal{C}_{ ell-1})$, one has . $$ rho_g^{ ell} B_ ell (z) = B_ ell ( rho_g^{ ell-1}z ) $$ . convolution . $ quad$ The main result of KT 18 is a classification of equivariant neural networks: they show each $B_ ell$ corresponds to a generalized convolutional layer, which we now introduce. According to the usual analytic definition, given $ varphi, theta : mathbb{R} to mathbb{R}$ sufficiently nice, their convolution $ varphi star theta$ is defined by $$ ( varphi star theta ) (u) = int_{ mathbb{R}} varphi(u-v) theta (v) , textrm{d} v $$ This operation is commutative, and we can interchangeably think of $ varphi$ and $ theta$ as signal and filter. . This definition can be generalized to (say, $ mathbb{R}$-valued) functions/signals over other domains, in particular compact groups, which admit unique Haar probability measures. . . Definition. ( convolution ) . $ quad$ Letting $ mu_{ textsf{G}}$ denote the unique Haar probability measure of compact group $ textsf{G}$, we define the convolution of two functions $ varphi, theta : textsf{G} to mathbb{R}$ by $$ ( varphi star theta) (g) = int_{ textsf{G}} varphi(gh^{-1}) theta(h) , textrm{d} mu_G(h) $$ . . $ quad$ We have already seen an instance in which the signal domain is a compact group $-$ when modeling images, RGB or grayscale, we took the domain to be the discrete two-dimensional torus $ mathbb{T}_n^2$. The domains of the signals that propagate through the network aren&#39;t necessarily groups. In general, we require these domains to be homogeneous spaces, which means they are equipped with a transitive $ textsf{G}$-action. Going forward, we also assume that $ textsf{G}$ is some finite group, which is reasonable if $ textsf{G}$ is to be represented to a computer. . $ quad$ Let $ textsf{G}$ be a compact group acting on homogeneous space $ Omega$. Let us fix a distinguished &#39;basepoint&#39; $v_e in Omega$. The basepoint allows us to index the space $ Omega$ using $ textsf{G}$, through the transitivity of its action. Any $v in Omega$ can be expressed as $g.v_e$ for some $g in textsf{G}$. Let us write $$ v_g := g.v_e ,, $$ and observe that group elements of $ textsf{G}$ which fix $v_e$ form a subgroup, the stabilizer of $v_e$, denoted $ textsf{H}$. The (left) quotient space $ textsf{G} / textsf{H}$ is the collection of left cosets $$ g textsf{H} := { gh : h in textsf{H} }, $$ and through the map $g mapsto v_g$, this quotient space can be identified with the domain $ Omega$ itself. . $ quad$ The quotient map $g mapsto g textsf{H}$ is natural to consider, given a subgroup $ textsf{H} subset textsf{G}$. It is convenient to introduce another $ textsf{G} / textsf{H} to textsf{G}$ which is effectively a one-sided inverse of the quotient map. This &#39;inverse&#39; of the quotient map can be rephrased as a selection of a coset representative $[g] in g textsf{H}$. For $v in Omega$, let $[v]$ denote a coset representative of the coset of group elements which map $v_e$ to $v$. This leads to an identification of $v in Omega$ with $[v] textsf{H} in textsf{G} / textsf{H}$. . . Definition. ( projection, lift ) . $ quad$ Consider a function $ varphi : textsf{G} to mathbb{R}$, with $ textsf{G}$ a group acting (transitively) on some homogeneous space $ Omega$ with stabilizer $ textsf{H}$, so that $ Omega cong textsf{G}/ textsf{H}$. The projection of $ varphi$ to $ Omega$ is the function $ varphi_ downarrow : Omega to mathbb{R}$ given by averaging $ varphi$ over the coset corresponding to a given $v in Omega$: $$ varphi_ downarrow (v) := frac{1}{ # textsf{H}} sum_{g in [v] textsf{H}} f(g). $$ Conversely, given $ varphi : Omega to mathbb{R}$, the lift of $ varphi$ to $ textsf{G}$ is the function $ varphi^ uparrow: textsf{G} to mathbb{R}$ given by $$ varphi^ uparrow (g) := varphi([v_g] textsf{H}). $$ . . $ quad$ The notion of a lift allows us to generalize the convolution operation to pairs of functions whose domains are perhaps distinct quotient spaces of group $ textsf{G}$. Let $ Omega$ and $ Omega&#39;$ be such quotient spaces, and consider $ varphi: Omega to mathbb{R}$ and $ theta : Omega&#39; to mathbb{R}$; their convolution $ varphi star theta : textsf{G} to mathbb{R}$ is defined by $$ ( varphi star theta) (g) = sum_{h in textsf{G}} varphi^ uparrow(gh^{-1}) , theta^ uparrow(h) , . $$ . $ quad$ The definition of a lift clearly generalizes to functions whose domain is some right quotient space $ textsf{H} backslash textsf{G}$, consisting of right cosets $$ textsf{H}g := { hg : h in textsf{H} }, $$ as well as to functions defined on the double quotient space $ textsf{H} backslash textsf{G} / textsf{K}$, for some subgroups $ textsf{H}, textsf{K} subset textsf{G}$, whose elements are the double cosets $$ textsf{H}g textsf{K} := { hgk : h in textsf{H}, k in textsf{K} }. $$ . Thus, the definition of convolution can be extended to pairs of functions whose domains are any type of quotient space associated to $ textsf{G}$. . . Definition. ( generalized convolution ) . $ quad$ Let $ textsf{G}$ be a finite group, and consider two, perhaps distinct, associated homogeneous space $ Omega$ and $ Omega&#39;$. Consider functions $ varphi : Omega to mathbb{R}$ and $ theta : Omega&#39; to mathbb{R}$. The generalized convolution $ varphi star theta : textsf{G} to mathbb{R}$ is $$ ( varphi star theta) (g) = sum_{h in textsf{G}} varphi^ uparrow(gh^{-1}) , theta^ uparrow(h). $$ . . $ quad$ Now that we have some understanding of the structural requirements we make of signal domains, we are more specific about the output $F_{ textsf{N}}$ of some neural network $ textsf{N}$; it can be expressed as . $$ mathcal{X}( Omega, mathcal{C} ) xrightarrow[ ,]{ B_1} mathcal{X}( Omega_1, mathcal{C}_1 ) xrightarrow[ ,]{ alpha } mathcal{X}( Omega_1, mathcal{C}_1 ) xrightarrow[ ,]{ B_2} mathcal{X}( Omega_2, mathcal{C}_2) xrightarrow[ ,]{ alpha } , dots , xrightarrow[ ,]{ B_L} mathcal{X}( Omega_L, mathcal{C}_L) xrightarrow[ ,]{ alpha } mathcal{X}( Omega_L, mathcal{C}_L) , , $$where $ Omega equiv Omega_0, Omega_1, dots, Omega_L$ is a sequence of homogeneous spaces associated to $ textsf{G}$. In particular, we may identify each $ Omega_ ell$ with its stabilizer subgroup $ textsf{H}_ ell$ in $G$, so that $ Omega_ ell cong textsf{G} / textsf{H}_ ell$. Each affine block $B_ ell$ consists of a linear transformation $T_ ell : mathcal{X}( Omega_{ ell-1}, mathcal{C}_{ ell-1}) to mathcal{X}( Omega_ ell, mathcal{C}_ ell)$, followed by addition of the bias vector $b^ ell$. The object $T_ ell$ is very similar to the weight matrix $w^ ell equiv [ w_{ij}^ ell ]_{i,j}$. The linear transformation $T_ ell$ associated to this weight matrix may involve first reshaping it into a tensor, depending on the signal domain. . . Definition. ( group-convolutional neural network ) . $ quad$ Let $ textsf{N}$ be a feed-forward neural network of depth $L$, whose layers $ Omega equiv Omega_0, Omega_1, dots, Omega_L$ are homogeneous spaces associated to finite group $ textsf{G}$. Each $ Omega_ ell$ corresponds to its stabilizer in $ textsf{G}$, denoted $ textsf{H}_ ell$. Let $( mathcal{C}_j)_{j=0}^L$ denote the sequence of channel vector spaces associated to each layer. The affine blocks of $ textsf{N}$ are, as usual, denoted $B_ ell$ for $ ell in {1, dots, L }$, and the linear transformation within each block is denoted $T_ ell$. We call $ textsf{N}$ a $ textsf{G}$-convolutional neural network ($ textsf{G}$-CNN) if each $T_ ell$ may be expressed as $$ T_ ell (z) = z star theta_ ell, $$ for some filter $$ theta_ ell in mathcal{X} ( textsf{H}_{ ell-1} backslash textsf{G} / textsf{H}_ ell , , mathcal{C}_{ ell-1} otimes mathcal{C}_{ ell} ), $$ where the symbol $ otimes$ denotes the tensor product of vector spaces. . . $ quad$ The tensor product will be defined in the next section. The relevance of $ textsf{G}$-CNN&#39;s to equivariance in neural networks is encapsulated in the following theorem: if we feel $ textsf{G}$-equivariance constitutes an important prior on the network structure, the notion of a $ textsf{G}$-CNN is fundamental. . . Theorem ( Kondor-Trivedi 2018, Theorem 1 ) . $ quad$ Let $ textsf{G}$ be a compact group with transitive action on each layer of the feed-forward neural network $ textsf{N}$. Then $ textsf{N}$ is $ textsf{G}$-equivariant, in the sense described above, if and only if it is a $ textsf{G}$-CNN. . . tensors . references . Schwarz, Topology for Physicists . | Wald, General Relativity . | . vectors and covectors, through the lens of rep. theory . $ quad$ Consider the action of some finite group $ textsf{G}$ on discrete domain $ Omega$. Let $ mathcal{X}$ denote the space of $ mathcal{C}$-valued signals, where $ mathcal{C}$ is itself a vector space over $ mathbb{R}$. In this case, $ mathcal{X}$ also has the structure of a vector space over $ mathbb{R}$. The action of $ textsf{G}$ on $ Omega$ induces an action on $ mathcal{X}$, putting elements $g$ of $ textsf{G}$ in correspondence with elements of $ textrm{GL}( mathcal{X})$, which denotes the vector space of invertible linear endomorphisms of $ mathcal{X}$. . $ quad$ Let $ rho$ denote the $ mathcal{X}$-representation of $ textsf{G}$ just described. A subspace $ mathcal{X}&#39; subset mathcal{X}$ is called an invariant subspace of representation $ rho$ if all operators $ rho(g)$ map $ mathcal{X}&#39;$ to itself. Clearly, restricting $ rho$ to $ mathcal{X}&#39;$ is itself a representation of $ textsf{G}$ over vector space $ mathcal{X}&#39;$. The representation $ rho$ is called irreducible if there are no invariant subspaces of $ mathcal{X}$ other than $ mathcal{X}$ itself and the trivial subspace $ {0 }$. . $ quad$ If $ rho_1$ and $ rho_2$ are representations of $ textsf{G}$ over $ mathbb{R}$-vector spaces $ mathcal{X}_1$ and $ mathcal{X}_2$, their direct sum is defined as the representation $ rho$ of $ textsf{G}$ over $V_1 oplus V_2$ given by $$ [ rho(g)] ,( v_1, v_2) = ( , rho_1(g) v_1, , rho_2(g) v_2 , ), $$ for any $v_1 in mathcal{X}_1$ and $v_2 in mathcal{X}_2$. . Two representations $ rho_1$ and $ rho_2$ over $ mathcal{X}_1$ and $ mathcal{X}_2$ are equivalent if there is an isomorphism $ varphi : mathcal{X}_1 to mathcal{X}_2$ so that . $$ varphi rho_1(g) = rho_2(g) varphi ,. $$ $ quad$ We say a representation $ rho$ is orthogonal if each $ rho(g)$ is an orthogonal linear transformation (it preserves the length of all unit vectors). If $ mathcal{Y}$ is an invariant subspace of an orthogonal representation $ rho$, so is its orthogonal complement $ mathcal{Y}^ perp$. Furthermore, $ mathcal{Y}$ and $ mathcal{Y}^ perp$ inherit representations of $ textsf{G}$ through restriction, and the original representation $ textsf{G}$ is equivalent to the direct sum of these restrictions. . $ quad$ For any representation $ rho$ of compact group $ textsf{G}$ over $ mathcal{X}$, one can find a scalar product on $ mathcal{X}$ that is invariant under $ rho$. This is equivalent to saying that for an appropriate choice of scalar product on $ mathcal{X}$, in terms of $ textsf{G}$, every representation of $ textsf{G}$ over $ mathcal{X}$ is orthogonal. One constructs this by taking any scalar product on $ mathcal{X}$, and then by averaging its images under the $ textsf{G}$-action. The averaging done is with respect to Haar measure. Crucially, the existence of this invariant scalar product implies that every finite-dimensional representation of a compact $ textsf{G}$ is equal to the direct sum of irreducible representations. . $ quad$ Let $ textrm{GL}(n, mathbb{R})$ denote the space of $n times n$ matrices with real entries. As thee group $ textrm{GL}( mathbb{R}^n)$ of linear transformations of $ mathbb{R}^n$ is isomorphic to $ textrm{GL}(n, mathbb{R})$, one can regard any group homomorphism $ textsf{G} to textrm{GL}(n, mathbb{R})$ as an $n$-dimensional representation of $ textsf{G}$. In the case that $ textsf{G}$ is itself $ textrm{GL}(n, mathbb{R})$, we have the natural representation $ rho(g) v equiv g v$, called the vector representation. The covector representation is defined by $ rho(g) v$ = $(g^T)^{-1}v$. The latter matrix can be written unambiguously as $g^{-T}$ as inverse and transpose commute with one another. The elements of a space on which a matrix group acts via the vector representation are vectors, and covectors can be defined analogously. We expand on the discussion of vectors and covectors after introducing a basis in the next section. . $ quad$ By the above discussion, we should feel no loss in generality working with orthogonal representations of groups over some signal $ mathbb{R}$-vector space $ mathcal{X}$. Moreover, as feeding any signal to a computer requires choosing a basis for $ mathcal{X}$, it is natural to expect this basis to be &quot;maximally compatible&quot; with the orthogonal representation, in the sense that the basis is orthonormal. . vectors and covectors, in coordinates . $ quad$ Thus it is natural to assume that an orthonormal basis $(e_j)_{j=1}^n$ for some $n$-dimensional vector space $ mathcal{X}$ is always given alongside $ mathcal{X}$, and it is through the lens of this basis that we explore the covector representation described above. The basis furnishes a canonical isomorphism between $ mathcal{X}$ and $ mathbb{R}^n$, and hence between $ textrm{GL}( mathcal{X})$ and $ textrm{GL}(n, mathbb{R})$. On the other hand, the inner product $ langle cdot, cdot rangle$ furnishes a canonical isomorphism between $ mathcal{X}$ and its dual $ mathcal{X}^*$. Elements of $ mathcal{X}$ are vectors and elements of $ mathcal{X}^*$ are called covectors. . The above is technically correct. But I can&#39;t help but wonder:if $ mathcal{X}$, as a vector space, has a further structure as a space of functions taking values in channel vector space $ mathcal{C}$, how should this be reflected in the basis for $ mathcal{X}$? Perhaps this structure means that it&#39;s proper to think of elements of the signal vector space as already having some tensor structure. . $ quad$ Through the assumed basis, any representation $ rho$ of $ textsf{G}$ corresponds uniquely to a representation of $ textsf{G}$ over $ mathbb{R}^n$, as well as to a representation of $ textsf{G}$ over the dual space $( mathbb{R}^n)^*$. In either case, group elements can effectively be identified with $n times n$ matrices. We will write coordinates of a vector $x in mathcal{X}$ using raised indices, i.e. for . $$ x = x^1e_1 + dots + x^ne_n , equiv ,x^je_j $$the $x^j$ are the $(e_j)$-coordinates of $x$. We are adopting the usual summation convention for tensors. . $ quad$ The basis $(e_j)$ identifies the vector $x$ with the $n$-tuple $(x^1, dots, x^n) in mathbb{R}^n$. It also gives rise to a canonical identification with a dual basis, through the inner product; this identification induces the isomorphism of $ mathcal{X}$ and $ mathcal{X}^*$ mentioned above. Given $e_j$, the associated dual basis vector $e^j$ is given by . $$ e^j = langle e_j, cdot rangle ,. $$ $ quad$ Having assumed an orthogonal representation, the action of any group element, through the representation, takes the $(e^j)$ to some other orthonormal basis, say $( tilde{e}_j)$. This leads to the questions of . how the dual basis $(e^j)$ transforms under the mapping $e^j mapsto langle tilde{e}_j, cdot rangle $, | and of how coordinates $(x^j)_{j=1}^n$ describing vector $x$ transform under this change of basis, which is effectively the same question. | . $ quad$ We can express the basis $( tilde{e}_j )$ in terms of the $( e_j)$ via $$ tilde{e}_j = S^i_je_i , , $$ where $S$ is called the direct transformation matrix from original basis to the new. Let $T$ denote $S^{-1}$, the direct transition matrix from the new basis to the old: $$ e_i = T_i^j tilde{e}_j $$ This inverse determines how the coordinates $x^j$ transform. This is to say, coordinate objects themselves transform contravariantly: $$ tilde{x}^j = T_i^j x^i ,. $$ . To see this, one starts with the two equivalent expressions for $x$, namely $x^i e_i$ and $ tilde{x}^j tilde{e}_j$, and then uses the identity $e_i = T_i^j tilde{e}_j$ on the first expression. . $ quad$ Let us discuss the form of the covector representation described above. We think of the action of group element $g$ as describing a change of coordinates. The vector representation describes this coordinate change directly, as an action on vectors. The covector representation encodes how covectors transform under the same change in coordinates. From the above discussion, we know that this transformation on covectors is contravariant, namely the induced action on covectors is given by $$ langle v , cdot rangle mapsto langle g^{-1}v, cdot rangle , $$ but the inner product allows us to transfer this to yet another action on vectors, through the relation . $$ langle g^{-1} v, w rangle = langle v, g^{-T} w rangle , $$thus the covector representation, defined in the previous section, describes how vectors see the induced contravariant transformation. Conveniently, in the orthogonal setting, the vector and covector representations coincide. . tensors . $ quad$ Recall that the tensor product $ mathcal{X}_1 otimes mathcal{X}_2$ of two vector spaces $ mathcal{X}_1$ and $ mathcal{X}_2$, with respective bases $e_{(1), ,1}, dots, e_{(1), ,m}$ and $e_{(2), ,1}, dots, e_{(2), ,n}$, is the space of formal linear combinations of the symbols $$ e_{(1), ,a} otimes e_{(2), ,b}, $$ meaning that each element of $ mathcal{X}_1 otimes mathcal{X}_2$ can be uniquely expressed in the form . $$ C^{ab} e_{(1), ,a} otimes e_{(2), ,b} $$where $C^{ab}$ are the coordinates of an object we denote $c$, and which is an example of a $2$-tensor. . . . Example. ( changing coordinates with a $2$-tensor ) . $ quad$ Suppose $ rho_1$ is a representation of $ textsf{G}$ over $ mathcal{X}_1$, and that $ rho_2$ is a representation of $ textsf{G}$ over $ mathcal{X}_2$. Suppose that $x = (x^1, dots, x^m) in mathcal{X}_1$ and that $y = (y^1, dots, y^m) in mathcal{X}_2$, so that $x$ and $y$ are each transformed under the $ textsf{G}$ action on each space. By definition, the quantity with components $x^iy^j$ transforms according to the tensor product representation $ rho_1 otimes rho_2$ over $ mathcal{X}_1 otimes mathcal{X}_2$. The representation $ rho_1 otimes rho_2$, acting in $ mathcal{X}_1 otimes mathcal{X}_2$, changes the coordinates $C^{ab}$ according to the following rule; we denote the result of this transformation by $D^{ab}$ $$ D^{ab} = ( rho_1(g))_k^a , ( rho_2(g))_ ell^b , C^{k ell}, $$ where $( rho_1(g))_k^a$ is the matrix of $ rho_1(g)$ in the basis $e_{(1), ,1}, dots, e_{(1), ,m}$, and analogously for the other matrix. . $ quad$ Let us consider this situation, specialized to the case $ mathcal{X}_1 = mathcal{X}$, and $ mathcal{X}_2 = mathcal{X}^*$. Using the above display, which relates the different coordinates representations of a tensor under a change of basis, along with the work of the previous section, one finds in this case $$ D^{ab} = S_k^a , T_ ell^b ,C^{k ell}, $$ where $S$ and $T$ are the change of basis matrices previously discussed. The point is that, because the representation $ rho$ is orthogonal, and because all bases are orthonormal, the action of any $ rho(g)$ for $g in textsf{G}$ functions as a change of basis. We are imagining $ rho(g)$ as $S$. Importantly, the above display can be used to &#39;lift&#39; the representation $ rho$ to the larger vector space $ mathcal{X} otimes mathcal{X}^*$. We index this induced representation by the valency of the tensors under consideration (defined just below). Here, the induced representation is denoted $ rho_{(1,1)}$, and for $g$ as above, $ rho_{(1,1)}(g)$ corresponds to the $4$-tensor $ST$ with entries $S_k^a , T_ ell^b$. . . . . Definition. ( tensors ) . $ quad$ A tensor of type, or valency $(k, ell)$ over vector space $ mathcal{X}$ is a multilinear map $$ A : underbrace{ mathcal{X}^* times , dots , times mathcal{X}^*}_{ k text{ copies }} , times , underbrace{ mathcal{X} times , dots , times mathcal{X} }_{ ell text{ copies }} to mathbb{R} . $$ In instances where the precise valancy is not required to identify $A$, we refer to $A$ as a $(k+ ell)$-tensor. The space of all such objects is denoted $ mathcal{X}[k, ell]$. When expressed in coordinates, the integer $k$ is the number of upper indices and $ ell$ the number of lower indices. . . $ quad$ An orthonormal basis on $ mathcal{X}$ produces an orthonormal basis on any $ mathcal{X}[k, ell]$. Through this induced basis, a tensor $A in mathcal{X}[k, ell]$ has entries $A_{ quad j_1, , dots , , , j_ ell} ^ { i_1, , dots , , , i_k}$, where each index ranges from $1$ to $n$, and where each entry or component takes values in $ mathbb{R}$. These entries are the $(e_j)$-coordinates of tensor $A$. . $ quad$ For coordinate representations of such objects, the transformation rules for coordinates above generalize. Indeed, Schwarz defines tensors, with $k$ upper indices and $ ell$ lower indices, as quantities transforming like a product of $k$ vectors and $ ell$ covectors. This is effectively the definition above. The space $ mathcal{X}[k, ell]$ is isomorphic to $ mathbb{R}^N$, where $N = n^{k + ell}$, and $ textrm{dim}( mathcal{X}) = n$. . $ quad$ The relevance of transforming coordinates is, again, that $ rho(g)$ can be interpreted as a change of coordinates. For a change of coordinates matrix $S equiv rho(g)$ (and its &#39;dual&#39; $T$), the generalized transformation rules are thus a recipe for obtaining the representation $ rho_{(k, ell)}$ over $ mathcal{X}[k, ell]$ from the original representation $ rho$ over $ mathcal{X}$. Here we use $S$ to denote the (orthogonal) direct transformation matrix from one orthonormal basis $(e_j)$ to another $( tilde{e}_j)$, and we use $T$ to denote $S^{-1}$ as before. . $ quad$ Given a tensor $A in mathcal{X}[k, ell]$, we us $A$ and $ tilde{A}$ with accompanying indices to denote the $(e_j)$- and $( tilde{e}_j)$-coordinate representations of $A$. The relation between these coordinate representations is as follows: . $$ tilde{A}_{ quad j_1, , dots , , , j_k} ^ { i_1, , dots , , , i_ ell} = S_{i_1}^{m_1} dots S_{i_ ell}^{m_ ell} T_{n_1}^{j_1} dots T_{n_k}^{j_k} A_{ quad m_1, , dots , , , m_ ell} ^ { n_1, , dots , , , n_k} ,. $$ Does anything need to be said about the endpoint case of $k = ell = 0$? . tensor operations . $ quad$ Aside from the operations which give $ mathcal{X}[k, ell]$ the structure of a vector space, there are two operations betwen such spaces to discuss, for now. The first is called contraction with respect to the $i$th (dual vector) and $j$th (vector) slots. For $k, ell geq 1$, contraction is a map $$ textsf{c} : mathcal{X}[k, ell] to mathcal{X}[k-1, ell-1] $$ given by $$ textsf{c} A = A( dots, e^a, dots ; dots, e_a, dots ), $$ where the repeated indexing denotes a sum indexed by $a$, which runs over the dimensions of $ mathcal{X}$, and where $(e_a)$ is an orthonormal basis for $ mathcal{X}$, with $(e^a)$ the corresponding orthonormal basis for $ mathcal{X}^*$. The $e^a$ are in the $i$th slot of $T$, while the $e_a$ are in the $j$th slot of $T$. The contraction of a tensor of type $(1,1)$, when viewed as a linear map from $V$ to itself, is just the trace of this map. Contraction thus generalizes trace, and both objects are independent of the choice of $e_a$. . $ quad$ The second operation we discuss is the outer product of tensors $A in mathcal{X}[k, ell]$ and $B in mathcal{X}[k&#39;, ell&#39;]$, denoted $A otimes B$. The tensor $A otimes B$ is an element of $ mathcal{X}[ k + k&#39;, ell + ell &#39;]$, defined as follows. Given $(k+k&#39;)$ dual vectors $v^{(1)}, dots, v^{(k+k&#39;)}$ and $( ell + ell&#39;)$ vectors $w_{(1)}, dots, w_{( ell + ell&#39;)}$, we define the action of $A otimes B$ on these objects to be the product $$ A( v^{(1)}, dots, v^{(k)} ; w_{(1)}, dots, w_{( ell)}) cdot B ( v^{(k+1)}, dots, v^{(k + k&#39;)} ; w_{( ell+1)}, dots, w_{( ell + ell&#39;)} ) ,. $$ . GDL blueprint . ( signal domain fixed ) . setup . Our formal treatment of a classification problem requires: . A finite group $ textsf{G}$, the data symmetry group. . | A sequence of discrete domains, $( Omega_j )_{j=0}^L$, with $ Omega equiv Omega_0$ the domain of input signals. . Each $ Omega_j$ is a homogeneous space with respect to $ textsf{G}$, meaning each domain admits a transitive $ textsf{G}$ action, denoted $ xi_j$. In particular, we can write $ Omega_j cong textsf{G} / textsf{H}_j$ for some subgroup $ textsf{H}_j subset textsf{G}$. | . | A sequence of finite dimensional channel vector spaces $( mathcal{C}_j )_{j=0}^L$, with $ mathcal{C}_0 equiv mathcal{C}$ the space of input channels. . The $ xi_j$ induce representions $ rho_j$ over respective signal vector spaces $ mathcal{X}( Omega_j, mathcal{C}_j)$ | . | A collection of orthonormal bases for each signal vector space. Unless stated otherwise, for each $j$, these are assumed to have the form $$ e_{(j), , textrm{space} , ,i} otimes e_{(j), , textrm{channel}, , q} $$ where the index $i$ runs between $1$ and $ # Omega_j$, while index $q$ runs between $1$ and $ textrm{dim}( mathcal{C}_j)$. . | . three flavors of equivariant map . $ quad$ The essential components of the GDL blueprint are $ textsf{G}$-equivariant maps. These are separated into three &#39;types&#39;, discussed below. Each map below is imagined as associated to some layer $j$ of a neural network, though we omit the subscripts for the time being. We consider at most two domains, $ Omega, tilde{ Omega}$ and at most two channel spaces $ mathcal{C}, tilde{ mathcal{C}}$. The signal domains are as described in the setup, and thus can be expressed as $ Omega equiv textsf{G} / textsf{H}$ and $ tilde{ Omega} equiv textsf{G} / tilde{ textsf{H}}$ for subgroups $ textsf{H}, tilde{ textsf{H}} subset textsf{G}$. The cases are . Affine, $ textsf{G}$-equivariant maps, $$ tilde{B} : mathcal{X}( Omega, mathcal{C}) to mathcal{X} ( tilde{ Omega}, tilde{ mathcal{C}}) $$ of the following form: first a generalized convolution with filter $$ tilde{ theta} in mathcal{X}( textsf{H} backslash textsf{G} / tilde{ textsf{H}} , , mathcal{C} otimes tilde{ mathcal{C}}) ,, $$ followed by the addition of a bias vector $ tilde{b} in mathcal{X}( tilde{ Omega}, tilde{ mathcal{C}} )$. The convolution itself can be expressed as a matrix, with respect to basis $$ e_{ textrm{space}, , ell} otimes tilde{e}_{ textrm{space}, k} otimes e_{ textrm{channel}, , q} otimes tilde{e}_{ textrm{channel}, ,r}, $$ where $ ell$ and $k$ range from $1$ to $ # Omega$ and $ # tilde{ Omega}$ respectively. Likewise, $q$ and $r$ range from $1$ to $ textrm{dim}( mathcal{C})$ and $ textrm{dim}( tilde{ mathcal{C}})$ respectively. The entries of this matrix, as well as those of the bias vector $ tilde{b}$ are all learned. . | Non-linear activation function $$ alpha : mathcal{X}( Omega, mathcal{C}) to mathcal{X}( Omega, mathcal{C}) $$ applied &#39;entrywise&#39; to a given input. This implicitly assumes that the input is expressed with respect to the basis $$ e_{ textrm{space}, ell} otimes e_{ textrm{channel}, ,q}, $$ There are no learned parameters associated to this map. Despite its basis dependence, the entrywise application of the nonlinearity is the reason this map is equivariant. . | Equivariant local pooling operators, $$ P : mathcal{X}( Omega, mathcal{C}) to mathcal{X}( tilde{ Omega}, tilde{ mathcal{C}} ) $$ which contain no learnable parameters. These perform a kind of renormalization (or coarse graining) of $ Omega$. These are not in general convolutions, because in particular they are not necessarily linear nor affine. In addition to the hope that such renormalization works as in its physics useage, washing away non-essential information, a smaller output layer usually means fewer learned parameters in the network overall. It is also a way to ensure that the successive map examines the preceding signal with effectively a larger field of view. . | Before discussing some examples of equivariant pooling in the sense of (3.) above, we remark that convolutional layers can effectively function as renormalization operations as well. . We give three examples of equivariant pooling. . . . Example. ( strideless group-pooling ) . $ quad$ Let $ textsf{G}$ be a finite group; an example we will bear in mind is the usual one for images, with $ textsf{G} equiv Omega equiv mathbb{T}_{2, ,n}$. In addition to the domain being the group acting on itself, we additionally identify $ mathbb{T}_{2, ,n}$ with its Cayley graph. Thus the graph distance in the Cayley graph endows $ mathbb{T}_{2, ,n}$ with a natural notion of distance. In general, we write $ textsf{d}_ Omega$ to denote this domain graph-distance, and we let $ textsf{B}_ Omega(v_g,r)$ for $r in mathbb{N}$ denote the ball around $v_g in Omega$ of radius $r$, with respect to $ textsf{d}_ Omega$. . $ quad$ Consider $ textsf{B}_ Omega(v_e,r)$, the ball of radius $r$ centered at the &#39;basepoint&#39; $v_e$ of $ Omega$ corresponding to the group identity $e$. Let us denote this neighborhood as $ textsf{B}_r$ for brevity. The following defines a pooling operation $P$ on signals $x in mathcal{X}( Omega, mathcal{C})$: $$ Px(v_g) = x left( textrm{argmax}_{h , in , g. textsf{B}_r } | x(h) |_{ mathcal{C}} right), $$ where $ | cdot |_{ mathcal{C} }$ denotes some norm on the channel vector space $ mathcal{C}$. . . . . . Example. ( subsampling ) . . $ quad$ . . . . . Example. ( coset pooling ) . . $ quad$ . . . hypothesis space . $ quad$ . discussion . Shift-invariance arises naturally in vision and pattern recognition. In this case, the desired function $f in textsf{H}$, typically implemented as a CNN, inputs an image and outputs the probability of the image to contain an object from a certain class. It is often reasonably assumed that the classification result should not be affected by the position of the object in the image, i.e., the function $f$ must be shift-invariant. . Multi-layer perceptrons lack this property, a reason why early (1970s) attempts to apply these architectures to pattern recognition problems failed. The development of NN architectures with local weight sharing, as epitomized by CNNs, was among other reasons motivated by the need for shift-invariant object classification. . A prototypical application requiring shift-equivariance is image segmentation, where the output of $f$ is a pixel-wise image mask. This segmentation mask must follow shifts in the input image. In this example, the domains of the input and output are the same, but since the input has three color channels while the output has emph{one channel per class}, the representations $( rho, mathcal{X}( Omega, mathcal{C}) )$ and $( rho&#39;, mathcal{Y} equiv mathcal{X}( Omega, mathcal{C}&#39;))$ are somewhat different. . When $f$ is implemented as a CNN, it may be written as a composition of $L$ functions, where $L$ is determined by the depth and other hyperparameters: $$ f = f_L circ f_{L-1} circ dots circ f_2 circ f_1 . $$ . Examining the individual layer functions making up CNN, one finds they are not shift-invariant in general but rather shift-equivariant. The last function applied, namely $f_L$, is typically a ``global-pooling&quot; function that is genuinely shift-invariant, causing $f$ to be shift-invariant, but to focus on this ignores the structure we will leverage for purposes of expressivity and regularity. .",
            "url": "https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2021/10/21/GDL1.html",
            "relUrl": "/jupyter/2021/10/21/GDL1.html",
            "date": " • Oct 21, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://the-ninth-wave.github.io/geometric-deep-learning/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://the-ninth-wave.github.io/geometric-deep-learning/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}