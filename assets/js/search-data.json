{
  
    
        "post0": {
            "title": "Lecture 1 ... Introduction",
            "content": ". Example (Mixed $p$-spin models). $ quad$ Let $ Sigma_N$ denote either the hypercube $ { pm 1 }^N$ or the sphere . $$ mathbb{S}_N triangleq { x in mathbb{R}^N : | x |^2 = N }, $$which contains the hypercube. Let $( beta_p)_{p geq 1}$ be a sequence of non-negative numbers, and we suppose . $$ sum_{p geq 1} beta_p^2 &lt; infty . $$Given such a sequence, we define the model function $ xi : mathbb{R} to mathbb{R}$ via . $$ xi(x) triangleq sum_{p geq 1} beta_p^2 x^p $$as well as the associated Hamiltonian $H_N : mathbb{S}_N to mathbb{R}$, where for $s in mathbb{S}$, . $$ H_N(s) := sum_{p geq 1} beta_p N^{- (p-1)/2} sum_{ i_1, dots, i_p } g_{i_1, dots, i_p} s_{i_1} cdot dots cdot s_{i_p}, $$where the $g_{i_1, dots, i_p}$ are i.i.d. standard normal random variables. The model function and the associated Hamiltonian constitute the mixed $p$-spin model associated to $( beta_p)$. . The Hamiltonian is a random function on $ mathbb{S}_N$; the model coefficients $ beta_p$ are taken to be square-summable to ensure that the object $H_N$ is a.s. smooth. . The covariance structure of $H_N$ is a function of the sphere&#39;s geometry: letting $ langle cdot, cdot rangle$ denote the Euclidean inner product, . $$ mathbb{E} H_N (s) H_N(t) = N xi left( N^{-1} langle s , t rangle right) $$ . Let us write . $$ R_{1 text{-}2} := N^{-1} langle s, t rangle equiv frac{ langle s, t rangle } { |s | |t | } $$when $s$ and $t$ are implicit, we refer to $R_{1 text{-}2}$ as their overlap. . When $ xi(x) = x^p$, the model is called the pure $p$-spin model. The pure $2$-spin model is more commonly known as the Sherrington-Kirkpatrick model. We prepend &quot;spherical&quot; to the model name when $ Sigma_N = mathbb{S}_N$ and omit this otherwise. . In any case, we regard $H_N$ as an energy landscape. Here are examples of questions one can ask about $H_N$: . Can we describe the asymptotic topology of the sublevel sets of these models? . | What is the asymptotic ground state energy of such models, i.e., can one show for some $c &lt; 0$ that . | . $$ min_{s in mathbb{S}} frac{H_N(s)}{N} to c quad text{a.s.} , ? $$ Given an inverse temperature parameter $ beta &gt;0 $ (not related to the $ beta_p$), we can form the Gibbs measure $ mu_{N, beta}$ associated to the Hamiltonian $H_N$ and this $ beta$. We suppress the dependence of $ mu_{N, beta}$ on the model function $ xi$. . The Gibbs measure $ mu_{N, beta}$ is thus a random probability measure on the state space $ Sigma_N$. . When $ Sigma_N = mathbb{S}_N$ let us conflate the Gibbs measure $ mathcal{G}_{N, beta}$ with the density which defines it, notationally. The density is with respect to the natural volume measure $ textrm{vol}_N$ on $ mathbb{S}_N$. . $$ mathcal{G}_{N, beta}(s) := frac{1}{ mathcal{Z}_{N, beta} } exp left( - beta H_N(s) right) , textrm{vol}_N( textrm{d} s), $$ where $ mathcal{Z}_{N, beta}$ is the partition function of $ mathcal{G}_{N, beta}$, a random normalizing constant of fundamental importance in statistical mechanics. . When $ Sigma_N = { pm 1 }$, the Gibbs measure $ mathcal{G}_{N, beta}$ is defined analogously. Examples of questions one can ask about these new objects: . Can one compute the limiting free energy of the model, $$ lim_{N to infty} frac{1}{N} log mathcal{Z}_{N, beta} $$ if it exists? | . In the setting of the pure $p$-spin model, for instance, fix a realization of the disorder $g_{i_1, dots, i_p}$. For $ beta &gt;0$ given, consider the associated Gibbs measure $ mathcal{G}_{N, beta}$. . Let $ mathcal{G}_{N, beta}^{ otimes 2}$ denote the law of a pair of configurations (elements $s,t$ of the state space $ Sigma_N$), where the two random variables are sampled from $ mathcal{G}_{N, beta}$, independently of one another. . For such a pair $(s,t) equiv (s_{N, beta}, t_{N, beta}) sim mathcal{G}_{N, beta}^{ otimes 2}$, we can form the overlap random variable . $$ R_{1 text{-}2} triangleq R_{1 text{-}2}(s,t) $$ What can be said about the asymptotic behavior of this random variable, especially as a function of $ beta$? | . Studying the overlap random variable to study the associated Gibbs measure is a way to leverage a phrase I hear repeated by experts, roughly: &quot;we don&#39;t know the ground states of the system, but the system does.&quot; .",
            "url": "https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2020/05/04/GDL4.html",
            "relUrl": "/jupyter/2020/05/04/GDL4.html",
            "date": " • May 4, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Lecture 1 ... Introduction",
            "content": ". Example (Mixed $p$-spin models). $ quad$ Let $ Sigma_N$ denote either the hypercube $ { pm 1 }^N$ or the sphere . $$ mathbb{S}_N triangleq { x in mathbb{R}^N : | x |^2 = N }, $$which contains the hypercube. Let $( beta_p)_{p geq 1}$ be a sequence of non-negative numbers, and we suppose . $$ sum_{p geq 1} beta_p^2 &lt; infty . $$Given such a sequence, we define the model function $ xi : mathbb{R} to mathbb{R}$ via . $$ xi(x) triangleq sum_{p geq 1} beta_p^2 x^p $$as well as the associated Hamiltonian $H_N : mathbb{S}_N to mathbb{R}$, where for $s in mathbb{S}$, . $$ H_N(s) := sum_{p geq 1} beta_p N^{- (p-1)/2} sum_{ i_1, dots, i_p } g_{i_1, dots, i_p} s_{i_1} cdot dots cdot s_{i_p}, $$where the $g_{i_1, dots, i_p}$ are i.i.d. standard normal random variables. The model function and the associated Hamiltonian constitute the mixed $p$-spin model associated to $( beta_p)$. . The Hamiltonian is a random function on $ mathbb{S}_N$; the model coefficients $ beta_p$ are taken to be square-summable to ensure that the object $H_N$ is a.s. smooth. . The covariance structure of $H_N$ is a function of the sphere&#39;s geometry: letting $ langle cdot, cdot rangle$ denote the Euclidean inner product, . $$ mathbb{E} H_N (s) H_N(t) = N xi left( N^{-1} langle s , t rangle right) $$ . Let us write . $$ R_{1 text{-}2} := N^{-1} langle s, t rangle equiv frac{ langle s, t rangle } { |s | |t | } $$when $s$ and $t$ are implicit, we refer to $R_{1 text{-}2}$ as their overlap. . When $ xi(x) = x^p$, the model is called the pure $p$-spin model. The pure $2$-spin model is more commonly known as the Sherrington-Kirkpatrick model. We prepend &quot;spherical&quot; to the model name when $ Sigma_N = mathbb{S}_N$ and omit this otherwise. . In any case, we regard $H_N$ as an energy landscape. Here are examples of questions one can ask about $H_N$: . Can we describe the asymptotic topology of the sublevel sets of these models? . | What is the asymptotic ground state energy of such models, i.e., can one show for some $c &lt; 0$ that . | . $$ min_{s in mathbb{S}} frac{H_N(s)}{N} to c quad text{a.s.} , ? $$ Given an inverse temperature parameter $ beta &gt;0 $ (not related to the $ beta_p$), we can form the Gibbs measure $ mu_{N, beta}$ associated to the Hamiltonian $H_N$ and this $ beta$. We suppress the dependence of $ mu_{N, beta}$ on the model function $ xi$. . The Gibbs measure $ mu_{N, beta}$ is thus a random probability measure on the state space $ Sigma_N$. . When $ Sigma_N = mathbb{S}_N$ let us conflate the Gibbs measure $ mathcal{G}_{N, beta}$ with the density which defines it, notationally. The density is with respect to the natural volume measure $ textrm{vol}_N$ on $ mathbb{S}_N$. . $$ mathcal{G}_{N, beta}(s) := frac{1}{ mathcal{Z}_{N, beta} } exp left( - beta H_N(s) right) , textrm{vol}_N( textrm{d} s), $$ where $ mathcal{Z}_{N, beta}$ is the partition function of $ mathcal{G}_{N, beta}$, a random normalizing constant of fundamental importance in statistical mechanics. . When $ Sigma_N = { pm 1 }$, the Gibbs measure $ mathcal{G}_{N, beta}$ is defined analogously. Examples of questions one can ask about these new objects: . Can one compute the limiting free energy of the model, $$ lim_{N to infty} frac{1}{N} log mathcal{Z}_{N, beta} $$ if it exists? | . In the setting of the pure $p$-spin model, for instance, fix a realization of the disorder $g_{i_1, dots, i_p}$. For $ beta &gt;0$ given, consider the associated Gibbs measure $ mathcal{G}_{N, beta}$. . Let $ mathcal{G}_{N, beta}^{ otimes 2}$ denote the law of a pair of configurations (elements $s,t$ of the state space $ Sigma_N$), where the two random variables are sampled from $ mathcal{G}_{N, beta}$, independently of one another. . For such a pair $(s,t) equiv (s_{N, beta}, t_{N, beta}) sim mathcal{G}_{N, beta}^{ otimes 2}$, we can form the overlap random variable . $$ R_{1 text{-}2} triangleq R_{1 text{-}2}(s,t) $$ What can be said about the asymptotic behavior of this random variable, especially as a function of $ beta$? | . Studying the overlap random variable to study the associated Gibbs measure is a way to leverage a phrase I hear repeated by experts, roughly: &quot;we don&#39;t know the ground states of the system, but the system does.&quot; .",
            "url": "https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2020/05/03/GDL3.html",
            "relUrl": "/jupyter/2020/05/03/GDL3.html",
            "date": " • May 3, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Lecture 1 ... Introduction",
            "content": ". Example (Mixed $p$-spin models). $ quad$ Let $ Sigma_N$ denote either the hypercube $ { pm 1 }^N$ or the sphere . $$ mathbb{S}_N triangleq { x in mathbb{R}^N : | x |^2 = N }, $$which contains the hypercube. Let $( beta_p)_{p geq 1}$ be a sequence of non-negative numbers, and we suppose . $$ sum_{p geq 1} beta_p^2 &lt; infty . $$Given such a sequence, we define the model function $ xi : mathbb{R} to mathbb{R}$ via . $$ xi(x) triangleq sum_{p geq 1} beta_p^2 x^p $$as well as the associated Hamiltonian $H_N : mathbb{S}_N to mathbb{R}$, where for $s in mathbb{S}$, . $$ H_N(s) := sum_{p geq 1} beta_p N^{- (p-1)/2} sum_{ i_1, dots, i_p } g_{i_1, dots, i_p} s_{i_1} cdot dots cdot s_{i_p}, $$where the $g_{i_1, dots, i_p}$ are i.i.d. standard normal random variables. The model function and the associated Hamiltonian constitute the mixed $p$-spin model associated to $( beta_p)$. . The Hamiltonian is a random function on $ mathbb{S}_N$; the model coefficients $ beta_p$ are taken to be square-summable to ensure that the object $H_N$ is a.s. smooth. . The covariance structure of $H_N$ is a function of the sphere&#39;s geometry: letting $ langle cdot, cdot rangle$ denote the Euclidean inner product, . $$ mathbb{E} H_N (s) H_N(t) = N xi left( N^{-1} langle s , t rangle right) $$ . Let us write . $$ R_{1 text{-}2} := N^{-1} langle s, t rangle equiv frac{ langle s, t rangle } { |s | |t | } $$when $s$ and $t$ are implicit, we refer to $R_{1 text{-}2}$ as their overlap. . When $ xi(x) = x^p$, the model is called the pure $p$-spin model. The pure $2$-spin model is more commonly known as the Sherrington-Kirkpatrick model. We prepend &quot;spherical&quot; to the model name when $ Sigma_N = mathbb{S}_N$ and omit this otherwise. . In any case, we regard $H_N$ as an energy landscape. Here are examples of questions one can ask about $H_N$: . Can we describe the asymptotic topology of the sublevel sets of these models? . | What is the asymptotic ground state energy of such models, i.e., can one show for some $c &lt; 0$ that . | . $$ min_{s in mathbb{S}} frac{H_N(s)}{N} to c quad text{a.s.} , ? $$ Given an inverse temperature parameter $ beta &gt;0 $ (not related to the $ beta_p$), we can form the Gibbs measure $ mu_{N, beta}$ associated to the Hamiltonian $H_N$ and this $ beta$. We suppress the dependence of $ mu_{N, beta}$ on the model function $ xi$. . The Gibbs measure $ mu_{N, beta}$ is thus a random probability measure on the state space $ Sigma_N$. . When $ Sigma_N = mathbb{S}_N$ let us conflate the Gibbs measure $ mathcal{G}_{N, beta}$ with the density which defines it, notationally. The density is with respect to the natural volume measure $ textrm{vol}_N$ on $ mathbb{S}_N$. . $$ mathcal{G}_{N, beta}(s) := frac{1}{ mathcal{Z}_{N, beta} } exp left( - beta H_N(s) right) , textrm{vol}_N( textrm{d} s), $$ where $ mathcal{Z}_{N, beta}$ is the partition function of $ mathcal{G}_{N, beta}$, a random normalizing constant of fundamental importance in statistical mechanics. . When $ Sigma_N = { pm 1 }$, the Gibbs measure $ mathcal{G}_{N, beta}$ is defined analogously. Examples of questions one can ask about these new objects: . Can one compute the limiting free energy of the model, $$ lim_{N to infty} frac{1}{N} log mathcal{Z}_{N, beta} $$ if it exists? | . In the setting of the pure $p$-spin model, for instance, fix a realization of the disorder $g_{i_1, dots, i_p}$. For $ beta &gt;0$ given, consider the associated Gibbs measure $ mathcal{G}_{N, beta}$. . Let $ mathcal{G}_{N, beta}^{ otimes 2}$ denote the law of a pair of configurations (elements $s,t$ of the state space $ Sigma_N$), where the two random variables are sampled from $ mathcal{G}_{N, beta}$, independently of one another. . For such a pair $(s,t) equiv (s_{N, beta}, t_{N, beta}) sim mathcal{G}_{N, beta}^{ otimes 2}$, we can form the overlap random variable . $$ R_{1 text{-}2} triangleq R_{1 text{-}2}(s,t) $$ What can be said about the asymptotic behavior of this random variable, especially as a function of $ beta$? | . Studying the overlap random variable to study the associated Gibbs measure is a way to leverage a phrase I hear repeated by experts, roughly: &quot;we don&#39;t know the ground states of the system, but the system does.&quot; .",
            "url": "https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2020/05/02/GDL2.html",
            "relUrl": "/jupyter/2020/05/02/GDL2.html",
            "date": " • May 2, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "1",
            "content": "Modern neural network (NN) design is built on two algorithmic principles: hierarchical feature learning ( concerning the architecture of the NN ), and learning by local gradient-descent driven by backpropagation ( concerning the learning dynamics undergone by the NN ). . An instance of training data is modeled as an element of some high-dimensional vector space, making a generic learning problem subject to the curse of dimensionality. Fortunately, most tasks of interest are not generic, inheriting regularities from the underlying low-dimensionality and structure of the physical world. . Exploiting known symmetries of a large system is a useful, classical remedy against the curse of dimensionality, and forms the basis of most physical theories. The notes [BBCV21] construct a blueprint for neural network architecture which incorporates these ``physical&quot; priors, termed geometric priors throughout the notes. Importantly, this blueprint provides a unified perspective of the most successful neural network architectures. . 1.1 categories and groups . . Def $ quad$ A graph is a pair $ mathcal{G} = ( mathcal{V}, mathcal{E})$, where $ mathcal{V}$ is a set whose elements are called vertices. The set $ mathcal{E}$ consists of edges, defined to be a multi-set of exactly two vertices in $ mathcal{V}$, not necessarily distinct. . . Def $ quad$ A directed graph is a pair of sets $ mathcal{G} = ( mathcal{V}, mathcal{A})$ of vertices and arrows (or directed edges). An arrow is an ordered pair of vertices. . . Def $ quad$ Consider an arrow $f$ of a directed graph $ mathcal{G} = ( mathcal{V}, mathcal{A})$, specifically $f equiv (a,b) in mathcal{A}$, with $a,b in mathcal{V}$. The operations $ mathbb{dom}$ and $ mathbb{cod}$ act on the arrows $f in mathcal{A}$ via $ mathbb{dom}f = a, , mathbb{cod} f = b$, and are called the domain operation and codomain operation, respectively. . . $ vdots$ . Given two arrows, $f$ and $g$ in some directed graph, we say that the ordered pair of arrows $(g,f)$ is a composable pair if $ mathbb{dom} g = mathbb{cod} f$. Going forward, let us express the relations $a = mathbb{dom} f$ and $b = mathbb{cod} f$ more concisely via $$ f : a to b , quad text { or equivalently, } , quad a xrightarrow[ ,]{ f} b $$ . The next definition formalizes the behavior of a collection of structure-respecting maps between mathematical objects. . $ vdots$ . . Def $ quad$ A category is a directed graph $ mathcal{C} = ( mathcal{O}, mathcal{A})$, whose vertices $ mathcal{O}$ we call objects, such that . For each object $a in mathcal{O}$, there is a unique identity arrow $ textrm{id}_a equiv mathbf{1}_a : a to a$, defined by the following property: for all arrows $f : b to a$ and $g : a to c$, composition with the identity arrow $ mathbf{1}_a $ gives $$ mathbf{1}_a circ f = f quad text{ and } quad g circ mathbf{1}_a = g $$ . | For each composable pair $(g, f)$ of arrows, there is a unique arrow $g circ f$ called their composite, with $g circ f : mathbb{dom} f to mathbb{cod} g$, such that the composition operation is associative. Namely, for given objects and arrows in the configuration $a xrightarrow[ ,]{ f} b xrightarrow[ ,]{ g} c xrightarrow[ ,]{ k} d$, one always has the equality $k circ (g circ f) = (k circ g ) circ f$. . | . $ vdots$ . Given a category $ mathcal{C} = ( mathcal{O}, mathcal{A})$, let $$ mathbb{hom} (b,c) := { , f : f in mathcal{A}, , mathbb{dom} f = b in mathcal{O}, , mathbb{cod} f = c in mathcal{O} , } $$ denote the set of arrows from $b$ to $c$. Henceforth, we use the terms morphism and arrow interchangeably. . Groups are collections of symmetries. A group $G$ is a category $ mathcal{C} = ( mathcal{O}, mathcal{A})$ with $ mathcal{O} = { o }$ ( so that we may identify $G$ with the collection of arrows $ mathcal{A}$ ) such that each arrow has a unique inverse: for $g in mathcal{A}$, there is an arrow $h$ such that $g circ h = h circ g = mathbf{1}_o$. . Each arrow $g in mathcal{A}$ thus has $ mathbb{dom} g = mathbb{cod} g = o$. As remarked, the arrows $g in mathcal{A}$ correspond to group elements $g in G$. The categorical interpretation suggests that the group emph{acts} on some abstract object $o in mathcal{O}$. In the present context, we care how groups act on data, and how this action is represented to a computer. . $ quad$ group representations . Linear representation theory allows us to study groups using linear algebra (a source ). We start by considering a function $ varphi : G times V to V$, where $G$ is a group, and where $V$ is a vector space over $ mathbb{R}$. This allows us to identify group elements $g$ with functions $ varphi(g, cdot) : V to V$ from the vector space to itself. When the map $ varphi$ is understood, or general ( as now ), we write $g.v$ in place of $ varphi(g,v)$, and we write $(g.)$ in place of $ varphi(g, cdot)$. . The &quot;representatives&quot; $(g.)$ of these group elements $g$ can be composed, and if this compositional structure is compatible with the original group operation, we say $ varphi$ is a group action on $V$. Specifically, $ varphi$ should satisfy $e.v = v$ for all $v in V$, where $e$ denotes the identity element of $G$, and in general one has $(gh).v = g.(h.v)$. . The map $ varphi$ is $ mathbb{R}$-linear if it is compatible with the $ mathbb{R}$-vector space structure on $V$, i.e. additive and homogeneous. Specifically, if for all $v,w in V$ and all scalars $ lambda in mathbb{R}$, one has $g.(v+w) = g.v + g.w$ and $g.( lambda v) = lambda g.v$. . $ vdots$ . . Def $ quad$ An $ mathbb{R}$-linear representation of group $G$ over $ mathbb{R}$-vector space $V$ is an $ mathbb{R}$-linear group action on $V$. . . $ vdots$ . The next example illustrates how linear group representations arise naturally when considering group actions on data. As mentioned, we consider input data as members of some vector space $V$, which we may assume to be finite dimensional for any practical discussion. Specifically, we consider some finite, discrete domain $ Omega$, which may also have the structure of an undirected graph. . A emph{ B{signal}} over $ Omega$ is a function $x : Omega to mathbb{R}^s$, where $s$ is the number of channels. The vector space $ mathcal{X}( Omega, mathbb{R}^s)$ is defined to be the collection of all such signals, for given $ Omega$ and $s$. . $ vdots$ . . . Example $ quad$ Consider, for some $n in mathbb{N}$, a signal domain $ Omega = mathbb{T}_n^2$, where $ mathbb{T}_n^2$ denotes the two-dimensional discrete torus of side-length $n$, namely $( mathbb{Z} / n mathbb{Z} )^2$. This domain has natural graph as well as group structures. . If we imagine each vertex of $ Omega$ to be a pixel, we can express an $n times n$-pixel color (RGB) image as a signal $x : Omega to mathbb{R}^3$, with the first, second and third coordinates of $ mathbb{R}^3$ reporting R, G and B values of a given pixel. . We make two observations: . As a vector space, $ mathcal{X}( Omega)$ is isomorphic to $ mathbb{R}^d$, with $d$ typically very large. In the above example, $d = 3n^2$, which is thirty-thousand for a $n times n equiv 100 times 100$ pixel image. . | Any group action on $ Omega$ induces a group action on $ mathcal{X}( Omega)$. . | Expanding on the latter, consider a group action of $G$ on domain $ Omega$. As the torus $ Omega$ already has group structure, it is natural to think of it acting on itself through translations, i.e. we now additionally consider $G = mathbb{T}_n^2$. . The action of $G equiv mathbb{T}_n^2$ on itself $ Omega equiv mathbb{T}_n^2$ induces a $G$-action on $ mathcal{X}( Omega)$ as follows: for $g in G$ signal $x in mathcal{X}( Omega)$, the action $(g, x) mapsto mathbf{g}.x in mathcal{X}( Omega)$ is defined pointwise at each $u in Omega$: $$ ( mathbf{g}.x)(u) := x(g. omega), $$ where the bold $( mathbf{g}.)$ is used to distinguish the action on signals from the action on the domain. . . . $ vdots$ . To summarize: any $G$-action on the domain $ Omega$ induces an $ mathbb{R}$-linear representation of $G$ over the vector space of signals on $ Omega$. . $ vdots$ . . . Example $ quad$ It seems like standard practice to encode the collection of classes associated to some ML classification problem as an orthonormal basis. These are given ( to the computer ) in the usual coordinate basis $$ e_1 equiv (1, 0, dots, 0), , e_2 equiv (0,1, dots, 0), , dots, , e_n equiv (0, dots, 0,1) ,, $$ hence the nomenclature one-hot. In the preceding example, if one considers a one-hot encoding of the vertices of $ mathbb{T}_n^2$, we see that each signal is expressed with respect to this coordinate system, in the sense that $x = sum_{j=1}^n x_j e_j$. . This kind of encoding is useful for considering general symmetries of the domain. For instance, if permuting node labels is a relevant symmetry, the action of the symmetric group $ frak{S}_n$ is naturally represented by $n times n$ permutation matrices. . . . $ vdots$ . The following definition reformulates the notion of a signal over the nodes of some graph as node features. . $ vdots$ . . Def $ quad$ We say a graph $ mathcal{G} = ( mathcal{V}, mathcal{E} )$ is equipped with emph{ B{node features}} if for each $v in mathcal{V}$, one has the additional data of an $s$-dimensional vector $x(v) in mathbb{R}^s$, called the features of node $v$. . . $ vdots$ . The term &#39;features&#39; is compatible with the usage in ML, supposing that our input signal has domain some graph $ mathcal{G}$. In this case, we can think of a neural network as a sequence of node-layers built ``on top of&quot; the graph $ mathcal{G}$. An input signal endows the first node layer of a NN with features, and the weights of the neural network propagate these through to node features on the nodes of the rest of the network. The features on the last layer of the network can be read off as the output of the NN function. .",
            "url": "https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2020/05/01/GDL1.html",
            "relUrl": "/jupyter/2020/05/01/GDL1.html",
            "date": " • May 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://the-ninth-wave.github.io/geometric-deep-learning/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://the-ninth-wave.github.io/geometric-deep-learning/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}