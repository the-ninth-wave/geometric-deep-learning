{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2021-10-22-GDL2.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"}},"cells":[{"cell_type":"markdown","metadata":{"id":"XZuXYfmJPu3l"},"source":["# 2\n","> GDL\n","\n","- toc: true \n","- badges: true\n","- comments: false\n","- categories: [jupyter]\n"]},{"cell_type":"markdown","metadata":{"id":"e6t_Gqemxl1t"},"source":["# geometric deep learning models "]},{"cell_type":"markdown","metadata":{"id":"tQPGhUITsfZ0"},"source":["## <font color=\"CornflowerBlue\">2.1 ... tensors in pytorch</font>"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":400},"id":"ZosyKQ2wsne5","executionInfo":{"status":"ok","timestamp":1635089593470,"user_tz":300,"elapsed":131991,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"388572ed-b458-4c8b-aed9-745b1e70ab19"},"source":["pip install torch --upgrade"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu111)\n","Collecting torch\n","  Downloading torch-1.10.0-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n","\u001b[K     |██████████████████████████████▎ | 834.1 MB 1.2 MB/s eta 0:00:42tcmalloc: large alloc 1147494400 bytes == 0x55e98d60e000 @  0x7f82bf648615 0x55e9534e44cc 0x55e9535c447a 0x55e9534e72ed 0x55e9535d8e1d 0x55e95355ae99 0x55e9535559ee 0x55e9534e8bda 0x55e95355ad00 0x55e9535559ee 0x55e9534e8bda 0x55e953557737 0x55e9535d9c66 0x55e953556daf 0x55e9535d9c66 0x55e953556daf 0x55e9535d9c66 0x55e953556daf 0x55e9534e9039 0x55e95352c409 0x55e9534e7c52 0x55e95355ac25 0x55e9535559ee 0x55e9534e8bda 0x55e953557737 0x55e9535559ee 0x55e9534e8bda 0x55e953556915 0x55e9534e8afa 0x55e953556c0d 0x55e9535559ee\n","\u001b[K     |████████████████████████████████| 881.9 MB 16 kB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n","Installing collected packages: torch\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.9.0+cu111\n","    Uninstalling torch-1.9.0+cu111:\n","      Successfully uninstalled torch-1.9.0+cu111\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.10.0+cu111 requires torch==1.9.0, but you have torch 1.10.0 which is incompatible.\n","torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.10.0 which is incompatible.\u001b[0m\n","Successfully installed torch-1.10.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["torch"]}}},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"S-LV_wGXsuvE","executionInfo":{"status":"ok","timestamp":1635089593778,"user_tz":300,"elapsed":315,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}}},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.utils.data as data"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cX0uHvDgsvnP","executionInfo":{"status":"ok","timestamp":1635089593780,"user_tz":300,"elapsed":14,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"e648a468-b644-4457-c3f5-ccb4490c4a22"},"source":["torch.manual_seed(42)\n","print(\"Using torch\", torch.__version__)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Using torch 1.9.0+cu111\n"]}]},{"cell_type":"markdown","metadata":{"id":"kvKKEvc-3a2k"},"source":["### <font color=\"teal\">... helper classes and functions </font>"]},{"cell_type":"code","metadata":{"id":"aF4K6jD7e8GR","executionInfo":{"status":"ok","timestamp":1635089593780,"user_tz":300,"elapsed":8,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}}},"source":["from PIL import Image\n","from torchvision import transforms"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wft7v5Cyc1q2","executionInfo":{"status":"ok","timestamp":1635089599768,"user_tz":300,"elapsed":5995,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"52d5437f-ec55-4674-ae93-fdb31931deaa"},"source":["!pip install tabletext"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tabletext\n","  Downloading tabletext-0.1.tar.gz (6.1 kB)\n","Building wheels for collected packages: tabletext\n","  Building wheel for tabletext (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tabletext: filename=tabletext-0.1-py3-none-any.whl size=6022 sha256=aa9d9579b7a4b73606e8bfcc3de7c5b82bdac6ff7e33c90d90e28fad443d8760\n","  Stored in directory: /root/.cache/pip/wheels/cc/ae/ab/697f6cd9887c63663da889f796c2c7ea280bc407b16f6fd081\n","Successfully built tabletext\n","Installing collected packages: tabletext\n","Successfully installed tabletext-0.1\n"]}]},{"cell_type":"code","metadata":{"id":"K4BKiUW4c5R-","executionInfo":{"status":"ok","timestamp":1635089599769,"user_tz":300,"elapsed":18,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}}},"source":["from tabletext import to_text"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kdHDqVYPS8iZ"},"source":["___\n","\n","#### method ... `info`\n","\n","_can currently handle 1- and 2-tensors_"]},{"cell_type":"code","metadata":{"id":"4CvwztRRSKPI","executionInfo":{"status":"ok","timestamp":1635093897418,"user_tz":300,"elapsed":151,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}}},"source":["#collapse-hide\n","\n","def info(obj, name):\n","\n","    ty_tens = type( torch.tensor([2]) )\n","\n","    ty_np = type( np.array([2]) )\n","\n","    delim = \"   \"\n","\n","    if obj.ndim == 1:\n","\n","        display_obj = obj[None,:]\n","\n","    elif obj.ndim == 2:\n","\n","        display_obj = obj\n","    else:\n","        display_obj = obj\n","\n","    if type(obj) == ty_tens:\n","\n","        print( \"tensor\", delim, name, \"\\n\" )\n","\n","        print( delim, \"num. dims   \", delim, obj.ndim )\n","\n","        print( delim, \"num. entries\", delim, np.array( obj ).size )\n","\n","        print( delim, \"shape       \", delim, np.array( obj.size() ) )\n","\n","    if type(obj) == ty_np:\n","\n","        print( \"np array\", delim, name, \"\\n\" )\n","\n","        print( delim, \"number of dimensions\", obj.ndim )\n","\n","        print( delim, \"number of entries\", delim, obj.size )\n","\n","        print( delim, \"shape\", delim, obj.shape )\n","\n","    print(\"\\n\")\n","\n","    display_list = display_obj.tolist()\n","\n","    J = len( display_list )\n","    K = len( display_list[0])\n","\n","    outer_list = []\n","\n","    for j in range(J):\n","\n","        inner_list = []\n","\n","        for k in range(K):\n","\n","            inner_list += [ str( display_list[j][k] )[:4] ]\n","\n","        outer_list += [ inner_list ]\n","\n","    print( to_text( outer_list ) )\n","\n","    print(\"\\n\\n\")"],"execution_count":97,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kw4sHooPSKY3"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"WZ4ZHjHsDpzL"},"source":["___\n","\n","#### method ... `viz_tens`\n","\n","args\n","\n","* `tens`\n","\n","* `display_size`"]},{"cell_type":"code","metadata":{"id":"OoDw8dGfDp9x","executionInfo":{"status":"ok","timestamp":1635089599771,"user_tz":300,"elapsed":15,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}}},"source":["#collapse-hide\n","def viz_tens( tens, display_size = 2 ):\n","\n","    size = display_size\n","\n","    if tens.ndim == 1:\n","\n","        display_obj = tens[None,:]\n","\n","        display_obj = display_obj.float()\n","\n","    elif tens.ndim == 2:\n","\n","        display_obj = tens\n","\n","        display_obj = display_obj.float()\n","\n","    pil_image = transforms.ToPILImage()( display_obj ).convert(\"RGB\")\n","\n","    fig = plt.figure( figsize = (size, size) )\n","\n","    f_rows, f_cols = 1, 1\n","\n","    fig.add_subplot( f_rows, f_cols, 1 )\n","\n","    plt.tick_params( left = False, bottom = False )\n","\n","    plt.axis('off')\n","\n","    plt.imshow(pil_image)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rt5ZSh3hDqHf"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"c0Sde9xlCy2g"},"source":["___\n","\n","#### method ... `viz_tens_list`\n","\n","args\n","\n","* `list_of_tensors`\n","\n","* `display_size = 6`"]},{"cell_type":"code","metadata":{"id":"97XsQAdKzzmP","executionInfo":{"status":"ok","timestamp":1635089599772,"user_tz":300,"elapsed":15,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}}},"source":["#collapse-hide\n","def viz_tens_list( list_of_tensors, display_size = 6 ):\n","\n","    size = display_size\n","\n","    image_list = [ transforms.ToPILImage()(x).convert(\"RGB\") for x in list_of_tensors ]\n","\n","    tensor_list = [ transforms.ToTensor()(image) for image in image_list]\n","\n","    grid = torchvision.utils.make_grid( tensor_list, padding = 2, pad_value = 1.0 )\n","\n","    grid_pil = transforms.ToPILImage()(grid).convert(\"RGB\")\n","\n","    fig = plt.figure( figsize = (size, size) )\n","\n","    f_rows, f_cols = 1, 1\n","\n","    fig.add_subplot( f_rows, f_cols, 1 )\n","\n","    plt.tick_params( left = False, bottom = False )\n","\n","    plt.axis('off')\n","\n","    plt.imshow(grid_pil)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HKxCdUGbaEQq"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"QFT00Jsas_l2"},"source":["### <font color=\"teal\">... initializing tensors</font>"]},{"cell_type":"markdown","metadata":{"id":"8Z5-7ZoXtXXv"},"source":["Tensors are similar to NumPy's `ndarrays`, except that tensors can run on GPUs or other hardware accelerators. Tensors and NumPy arrays can often share the same underlying memory, eliminating the need to copy data. Tensors are also optimized for automatic differentiation\n"]},{"cell_type":"markdown","metadata":{"id":"YH8sJWhJtY_w"},"source":["Here are some ways they can be initialized in torch.\n","\n","1. from lists: $\\quad$ `X_1 = torch.tensor( list_object )`\n","\n","2. from a numpy array $\\quad$ `X_2 = torch.from_numpy( array_object )`\n","\n","3. with random or constant values, of a given shape. For example,\n","\n","    a. entries all ones: $\\quad$ `X_3_a = torch.ones( shape )`\n","\n","    b. entries all zeros $\\quad$ `X_3_b = torch.zeros( shape )`\n","\n","    c. entries are i.i.d. $\\text{Unif}(0,1)$ $\\quad$ `X_3_c = torch.rand( shape )`\n","\n","    d. entries are i.i.d. standard normal $\\quad$ `X_3_d = torch.randn( shape )`\n","\n","    e. from values stored in memory $\\quad$ `X_3_e = torch.Tensor( shape )`\n","    \n","    f. a list of consecutive integers between $N$ and $M$, inclusive, as tensor object $\\quad$ `X_3_f = torch.arange(N,M)`"]},{"cell_type":"markdown","metadata":{"id":"FWdPusy12fp4"},"source":["#### ... 1.   from lists"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":325},"id":"CfPAEk8N2TcU","executionInfo":{"status":"ok","timestamp":1635089600248,"user_tz":300,"elapsed":487,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"e5831d9f-c0c8-4fa2-c313-3f321ca5e526"},"source":["#collapse-hide\n","X_1 = torch.tensor( [ 2, 3 ] )\n","info(X_1, \"X1\")\n","viz_tens( 25 * X_1 ) # the factor there to help distinguish values"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor     X1 \n","\n","    num. dims        1\n","    num. entries     2\n","    shape            [2]\n","\n","\n","┌───┬───┐\n","│ 2 │ 3 │\n","└───┴───┘\n","\n","\n","\n"]},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAH4AAABGCAYAAAAKCiBIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAA2klEQVR4nO3dsQ0CIBBAUTFOx/KMwRg4gXaGxP9ee80lP1dQMc45D3qetxfgDuGjhI8SPkr4KOGjXt+Ga62/fuvtvW+v8FNzzvFp5uKjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPmr4RrzJxUcJHyV8lPBRwkcJH/UGmigNh5tk3yoAAAAASUVORK5CYII=\n","text/plain":["<Figure size 144x144 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"SrAFRNFN4l79"},"source":["#### ... 2. from numpy"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":306},"id":"15BQPXcG4rF1","executionInfo":{"status":"ok","timestamp":1635089600417,"user_tz":300,"elapsed":173,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"7e6879a1-aab3-409f-9bd4-8ce37f7b66cc"},"source":["#collapse-hide\n","X_2 = torch.from_numpy( np.array( [ 2, 3, 4 ] ) )\n","info(X_2, \"'X two'\")\n","viz_tens( 25 * X_2 )"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor     'X two' \n","\n","    num. dims        1\n","    num. entries     3\n","    shape            [3]\n","\n","\n","┌───┬───┬───┐\n","│ 2 │ 3 │ 4 │\n","└───┴───┴───┘\n","\n","\n","\n"]},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAH4AAAAzCAYAAABR5bw6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAsklEQVR4nO3csQ0DIRAAQWO5UwqmDMrgG7Cd/ks7k15y0uoCEsY550XP++4FuIfwUcJHCR8lfJTwUZ9/w7XW4956e++7V/jqiXvNOcevmYuPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4a/bJtcfJTwUcJHCR8lfJTwURc1fBBhoRLH+gAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 144x144 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"KX8wXz3IQ7_b"},"source":["The next examples allow a shape to be provided as an argument. "]},{"cell_type":"code","metadata":{"id":"_E3I4VRyS9rv","executionInfo":{"status":"ok","timestamp":1635089600419,"user_tz":300,"elapsed":8,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}}},"source":["#collapse-hide\n","\n","shape_dict = {}\n","\n","shape_dict[\"i\"] = ( 3 )\n","\n","shape_dict[\"ii\"] = ( 2, 3 )\n","\n","shape_dict[\"iii\"] = ( 2, 3, )"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y7l6vDAE5rGh"},"source":["#### ... 3. (a) ones of given shape"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"EU78SXCK5uCO","executionInfo":{"status":"ok","timestamp":1635089601036,"user_tz":300,"elapsed":624,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"69d2f4fd-4e2b-423a-9d93-ba52a621a54f"},"source":["#collapse-hide\n","\n","for key in shape_dict:\n","    s = key\n","    X_3_a = torch.ones(shape_dict[s])\n","    info(X_3_a, \"'X three (a)'\"[:-1] + \" \" + s + \"'\" )\n","    viz_tens( 25 * X_3_a )"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor     'X three (a) i' \n","\n","    num. dims        1\n","    num. entries     3\n","    shape            [3]\n","\n","\n","┌─────┬─────┬─────┐\n","│ 1.0 │ 1.0 │ 1.0 │\n","└─────┴─────┴─────┘\n","\n","\n","\n","tensor     'X three (a) ii' \n","\n","    num. dims        2\n","    num. entries     6\n","    shape            [2 3]\n","\n","\n","┌─────┬─────┬─────┐\n","│ 1.0 │ 1.0 │ 1.0 │\n","├─────┼─────┼─────┤\n","│ 1.0 │ 1.0 │ 1.0 │\n","└─────┴─────┴─────┘\n","\n","\n","\n","tensor     'X three (a) iii' \n","\n","    num. dims        2\n","    num. entries     6\n","    shape            [2 3]\n","\n","\n","┌─────┬─────┬─────┐\n","│ 1.0 │ 1.0 │ 1.0 │\n","├─────┼─────┼─────┤\n","│ 1.0 │ 1.0 │ 1.0 │\n","└─────┴─────┴─────┘\n","\n","\n","\n"]},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAH4AAAAzCAYAAABR5bw6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAqElEQVR4nO3csQ2AMAwAQYIYIvtPly3CBNAi9Hetm0gvF24y9t4HPefXD+AbwkcJHyV8lPBRwkddb8O1llvvx+ac42lm46OEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svio4S/bJhsfJXyU8FHCRwkfJXzUDUneCmEkcikWAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 144x144 with 1 Axes>"]},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAA/UlEQVR4nO3WwQkDMQwAwTikCPdfnbvQVZB8TdiZrz6CRaA1My963rcX4A7ho4SPEj5K+KjPr+E5x8v/x/be69vMxUcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJH7Vm5vYOXODio4SPEj5K+Cjho4SPegDe4Qqr0aiDOwAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 144x144 with 1 Axes>"]},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAA/UlEQVR4nO3WwQkDMQwAwTikCPdfnbvQVZB8TdiZrz6CRaA1My963rcX4A7ho4SPEj5K+KjPr+E5x8v/x/be69vMxUcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJH7Vm5vYOXODio4SPEj5K+Cjho4SPegDe4Qqr0aiDOwAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 144x144 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"_N4bM_vF7Up4"},"source":["#### ... 3. (b) zeros of given shape"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"vRQbqdB-qmfx","executionInfo":{"status":"ok","timestamp":1635089601600,"user_tz":300,"elapsed":568,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"9aaf26bc-4454-42bd-e4cc-6ebafaba2feb"},"source":["#collapse-hide\n","\n","for key in shape_dict:\n","    s = key\n","    X_3_b = torch.zeros(shape_dict[s])\n","    info(X_3_b, \"'X three (a)'\"[:-1] + \" \" + s + \"'\" )\n","    viz_tens( 25 * X_3_b )"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor     'X three (a) i' \n","\n","    num. dims        1\n","    num. entries     3\n","    shape            [3]\n","\n","\n","┌─────┬─────┬─────┐\n","│ 0.0 │ 0.0 │ 0.0 │\n","└─────┴─────┴─────┘\n","\n","\n","\n","tensor     'X three (a) ii' \n","\n","    num. dims        2\n","    num. entries     6\n","    shape            [2 3]\n","\n","\n","┌─────┬─────┬─────┐\n","│ 0.0 │ 0.0 │ 0.0 │\n","├─────┼─────┼─────┤\n","│ 0.0 │ 0.0 │ 0.0 │\n","└─────┴─────┴─────┘\n","\n","\n","\n","tensor     'X three (a) iii' \n","\n","    num. dims        2\n","    num. entries     6\n","    shape            [2 3]\n","\n","\n","┌─────┬─────┬─────┐\n","│ 0.0 │ 0.0 │ 0.0 │\n","├─────┼─────┼─────┤\n","│ 0.0 │ 0.0 │ 0.0 │\n","└─────┴─────┴─────┘\n","\n","\n","\n"]},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAH4AAAAzCAYAAABR5bw6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAo0lEQVR4nO3cMQqAQAwAQSP+/8vxBdqK7Eyb5mBJkeZmdw96zq8fwDeEjxI+Svgo4aOEj7rehjPj1vux3Z2nmY2PEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho8Zftk02Pkr4KOGjhI8SPkr4qBuM0wphyOWPswAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 144x144 with 1 Axes>"]},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAA+ElEQVR4nO3WsQnEQAwAwdfj/luWK7DTw+xMqkSwCDS7+6Pnf3oBzhA+Svgo4aOEj7rehjPj5f+w3Z2nmYuPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5qdvf0Dhzg4qOEjxI+Svgo4aOEj7oBIeUKq3KMmaAAAAAASUVORK5CYII=\n","text/plain":["<Figure size 144x144 with 1 Axes>"]},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAA+ElEQVR4nO3WsQnEQAwAwdfj/luWK7DTw+xMqkSwCDS7+6Pnf3oBzhA+Svgo4aOEj7rehjPj5f+w3Z2nmYuPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5qdvf0Dhzg4qOEjxI+Svgo4aOEj7oBIeUKq3KMmaAAAAAASUVORK5CYII=\n","text/plain":["<Figure size 144x144 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"lX5iyNSa7ZNg"},"source":["#### ... 3. (c) i.i.d. uniform of given shape"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Sh28SUSjruFw","executionInfo":{"status":"ok","timestamp":1635089602388,"user_tz":300,"elapsed":791,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"55422550-5b01-4cc8-f858-3b9ed1b5bb0d"},"source":["#collapse-hide\n","\n","for key in shape_dict:\n","    s = key\n","    X_3_c = torch.rand(shape_dict[s])\n","    info(X_3_c, \"X 3 (c) \" + \" \" + s )\n","    viz_tens( 25 * X_3_c )\n"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor     X 3 (c)  i \n","\n","    num. dims        1\n","    num. entries     3\n","    shape            [3]\n","\n","\n","┌──────┬──────┬──────┐\n","│ 0.88 │ 0.91 │ 0.38 │\n","└──────┴──────┴──────┘\n","\n","\n","\n","tensor     X 3 (c)  ii \n","\n","    num. dims        2\n","    num. entries     6\n","    shape            [2 3]\n","\n","\n","┌──────┬──────┬──────┐\n","│ 0.95 │ 0.39 │ 0.60 │\n","├──────┼──────┼──────┤\n","│ 0.25 │ 0.79 │ 0.94 │\n","└──────┴──────┴──────┘\n","\n","\n","\n","tensor     X 3 (c)  iii \n","\n","    num. dims        2\n","    num. entries     6\n","    shape            [2 3]\n","\n","\n","┌──────┬──────┬──────┐\n","│ 0.13 │ 0.93 │ 0.59 │\n","├──────┼──────┼──────┤\n","│ 0.86 │ 0.56 │ 0.74 │\n","└──────┴──────┴──────┘\n","\n","\n","\n"]},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAH4AAAAzCAYAAABR5bw6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAt0lEQVR4nO3cuQ3DMBAAQdNwzyxPNagcRXQBflIK2Jn0kgMWFzDhWGs96HnuXoA9hI8SPkr4KOGjhI96/Rte13W7t955nrtX+Oo4jt0rfJhzjl8zFx8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHDX/ZNrn4KOGjhI8SPkr4KOGj3n9EEGE9itKdAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 144x144 with 1 Axes>"]},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABLElEQVR4nO3csW2EQBBAUWM5oSh6uYCKKIOCqIA+cGzdHeme/N9LNxnpa6SNZrqu64ue79EDMIbwUcJHCR8lfNTP3eN5nh/35T+OY/QILy3LMnqEJ/M8T+/ebHyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfNd2dNF3X9eOOHz0ej9EjvLTv++gRnmzb5vgRfwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCR91eveL/svFRwkcJHyV8lPBRwkf9AphsF6frkzUoAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 144x144 with 1 Axes>"]},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABK0lEQVR4nO3cu2nFQBRFUcu4J/UhUDlKVI/aUzwu4H14kcew10onObC5oEjLGOOLnu/ZA5hD+Cjho4SPEj7q593jtm3/7pP/vu/ZE546z3P2hAfrui6v3lx8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJH/X250f7vv/Vjo8dxzF7wlPXdc2e8GBd15dvLj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5qGWPM3sAELj5K+Cjho4SPEj5K+KhfR1ASzMQA6osAAAAASUVORK5CYII=\n","text/plain":["<Figure size 144x144 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"DpBCD-VT7mFF"},"source":["#### ... 3. (d) i.i.d. standard normal of given shape"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"bZ9SYjRcsE9L","executionInfo":{"status":"ok","timestamp":1635089602690,"user_tz":300,"elapsed":312,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"aea9d471-7109-442b-8658-2e6c0d5f55e5"},"source":["#collapse-hide\n","\n","for key in shape_dict:\n","    s = key\n","    X_3_d = torch.randn(shape_dict[s])\n","    info(X_3_d, \"X 3 (d)\" + \" \" + s )\n","    viz_tens( 25 * X_3_d )"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor     X 3 (d) i \n","\n","    num. dims        1\n","    num. entries     3\n","    shape            [3]\n","\n","\n","┌──────┬──────┬──────┐\n","│ -1.2 │ 0.52 │ 1.22 │\n","└──────┴──────┴──────┘\n","\n","\n","\n","tensor     X 3 (d) ii \n","\n","    num. dims        2\n","    num. entries     6\n","    shape            [2 3]\n","\n","\n","┌──────┬──────┬──────┐\n","│ 0.15 │ -0.3 │ -0.4 │\n","├──────┼──────┼──────┤\n","│ -0.2 │ -0.1 │ -1.1 │\n","└──────┴──────┴──────┘\n","\n","\n","\n","tensor     X 3 (d) iii \n","\n","    num. dims        2\n","    num. entries     6\n","    shape            [2 3]\n","\n","\n","┌──────┬──────┬──────┐\n","│ -0.8 │ -0.7 │ 0.12 │\n","├──────┼──────┼──────┤\n","│ -0.1 │ -2.2 │ -0.6 │\n","└──────┴──────┴──────┘\n","\n","\n","\n"]},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAH4AAAAzCAYAAABR5bw6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAuElEQVR4nO3cuQ3DMBAAQdNwVSxJDaki1UUX4CelgJ1JLzlgcQETjrXWg57n7gXYQ/go4aOEjxI+Svio17/hcRy3e+td17V7ha/mnLtX+HCe5/g1c/FRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV81PCXbZOLjxI+Svgo4aOEjxI+6g2mew1hx7zLoQAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 144x144 with 1 Axes>"]},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABKUlEQVR4nO3csW2EQBRFUWNtFXRGTIX0QDO0QDIuwOuVN/FYuuekkzzp6ktELGOMD3o+Zw9gDuGjhI8SPkr4qMerx/M8/90n/33fsyc8tW3b7AnfXNe1/PTm4qOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Sviolz8/Oo7jr3b82rqusyc8te/77AlvcfFRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRyxhj9gYmcPFRwkcJHyV8lPBRwkd9AbVVFDcCSemUAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 144x144 with 1 Axes>"]},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABJ0lEQVR4nO3csW2EQBRFUWNtB6R0Q/9QCwWgcQFer7yJx9I9J53kSVdfImIZY3zQ8zl7AHMIHyV8lPBRwkc9Xj1e1/XvPvn3fZ894an7vmdP+OY8z+WnNxcfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCR738+dG6rn+149eO45g94alt22ZPeIuLjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEj1rGGLM3MIGLjxI+Svgo4aOEjxI+6gsRMxOkcly2jgAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 144x144 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"JwY4smEXs9Gm"},"source":["#### ... 3. (f) sequence"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":280},"id":"O2oQdYhktbUi","executionInfo":{"status":"ok","timestamp":1635092140868,"user_tz":300,"elapsed":361,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"2c932fe0-b044-47ae-c906-38ef8a3de7d4"},"source":["#collapse-hide\n","\n","X_3_f = torch.arange(10,20) \n","info(X_3_f, \"X 3 (f)\")\n","viz_tens( 25 * X_3_f ) # note the modular arith. being performed automatically"],"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor     X 3 (f) \n","\n","    num. dims        1\n","    num. entries     10\n","    shape            [10]\n","\n","\n","┌────┬────┬────┬────┬────┬────┬────┬────┬────┬────┐\n","│ 10 │ 11 │ 12 │ 13 │ 14 │ 15 │ 16 │ 17 │ 18 │ 19 │\n","└────┴────┴────┴────┴────┴────┴────┴────┴────┴────┘\n","\n","\n","\n"]},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAH4AAAAZCAYAAAD30ppqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAf0lEQVRoge3awQnAIBAAwVwIXP/t2cCVcakgwTzEwO58FRUWH4LR3Yd4zt0H0B6GhzI8lOGhDA9leKjrbTAzp996VTW96Ze5K9f+w9yVa48x4mnMGw9leCjDQxkeyvBQhocyPJThoQwPZXio8AcOkzceyvBQhocyPJThoQwPdQM4ZCIt7yfBxgAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 144x144 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"c2_VNHBYt4Cb"},"source":["### <font color=\"teal\">numpy objects and tensors</font>"]},{"cell_type":"markdown","metadata":{"id":"m-3HAcmauOR_"},"source":["Tensors can be converted to numpy arrays, and numpy arrays back to tensors.\n","To transform a numpy array into a tensor, we can use the function `torch.from_numpy`, and we use `np.array` for the other direction."]},{"cell_type":"markdown","metadata":{"id":"O2CfkAyIwDZG"},"source":["The conversion of tensors to numpy require the tensor to be on the CPU, and not the GPU.\n","\n","In case you have a tensor on GPU, you need to call `.cpu()` on the tensor beforehand.\n","Hence, you get a line like `np_arr = tensor.cpu().numpy()`."]},{"cell_type":"markdown","metadata":{"id":"Wi-IudDowL3g"},"source":["Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xuMNCnWhwSFw","executionInfo":{"status":"ok","timestamp":1635089602971,"user_tz":300,"elapsed":147,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"5bead8a7-0a6d-4d2b-97f0-2c396db1a452"},"source":["t = torch.ones(5)\n","print(f\"t: {t}\")\n","n = t.numpy()\n","print(f\"n: {n}\")"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["t: tensor([1., 1., 1., 1., 1.])\n","n: [1. 1. 1. 1. 1.]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qAfGPmNLwUCt","executionInfo":{"status":"ok","timestamp":1635089602971,"user_tz":300,"elapsed":6,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"ca8cc410-9a8a-43f0-d158-d1a255a2aa7b"},"source":["t.add_(1)\n","print(f\"t: {t}\")\n","print(f\"n: {n}\")"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["t: tensor([2., 2., 2., 2., 2.])\n","n: [2. 2. 2. 2. 2.]\n"]}]},{"cell_type":"markdown","metadata":{"id":"rD8WrGTVtfnb"},"source":["### <font color=\"teal\">... tensor operations</font>"]},{"cell_type":"markdown","metadata":{"id":"fgOtJXKztkeQ"},"source":["Most operations existing in numpy also exist in PyTorch. A full list of operations can be found in the [PyTorch documentation](https://pytorch.org/docs/stable/tensors.html#).\n","\n","* Each torch operation can be run on the GPU.\n","\n","* By default, tensors are created on the CPU. Unless we are using a package like `pytorch-lightning`, we need to explicitly move tensors to the GPU using the `.to` method, after checking GPU availability. \n","\n","* Copying large tensors across devices can be expensive in terms of time and memory.\n"]},{"cell_type":"markdown","metadata":{"id":"YyKSQh8pwkXX"},"source":["#### ... adding tensors"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":198},"id":"vL5_gbFdGouM","executionInfo":{"status":"ok","timestamp":1635089603265,"user_tz":300,"elapsed":297,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"6b4fa541-5fcd-4766-e6ce-08264bd5c607"},"source":["#collapse-hide\n","\n","X_1, X_2 = torch.rand(2,3), torch.rand(2,3)\n","viz_tens_list( [ X_1, X_2 ] )"],"execution_count":21,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAV0AAAC1CAYAAAD86CzsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAC+klEQVR4nO3asW3CYBhF0TjyAMzBKog1aKiYhmWQ2IDeI1hiB7NAAKW5jsI57Vc8V1d/4WFZli8AGt9rfwDAJxFdgJDoAoREFyAkugCh8c3drw0Avzc8O3jpAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBrX/oA1nc/ndG+73aZ7p9Mp3bvdbuleaZ7ndO9yuaR7m80m3dvtduneX+KlCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKExrU/YE3H4zHd2+/36d79fk/3/rPr9ZruTdOU7h0Oh3Tvk3npAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugChYVmWV/eXRwB+NDw7eOkChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQGt/ch+QrAD6Ely5ASHQBQqILEBJdgJDoAoREFyD0AKc9IFJT4QG+AAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x432 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"XfB72xmXwvAf","executionInfo":{"status":"ok","timestamp":1635089603432,"user_tz":300,"elapsed":170,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"30db0c38-67dc-420b-f682-4fb85d5dbec0"},"source":["#collapse-hide\n","\n","Y = X_1 + X_2\n","viz_tens(Y) "],"execution_count":22,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABKklEQVR4nO3cwWnDQBBA0Si4CakF1aKaBCpWbSgF2BY5ZU3+e9e9DHwG9jTTdV1f9HyPHoAxhI8SPkr4KOGjHneP+75/3Jf/PM/RI7x0HMfoEZ7M8zy9e7PxUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfNTt8aN1Xf9qjl/btm30CC8tyzJ6hCd352ptfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfNR0dxmJ/8vGRwkfJXyU8FHCRwkf9QPQnxJgHxHSmQAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 144x144 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"9d_36o-r0emo"},"source":["#### ... stacking tensors"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VmrgRs7Go4he","executionInfo":{"status":"ok","timestamp":1635089603433,"user_tz":300,"elapsed":14,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"5890ad1b-f68b-49ba-9dba-43fe5601b786"},"source":["#collapse-hide\n","\n","X_1, X_2 = torch.arange(5,10), torch.arange(10,15)\n","\n","info(X_1, \"X 1\")\n","\n","info(X_2, \"X 2\")"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor     X 1 \n","\n","    num. dims        1\n","    num. entries     5\n","    shape            [5]\n","\n","\n","┌───┬───┬───┬───┬───┐\n","│ 5 │ 6 │ 7 │ 8 │ 9 │\n","└───┴───┴───┴───┴───┘\n","\n","\n","\n","tensor     X 2 \n","\n","    num. dims        1\n","    num. entries     5\n","    shape            [5]\n","\n","\n","┌────┬────┬────┬────┬────┐\n","│ 10 │ 11 │ 12 │ 13 │ 14 │\n","└────┴────┴────┴────┴────┘\n","\n","\n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OCZCoLoypwe4","executionInfo":{"status":"ok","timestamp":1635089603433,"user_tz":300,"elapsed":10,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"0bfce4e6-0a45-47cf-e013-f60df82f792f"},"source":["#collapse-hide\n","\n","Y = torch.stack([X_1, X_2], dim = 0)\n","info(Y, \"Y\")"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor     Y \n","\n","    num. dims        2\n","    num. entries     10\n","    shape            [2 5]\n","\n","\n","┌────┬────┬────┬────┬────┐\n","│ 5  │ 6  │ 7  │ 8  │ 9  │\n","├────┼────┼────┼────┼────┤\n","│ 10 │ 11 │ 12 │ 13 │ 14 │\n","└────┴────┴────┴────┴────┘\n","\n","\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"vsFE3lVSxiqT"},"source":["#### ... in-place operations"]},{"cell_type":"markdown","metadata":{"id":"xqGLzbThyCcm"},"source":["Operations that store the result into the operand are called _in-place_. They are  usually marked with a underscore postfix, e.g. \"`add_`\" instead of \"`add`\". The operation `X.copy_(Y)` will change `X`."]},{"cell_type":"markdown","metadata":{"id":"_fluWRXzyG5D"},"source":["Calling `x1 + x2` creates a new tensor containing the sum of the two inputs.\n","However, we can also use in-place operations that are applied directly on the memory of a tensor.\n","We therefore change the values of `x2` without the chance to re-accessing the values of `x2` before the operation.\n","An example is shown below:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CBA_fQJYycd2","executionInfo":{"status":"ok","timestamp":1635089603621,"user_tz":300,"elapsed":195,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"376dcebb-409f-4497-f72c-c39cddc58efc"},"source":["#collapse-hide\n","\n","X_1, X_2 = torch.rand(2, 3), torch.rand(2, 3)\n","print(\"\\t\",\"before\", \"\\n\")\n","info(X_1, \"'X one'\")\n","info(X_2, \"'X two'\")\n","\n","print(\"\\n\\n\")\n","\n","X_2.add_(X_1)\n","print(\"\\t\",\"after\",\"\\n\")\n","info(X_1, \"'X one'\")\n","info(X_2, \"'X two'\")"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["\t before \n","\n","tensor     'X one' \n","\n","    num. dims        2\n","    num. entries     6\n","    shape            [2 3]\n","\n","\n","┌──────┬──────┬──────┐\n","│ 0.14 │ 0.53 │ 0.15 │\n","├──────┼──────┼──────┤\n","│ 0.65 │ 0.32 │ 0.65 │\n","└──────┴──────┴──────┘\n","\n","\n","\n","tensor     'X two' \n","\n","    num. dims        2\n","    num. entries     6\n","    shape            [2 3]\n","\n","\n","┌──────┬──────┬──────┐\n","│ 0.39 │ 0.91 │ 0.20 │\n","├──────┼──────┼──────┤\n","│ 0.20 │ 0.20 │ 0.94 │\n","└──────┴──────┴──────┘\n","\n","\n","\n","\n","\n","\n","\t after \n","\n","tensor     'X one' \n","\n","    num. dims        2\n","    num. entries     6\n","    shape            [2 3]\n","\n","\n","┌──────┬──────┬──────┐\n","│ 0.14 │ 0.53 │ 0.15 │\n","├──────┼──────┼──────┤\n","│ 0.65 │ 0.32 │ 0.65 │\n","└──────┴──────┴──────┘\n","\n","\n","\n","tensor     'X two' \n","\n","    num. dims        2\n","    num. entries     6\n","    shape            [2 3]\n","\n","\n","┌──────┬──────┬──────┐\n","│ 0.54 │ 1.44 │ 0.36 │\n","├──────┼──────┼──────┤\n","│ 0.85 │ 0.52 │ 1.60 │\n","└──────┴──────┴──────┘\n","\n","\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"G9qBA5rQyHqP"},"source":["> In-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss of history. Hence, their use is discouraged. "]},{"cell_type":"markdown","metadata":{"id":"v31LbYKfyyEi"},"source":["#### ... reshaping tensors"]},{"cell_type":"markdown","metadata":{"id":"nocsFb5Ly0g9"},"source":["Another common operation aims at changing the shape of a tensor.\n","A tensor of size `(2,3)` can be re-organized to any other shape with the same number of elements (e.g. a tensor of size `(6)`, or `(3,2)`, ...).\n","\n","In PyTorch, this reshaping operation is called `view`:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A5ZiVTnqy3K4","executionInfo":{"status":"ok","timestamp":1635089603621,"user_tz":300,"elapsed":31,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"2fd8abdb-0692-4cd0-af1b-0db748f124ab"},"source":["#collapse-hide\n","\n","X = torch.arange(6)\n","info(X,\"X\")"],"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor     X \n","\n","    num. dims        1\n","    num. entries     6\n","    shape            [6]\n","\n","\n","┌───┬───┬───┬───┬───┬───┐\n","│ 0 │ 1 │ 2 │ 3 │ 4 │ 5 │\n","└───┴───┴───┴───┴───┴───┘\n","\n","\n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HnKJbrpVy7cX","executionInfo":{"status":"ok","timestamp":1635089603622,"user_tz":300,"elapsed":28,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"fa539dce-b2e2-41ff-9278-5746eb86e339"},"source":["#collapse-hide\n","\n","X = X.view(2, 3)\n","info(X,\"X\")"],"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor     X \n","\n","    num. dims        2\n","    num. entries     6\n","    shape            [2 3]\n","\n","\n","┌───┬───┬───┐\n","│ 0 │ 1 │ 2 │\n","├───┼───┼───┤\n","│ 3 │ 4 │ 5 │\n","└───┴───┴───┘\n","\n","\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"ML1CxKBirIZN"},"source":["#### ... transposing tensors (permuting dimensions)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zrcsGSx0rO-l","executionInfo":{"status":"ok","timestamp":1635089603622,"user_tz":300,"elapsed":23,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"b21680af-a6ef-43f2-d5e8-bdf8622e46c1"},"source":["#collapse-hide\n","\n","X = X.permute(1,0)\n","info(X,\"X with 0th and 1st dimensions permuted\")"],"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor     X with 0th and 1st dimensions permuted \n","\n","    num. dims        2\n","    num. entries     6\n","    shape            [3 2]\n","\n","\n","┌───┬───┐\n","│ 0 │ 3 │\n","├───┼───┤\n","│ 1 │ 4 │\n","├───┼───┤\n","│ 2 │ 5 │\n","└───┴───┘\n","\n","\n","\n"]}]},{"cell_type":"code","metadata":{"id":"4S-Un1nuwJZw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635089603623,"user_tz":300,"elapsed":21,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"636a13b4-c826-46cf-b771-5b96fe78d638"},"source":["#collapse-hide\n","\n","X = X.T\n","info(X, \"X transposed again\")"],"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor     X transposed again \n","\n","    num. dims        2\n","    num. entries     6\n","    shape            [2 3]\n","\n","\n","┌───┬───┬───┐\n","│ 0 │ 1 │ 2 │\n","├───┼───┼───┤\n","│ 3 │ 4 │ 5 │\n","└───┴───┴───┘\n","\n","\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"oYiLwgu3rXvp"},"source":["#### ... numpy-like indexing and slicing"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y3ae8RldrbH7","executionInfo":{"status":"ok","timestamp":1635089603623,"user_tz":300,"elapsed":17,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"bae0c572-ee1f-4749-d291-367637a84e38"},"source":["#collapse-hide\n","\n","X = torch.rand( 4,4 )\n","info( X, \"X\" )\n","info( X[0], \"first row of X\" )\n","info( X[:,0], \"first column of X\" )\n","info( X[...,-1], \"last column of X\" )\n","info( X[:2, -1], \"First two rows, last column\")\n","info( X[1:3, :], \"Middle two rows\" )"],"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor     X \n","\n","    num. dims        2\n","    num. entries     16\n","    shape            [4 4]\n","\n","\n","┌──────┬──────┬──────┬──────┐\n","│ 0.66 │ 0.98 │ 0.08 │ 0.00 │\n","├──────┼──────┼──────┼──────┤\n","│ 0.10 │ 0.16 │ 0.70 │ 0.67 │\n","├──────┼──────┼──────┼──────┤\n","│ 0.91 │ 0.24 │ 0.15 │ 0.76 │\n","├──────┼──────┼──────┼──────┤\n","│ 0.29 │ 0.80 │ 0.38 │ 0.78 │\n","└──────┴──────┴──────┴──────┘\n","\n","\n","\n","tensor     first row of X \n","\n","    num. dims        1\n","    num. entries     4\n","    shape            [4]\n","\n","\n","┌──────┬──────┬──────┬──────┐\n","│ 0.66 │ 0.98 │ 0.08 │ 0.00 │\n","└──────┴──────┴──────┴──────┘\n","\n","\n","\n","tensor     first column of X \n","\n","    num. dims        1\n","    num. entries     4\n","    shape            [4]\n","\n","\n","┌──────┬──────┬──────┬──────┐\n","│ 0.66 │ 0.10 │ 0.91 │ 0.29 │\n","└──────┴──────┴──────┴──────┘\n","\n","\n","\n","tensor     last column of X \n","\n","    num. dims        1\n","    num. entries     4\n","    shape            [4]\n","\n","\n","┌──────┬──────┬──────┬──────┐\n","│ 0.00 │ 0.67 │ 0.76 │ 0.78 │\n","└──────┴──────┴──────┴──────┘\n","\n","\n","\n","tensor     First two rows, last column \n","\n","    num. dims        1\n","    num. entries     2\n","    shape            [2]\n","\n","\n","┌──────┬──────┐\n","│ 0.00 │ 0.67 │\n","└──────┴──────┘\n","\n","\n","\n","tensor     Middle two rows \n","\n","    num. dims        2\n","    num. entries     8\n","    shape            [2 4]\n","\n","\n","┌──────┬──────┬──────┬──────┐\n","│ 0.10 │ 0.16 │ 0.70 │ 0.67 │\n","├──────┼──────┼──────┼──────┤\n","│ 0.91 │ 0.24 │ 0.15 │ 0.76 │\n","└──────┴──────┴──────┴──────┘\n","\n","\n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5fqX_-7SsYYb","executionInfo":{"status":"ok","timestamp":1635089603624,"user_tz":300,"elapsed":14,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"f9c924a4-820f-4d0e-a7f9-d4215e42125d"},"source":["#collapse-hide\n","\n","X[:,1] = 0\n","info(X, \"modified X\")"],"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor     modified X \n","\n","    num. dims        2\n","    num. entries     16\n","    shape            [4 4]\n","\n","\n","┌──────┬─────┬──────┬──────┐\n","│ 0.66 │ 0.0 │ 0.08 │ 0.00 │\n","├──────┼─────┼──────┼──────┤\n","│ 0.10 │ 0.0 │ 0.70 │ 0.67 │\n","├──────┼─────┼──────┼──────┤\n","│ 0.91 │ 0.0 │ 0.15 │ 0.76 │\n","├──────┼─────┼──────┼──────┤\n","│ 0.29 │ 0.0 │ 0.38 │ 0.78 │\n","└──────┴─────┴──────┴──────┘\n","\n","\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"cyVcUuRKuFA9"},"source":["#### ... other operations"]},{"cell_type":"markdown","metadata":{"id":"1PEj00HzuIFg"},"source":["Here are some ways to perform matrix multiplication:\n","\n","* `torch.matmul` $\\quad$ Performs the matrix product over two tensors, where the specific behavior depends on the dimensions.\n","If both inputs are matrices (2-dimensional tensors), it performs the standard matrix product.\n","For higher dimensional inputs, the function supports broadcasting (for details see the [documentation](https://pytorch.org/docs/stable/generated/torch.matmul.html?highlight=matmul#torch.matmul)).\n","\n","    It can also be written as `a @ b`, similar to numpy.\n","\n","* `torch.mm` $\\quad$ Performs the matrix product over two matrices, but doesn't support broadcasting (see [documentation](https://pytorch.org/docs/stable/generated/torch.mm.html?highlight=torch%20mm#torch.mm))\n","\n","* `torch.bmm` $\\quad$ Performs the matrix product with a support batch dimension. Let `T` be a tensor of shape `(b,  n, m)`, and `R` a tensor of shape `(b, m, p)`, the output tensor is of shape `(b, n , p)`, obtained by \"entry-wise\" matrix multiplication along the batch dimension. \n","\n","* `torch.einsum` $\\quad$ Performs matrix multiplications and more (i.e. sums of products) using the Einstein summation convention.\n","\n","Usually, we use `torch.matmul` or `torch.bmm`. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ClIpMlBPug0-","executionInfo":{"status":"ok","timestamp":1635089603817,"user_tz":300,"elapsed":203,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"d77f99ca-cd75-4f16-e6b3-680ee0bae9bb"},"source":["#collapse-hide\n","\n","X, Y = torch.arange(6).view(2, 3), torch.arange(9).view(3, 3)\n","\n","info(X,\"X\")\n","\n","info(Y,\"Y\")\n"],"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor     X \n","\n","    num. dims        2\n","    num. entries     6\n","    shape            [2 3]\n","\n","\n","┌───┬───┬───┐\n","│ 0 │ 1 │ 2 │\n","├───┼───┼───┤\n","│ 3 │ 4 │ 5 │\n","└───┴───┴───┘\n","\n","\n","\n","tensor     Y \n","\n","    num. dims        2\n","    num. entries     9\n","    shape            [3 3]\n","\n","\n","┌───┬───┬───┐\n","│ 0 │ 1 │ 2 │\n","├───┼───┼───┤\n","│ 3 │ 4 │ 5 │\n","├───┼───┼───┤\n","│ 6 │ 7 │ 8 │\n","└───┴───┴───┘\n","\n","\n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tfE3Q4yIvZ6D","executionInfo":{"status":"ok","timestamp":1635089603819,"user_tz":300,"elapsed":34,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"aefe07c1-a33a-499c-8da1-107d1c03c7e4"},"source":["#collapse-hide\n","\n","Z = torch.matmul(X,Y)\n","info(Z, \"Z\")"],"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor     Z \n","\n","    num. dims        2\n","    num. entries     6\n","    shape            [2 3]\n","\n","\n","┌────┬────┬────┐\n","│ 15 │ 18 │ 21 │\n","├────┼────┼────┤\n","│ 42 │ 54 │ 66 │\n","└────┴────┴────┘\n","\n","\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"rNkoOvNUwcD0"},"source":["Given a tensor `X`, the tensors `Y_1`,`Y_2`, `Y_3` computed below all have the same value:\n","\n","    Y_1 = X @ X.T\n","    Y_2 = X.matmul(X.T)\n","    Y_3 = torch.rand_like(X)\n","    torch.matmul(X, X.T, out = Y_3)"]},{"cell_type":"markdown","metadata":{"id":"hL9V6eauw54L"},"source":["On the other hand, `*` denotes the entrywise product of two tensors. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xD8EVlKhxGqC","executionInfo":{"status":"ok","timestamp":1635089603819,"user_tz":300,"elapsed":31,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"b19c273e-f89b-4c71-cef4-621b1eacc23f"},"source":["#collapse-hide\n","\n","X = torch.arange(6).view(2, 3)\n","info(X,\"X\")"],"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor     X \n","\n","    num. dims        2\n","    num. entries     6\n","    shape            [2 3]\n","\n","\n","┌───┬───┬───┐\n","│ 0 │ 1 │ 2 │\n","├───┼───┼───┤\n","│ 3 │ 4 │ 5 │\n","└───┴───┴───┘\n","\n","\n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PhK2YS78xZQg","executionInfo":{"status":"ok","timestamp":1635089603820,"user_tz":300,"elapsed":27,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"62ff4d96-f498-44fd-8a25-957d3222975d"},"source":["#collapse-hide\n","\n","info( X * X, \"(a)\")\n","info( X.mul(X), \"(b)\")"],"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor     (a) \n","\n","    num. dims        2\n","    num. entries     6\n","    shape            [2 3]\n","\n","\n","┌───┬────┬────┐\n","│ 0 │ 1  │ 4  │\n","├───┼────┼────┤\n","│ 9 │ 16 │ 25 │\n","└───┴────┴────┘\n","\n","\n","\n","tensor     (b) \n","\n","    num. dims        2\n","    num. entries     6\n","    shape            [2 3]\n","\n","\n","┌───┬────┬────┐\n","│ 0 │ 1  │ 4  │\n","├───┼────┼────┤\n","│ 9 │ 16 │ 25 │\n","└───┴────┴────┘\n","\n","\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"PX9GVY9_yOq3"},"source":["You can use `torch.cat` to concatenate a sequence of tensors along a given dimension. See also `torch.stack`, another tensor joining op that is subtly different from `torch.cat`"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l8ceai_hyU-K","executionInfo":{"status":"ok","timestamp":1635089603821,"user_tz":300,"elapsed":26,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"205acbc7-88ad-45ee-f551-650205c6d305"},"source":["#collapse-hide\n","\n","Y = torch.cat( [X, X, X], dim =1)\n","info(Y, \"Y\")"],"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor     Y \n","\n","    num. dims        2\n","    num. entries     18\n","    shape            [2 9]\n","\n","\n","┌───┬───┬───┬───┬───┬───┬───┬───┬───┐\n","│ 0 │ 1 │ 2 │ 0 │ 1 │ 2 │ 0 │ 1 │ 2 │\n","├───┼───┼───┼───┼───┼───┼───┼───┼───┤\n","│ 3 │ 4 │ 5 │ 3 │ 4 │ 5 │ 3 │ 4 │ 5 │\n","└───┴───┴───┴───┴───┴───┴───┴───┴───┘\n","\n","\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZFkFlehIymfT"},"source":["If you have a one-element tensor, for example obtained by aggregating all values of a given tensor into a single value, you can convert it to a Python numerical value using `item()`:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J8AP_WDcyntl","executionInfo":{"status":"ok","timestamp":1635089603821,"user_tz":300,"elapsed":22,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"399af8e9-ebb5-4218-b66f-125e36ce9967"},"source":["#collapse-hide\n","\n","agg = X.sum()\n","agg_item = agg.item() \n","print(agg_item, \"\\t\", type(agg_item))"],"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["15 \t <class 'int'>\n"]}]},{"cell_type":"markdown","metadata":{"id":"qCpwmuFyMYk3"},"source":["## <font color=\"CornflowerBlue\">2.2 ... one-dimensional convolutions of scalar-valued signals</font>"]},{"cell_type":"markdown","metadata":{"id":"s3jRDbDFM-_w"},"source":["We now consider one of the simplest possible settings for learning, in which the underlying domain is a one-dimensional grid, and where signals over the domain have only a single channel. \n","\n","In this case, the signal domain is a group itself, the cyclic group of order $n$, \n","$$\n","C_n = \\langle \\, a : a^n = 1 \\, \\rangle \\equiv \\{ \\, 1, a, a^2, \\dots, a^{n-1} \\, \\}.\n","$$\n","It is convenient to parametrize the group, and hence the grid, through the exponent of the generator \n","$$\n","C_n \\equiv \\{ 0, 1, \\dots, n -1 \\}\n","$$\n","as this indexing is consistent with the way most python indexes vectors. In this setting, the group operation may be reinterpreted as addition modulo $n$."]},{"cell_type":"markdown","metadata":{"id":"MB5Ml1nkRbBs"},"source":["As the input domain is fixed, it feels natural to consider the GDL group $G$ to be $C_n$ as well. \n","\n","We suppose that signals are scalar-valued, and are encoded in the natural basis, so that each $x \\in \\mathcal{X}(C_n, \\mathbb{R})$ may be expressed as \n","\n","$$\n","\\mathcal{X}(C_n,\\mathbb{R}) = \\{ x : C_n \\to \\mathbb{R} \\} ,\n","$$\n","is finite dimensional, and each $x \\in \\mathcal{X}(C_n, \\mathbb{R})$ may be expressed as \n","$$\n","x = \n","\\left[ \n","\\begin{matrix}\n","x_0\\\\ \n","\\vdots\\\\\n","\\,x_{n-1}\\,\n","\\end{matrix}\n","\\right] \n","$$"]},{"cell_type":"markdown","metadata":{"id":"y1QX0iPUyVZd"},"source":["With this basis, we can describe the representation $\\rho$ of $G \\equiv C_n$ concretely, as a matrix. "]},{"cell_type":"markdown","metadata":{"id":"H-bA8Ykqywt4"},"source":["Given a vector $\\theta = (\\theta_0 , \\dots, \\theta_{n-1})$, recall the associated <font color=\"purple\">_circulant matrix_</font> is the $n \\times n$ matrix with entries \n","$$\n","S(\\theta) := \\left( \\, \\theta_{ (u - v) \\mod n} \\right)_{ 0 \\, \\leq \\,u,\\,v \\, \\leq n-1 } \n","$$"]},{"cell_type":"markdown","metadata":{"id":"_Ilur6U0zVps"},"source":["In the specific case of $\\theta_{+} := (0,1,0,\\dots, 0)^T$, the associated circulant matrix, $S_{+} := S(\\theta_{+})$ acts on vectors by shifting the entries of vectors to the right by one position, corresponding to addition by one, modulo $n$. \n","\n","We call $S_+$ the _<font color=\"purple\">(right) shift operator</font>_. "]},{"cell_type":"markdown","metadata":{"id":"VACCLRh_zaU2"},"source":["___\n","__Lemma__ $\\quad$\n","A matrix is circulant if and only if it commutes with $S_+$. Moreover, given any two vectors $\\theta, \\eta \\in \\mathbb{R}^n$, one has $S(\\theta) S(\\eta) = S(\\eta) S(\\theta)$. \n","___"]},{"cell_type":"markdown","metadata":{"id":"2BOxnG0Pz2YD"},"source":["The importance of $S_+$ to the present discussion is that it generates a group isomorphic to the one-dimensional translation group $C_n$; the matrices $\\{ I, S_+, S_+^2, \\dots, S_+^{n-1} \\}$ constitute a faithful representation of $C_n$. "]},{"cell_type":"markdown","metadata":{"id":"j0N6Rklm1Sn0"},"source":["Circulant matrices are synonymous with discrete convolutions; given $x, \\theta \\in \\mathcal{X}(\\Omega,\\mathbb{R}) \\equiv \\mathbb{R}^n$, their _convolution_ $x \\star \\theta$ is defined by \n","$$\n","( x \\star \\theta )_u := \\sum_{v = 0}^{n-1} x_{v \\mod n}\\, \\theta_{ (u-v) \\mod n} \\equiv S(\\theta) x \n","$$\n","\n","Thus, the next corollary effectively follows from the much stronger theorem stated at the end of [section 1.3](https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2021/10/21/GDL1.html#1.3-...-equivariance-in-neural-networks). "]},{"cell_type":"markdown","metadata":{"id":"DTf8ASH40Qqb"},"source":["___\n","\n","__Corollary__ $\\quad$ Any $f : \\mathcal{X}(C_n, \\mathbb{R}) \\to \\mathcal{X}(C_n,\\mathbb{R})$ which is linear and $C_n$-equivariant can be expressed (in the input coordinate system) as an $n \\times n$ circulant matrix $S(\\theta)$ for some vector $\\theta$.\n","\n","___\n"]},{"cell_type":"markdown","metadata":{"id":"Ua2o1Ched6tr"},"source":["___\n","___\n","\n","### <font color=\"teal\">...example: local averaging as a circulant matrix</font>"]},{"cell_type":"markdown","metadata":{"id":"lu9LD2VC0nIT"},"source":["\n","\n","$\\quad$ Recall a [previous recipe](https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2021/10/21/GDL1.html#...-example:-permutations-and-local-averaging) for an equivariant function $F= \\Phi( X, A)$ using a local aggregation function $\\varphi$. \n","\n","In our present case of $\\Omega \\equiv G \\equiv C_n$, we may write this local aggregation more concretely as\n","$$\n","\\varphi ( x_u, X_{\\textsf{nbhd}(u)} ) = \\varphi( x_{u-1}, \\, x_u, \\, x_{u+1} ),\n","$$\n","with addition and subtraction in the indices above understood to be modulo $n$. \n","\n"," If in addition, we insist that $\\varphi$ is linear, then it has the form \n","$$\n"," \\varphi( x_{u-1}, \\, x_u, \\, x_{u+1} ) = \\theta_{-1} x_{u-1} + \\theta_0 x_u + \\theta_1 x_{u+1},\n","$$\n","and in this case we can express $F = \\Phi (X, A )$ through the following matrix multiplication:\n","$$\n","\\left[\n","\\begin{matrix}\n","\\theta_0 & \\theta_1 & \\text{ } & \\text{ } & \\theta_{-1} \\\\\n","\\theta_{-1} & \\theta_0 & \\theta_1 & \\text{ } &   \\text{ } \\\\\n","\\text{} & \\ddots & \\ddots & \\ddots & \\text{ } \\\\\n","\\text{ } & \\text{ } & \\theta_{-1} & \\theta_0 & \\theta_1 \\\\\n","\\theta_1 & \\text{ } & \\text{ } & \\theta_{-1} & \\theta_0 \n","\\end{matrix} \n","\\right]\n","\\left[\n","\\begin{matrix}\n","x_0 \\\\\n","x_1 \\\\\n","\\vdots \\\\\n","\\,x_{n-2} \\, \\\\\n","x_{n-1}  \n","\\end{matrix}\n","\\right]\n","$$\n","This multi-diagonal structure is often synonymous with the concept of weight sharing in ML literature. \n","\n","___\n","___"]},{"cell_type":"markdown","metadata":{"id":"-MK9npotqkY6"},"source":["## <font color=\"CornflowerBlue\">2.3 ... one-dimensional convolutions in torch</font>"]},{"cell_type":"markdown","metadata":{"id":"ILrpCldWl6mP"},"source":["The object in `torch` for executing convolutions of signals over $C_n$ is called `torch.nn.Conv1d`. We'll denote an instance of this object by $\\tilde{B}$ "]},{"cell_type":"markdown","metadata":{"id":"Dm0Zkj7_t9te"},"source":["___\n","\n","#### class ... `torch.nn.Conv1d`\n","\n","|      args        |           |            |   |   |\n","|--------------|-----------|------------|-----|----|\n","| `in_channels` | `out_channels`      | `kernel_size` | `stride = 1` |\n","|  `padding = 0`     | `dilation = 1`  | `groups = 1`      | `bias = True` |\n","| `padding_mode = 'zeros'` | `device = None` | `dtype = None` | \n"]},{"cell_type":"markdown","metadata":{"id":"tPkwuQOxJnSx"},"source":["Let us now relate the shapes of the input and output to the parameters\n","\n","\n","| input parameter      | LaTeX symbol |\n","| ----------- | ----------- |\n","| `in_channels`      | $\\text{dim}(\\mathcal{C})$     |\n","| `out_channels`   | $\\text{dim}(\\tilde{\\mathcal{C}})$        |\n","| `kernel_size`      | $k$       |\n","| `stride`   | $\\lambda$        |\n","| `padding`   | $\\rho$        |\n","| `dilation`      | $\\delta$       |\n","| `groups`   | $M$        \n","\n"]},{"cell_type":"markdown","metadata":{"id":"OTQF6DHD0Bo3"},"source":["This class specifies a [one-dimensional convolution operation](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html).  Because of batching and the channel dimension, the input and output are $3$-tensors. We use $N$ to denote the batch size, and we will continue to use $n$ for the input length.\n","\n","In general, $\\mathcal{C}$ and $\\mathcal{C}_1$ are the vector spaces of input channels and output channels respectively. In our current setting, assuming that signals $x$ are scalar signals over the cyclic group, one has $\\textrm{dim}(\\mathcal{C}) = 1$. The shape of the input tensor is thus\n","\n","$$\n","(N, 1, n)\n","$$\n","\n","Even though we restrict ourselves to scalar-valued input signals, we will allow $\\textrm{dim}(\\tilde{\\mathcal{C}}) > 1$, and we will write $\\tilde{n}$ to denote the length of the output, so that the shape of the output tensor is\n","\n","$$\n","(N, \\textrm{dim}(\\tilde{\\mathcal{C}}), \\tilde{n})\n","$$"]},{"cell_type":"markdown","metadata":{"id":"eSJ2KmfmMP3G"},"source":["For the moment, we will also take $N=1$. Thus, inputs can be though of effectively as vectors, which we can easily visualize. We will think of the output as a length-$\\textrm{dim}(\\tilde{\\mathcal{C}})$ list of vectors (with scalar entries).\n","\n","Our present goal is to try to understand how the number of learnable parameters depends on these shapes, as well as to visualize the effect of a convolutional layer in this simplest setting. "]},{"cell_type":"markdown","metadata":{"id":"MxpBmornN95r"},"source":["First, we remark that the relationship between $n_1$ and $n$, in terms of the input parameters, can be expressed as follows:\n","\n","$$\n","\\tilde{n} = \\left\\lfloor \\frac{ n + 2 \\rho - \\delta (k-1) -1 }{\\lambda} + 1 \\right\\rfloor\n","$$"]},{"cell_type":"markdown","metadata":{"id":"MO6s0UEGUTng"},"source":["___\n","\n","### method ... `compute_out_size` "]},{"cell_type":"code","metadata":{"id":"OrfbBi5TUQDV","executionInfo":{"status":"ok","timestamp":1635090323816,"user_tz":300,"elapsed":430,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}}},"source":["#collapse-hide\n","\n","def compute_out_size(in_size,\n","                     padding,\n","                     dilation,\n","                     kernel_size,\n","                     stride):\n","    numerator = in_size + 2 * padding - dilation * ( kernel_size - 1) - 1\n","\n","    arg_of_floor = (numerator / stride) + 1\n","\n","    return np.floor( arg_of_floor )"],"execution_count":47,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4jafE7PpUU7z"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"vJFVlMsiNEyV"},"source":["___\n","___\n","\n","### <font color=\"teal\">... example: concrete layer"]},{"cell_type":"markdown","metadata":{"id":"JikeXznQWDg1"},"source":["For now, we will instantiate a parameters dictionary with `dilation = 1` and `padding = 0`. Let us use $n = 32$, so that \n","\n","$$\n","\\tilde{n} = \\left\\lfloor \\frac{n-k}{\\lambda} + 1 \\right\\rfloor \\equiv \\left\\lfloor \\frac{32-k}{\\lambda} + 1 \\right\\rfloor\n","$$\n","\n","For our example, we specialize to `kernel_size = 4` and `stride = 2`, in which case $\\tilde{n} = 15$. \n"]},{"cell_type":"code","metadata":{"id":"6FSbS-lOZqd1","executionInfo":{"status":"ok","timestamp":1635091183014,"user_tz":300,"elapsed":356,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}}},"source":["odcp = {} # one dimensional conv. parameters\n","\n","odcp[\"in_size\"] = 32\n","odcp[\"in_channels\"] = 1\n","odcp[\"padding\"] = 0\n","odcp[\"dilation\"] = 1\n","odcp[\"kernel_size\"] = 4\n","odcp[\"stride\"] = 2\n","odcp[\"groups\"] = 1\n","odcp[\"out_channels\"] = 16"],"execution_count":52,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Iq59CwHEYZbc","executionInfo":{"status":"ok","timestamp":1635091184698,"user_tz":300,"elapsed":197,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"9870b13b-f352-4b1b-e531-9b63510f9ac3"},"source":["#collapse-hide\n","\n","odcp[\"out_size\"] = compute_out_size(\n","    in_size = odcp[\"in_size\"],\n","    padding = odcp[\"padding\"],\n","    dilation = odcp[\"dilation\"],\n","    kernel_size = odcp[\"kernel_size\"],\n","    stride = odcp[\"stride\"]\n","    )\n","\n","print( odcp[\"out_size\"] )"],"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["15.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"IfbpgN7L-t46"},"source":["One of the important free parameters we can choose to initialize a one-dimensional convolutional layer is the number of `out_channels`. We choose `out_channels = 16` in our first example. \n","\n","In many vision related CNNs, the first layer takes in a grayscale or RGB image, outputting features with a _much_ larger number of channels. \n","\n","> I am thinking of each output channel as a separate \"lint roller\" to run over the image. "]},{"cell_type":"markdown","metadata":{"id":"Mece-M4BBFgZ"},"source":["From the [docs](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html), the parameter `groups` \"controls the connections between inputs and outputs.\"\n","\n","Both `in_channels` and `out_channels` must be divisible by `groups`: \n","\n","* At `groups = 1`, all inputs are convolved to all outputs.\n","\n","* At `groups = 2`, the operation becomes equivalent to having two convolutional layers side by side, each seeing half the input channels and producing half the output channels, and both subsequently concatenated.\n","\n","* At `groups = in_channels`, each input channel is convolved with its own set of filters (of size `out_channels` / `in_channels`). \n","\n","By taking `in_channels = 1`, we must have `groups = 1` also. "]},{"cell_type":"markdown","metadata":{"id":"zJXR3Y8CO5z5"},"source":["The learned parameters of layer $\\tilde{B}$ are of two types: those in the weight tensor $\\tilde{\\theta}$, and those in the bias vector $\\tilde{b}$. At this point, for simplicity, we set $\\tilde{b} \\equiv 0$ via `bias = False`, so that the only learned parameters are those in $\\tilde{\\theta}$. "]},{"cell_type":"code","metadata":{"id":"jfUZNcTt7G94","executionInfo":{"status":"ok","timestamp":1635092210348,"user_tz":300,"elapsed":3,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}}},"source":["from torch.nn import Conv1d"],"execution_count":56,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pm1OldthEuoK"},"source":["We now initialize the convolutional layer. "]},{"cell_type":"code","metadata":{"id":"l6iKKG0U8DNm","executionInfo":{"status":"ok","timestamp":1635093504319,"user_tz":300,"elapsed":410,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}}},"source":["B_tilde = Conv1d(in_channels = odcp[\"in_channels\"],\n","                 out_channels = odcp[\"out_channels\"],\n","                 kernel_size = odcp[\"kernel_size\"],\n","                 stride = odcp[\"stride\"],\n","                 padding = odcp[\"padding\"],\n","                 dilation = odcp[\"dilation\"],\n","                 groups = odcp[\"groups\"],\n","                 bias = False\n","                )"],"execution_count":85,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HOsMkIV0FN5W"},"source":["Let us initialize a few possible input signals: `X_1`, `X_2`, `X_3`."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":844},"id":"qbstZr1dFZsb","executionInfo":{"status":"ok","timestamp":1635094476813,"user_tz":300,"elapsed":972,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"aad6df18-1f63-40ee-a88a-bbafaaf44af1"},"source":["#collapse-hide\n","\n","X_1 = torch.arange(0,32)\n","info(X_1, \"X 1\")\n","viz_tens( X_1, display_size = 10 )\n","\n","X_2 = torch.rand( (32) ) \n","info(X_2, \"X 2\")\n","viz_tens( X_2, display_size = 10 )\n","\n","X_3 = torch.randn( (32) )\n","info(X_3, \"X 3\")\n","viz_tens( X_3, display_size = 10 )"],"execution_count":102,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor     X 1 \n","\n","    num. dims        1\n","    num. entries     32\n","    shape            [32]\n","\n","\n","┌───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┐\n","│ 0 │ 1 │ 2 │ 3 │ 4 │ 5 │ 6 │ 7 │ 8 │ 9 │ 10 │ 11 │ 12 │ 13 │ 14 │ 15 │ 16 │ 17 │ 18 │ 19 │ 20 │ 21 │ 22 │ 23 │ 24 │ 25 │ 26 │ 27 │ 28 │ 29 │ 30 │ 31 │\n","└───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┘\n","\n","\n","\n","tensor     X 2 \n","\n","    num. dims        1\n","    num. entries     32\n","    shape            [32]\n","\n","\n","┌──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┐\n","│ 0.25 │ 0.69 │ 0.89 │ 0.36 │ 0.29 │ 0.04 │ 0.24 │ 0.06 │ 0.38 │ 0.60 │ 0.03 │ 0.93 │ 0.81 │ 0.01 │ 0.26 │ 0.66 │ 0.39 │ 0.44 │ 0.27 │ 0.90 │ 0.22 │ 0.91 │ 0.53 │ 0.60 │ 0.89 │ 0.41 │ 0.21 │ 0.41 │ 0.90 │ 0.12 │ 0.61 │ 0.00 │\n","└──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┘\n","\n","\n","\n","tensor     X 3 \n","\n","    num. dims        1\n","    num. entries     32\n","    shape            [32]\n","\n","\n","┌──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┐\n","│ -0.2 │ 1.51 │ 0.03 │ 1.03 │ 0.88 │ -1.3 │ -0.3 │ 1.01 │ -1.6 │ -0.0 │ -1.2 │ 1.19 │ -0.7 │ 1.11 │ 0.29 │ 0.29 │ -0.5 │ 1.94 │ -0.4 │ -0.3 │ 1.44 │ -0.7 │ 2.49 │ -0.3 │ 0.87 │ 0.21 │ 1.22 │ -0.5 │ -0.1 │ -0.4 │ -0.1 │ -0.2 │\n","└──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┘\n","\n","\n","\n"]},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAjwAAAAfCAYAAADjqBcrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAA30lEQVR4nO3bwQnDMBBFwSikDvffpVJBcsmKmMfMVfDZ48Pgtfd+AACUPf99AADAaYIHAMgTPABAnuABAPIEDwCQ9/r2uNb6+Reuqb/AJnbccu+NqR23nNuY2rnLxtSOW+69MbXjlnMbUzvXda1Pb77wAAB5ggcAyBM8AECe4AEA8gQPAJAneACAPMEDAOQJHgAgT/AAAHmCBwDIEzwAQJ7gAQDyBA8AkCd4AIA8wQMA5AkeACBP8AAAeWvv/e8bAACO8oUHAMgTPABAnuABAPIEDwCQJ3gAgDzBAwDkvQHJm2Q54H0SmgAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 720x720 with 1 Axes>"]},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAjwAAAAfCAYAAADjqBcrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABfElEQVR4nO3bIYpCYRSG4eMw0eROREEwic3sHjRaDS7AVYhbsFvsJnEDFqsrkDsrmCnnl4HD89QLHycovlyw13VdAABU9vXfBwAAfJrgAQDKEzwAQHmCBwAoT/AAAOV9//VwPp+n/8K12WyyExERMR6P0xvb7bbBJRGv1yu9cblcGlwSMRwO0xu32y29sVwu0xsREZPJJL3R6jP3fD7TG4/Ho8ElEaPRKL0xm83yh0TEer1Ob5zP5/RGv99Pb0RE3O/39MbxeMwfEm2+R6fTqcElEYfDIb0xGAzSG4vFIr0REbHf79Mb1+u1wSVtfs92u12DSyKm02l6o9Utq9UqvfF+v3u/PfOGBwAoT/AAAOUJHgCgPMEDAJQneACA8gQPAFCe4AEAyhM8AEB5ggcAKE/wAADlCR4AoDzBAwCUJ3gAgPIEDwBQnuABAMoTPABAeYIHACiv13Xdf98AAPBR3vAAAOUJHgCgPMEDAJQneACA8gQPAFCe4AEAyvsBRL83OchkoxIAAAAASUVORK5CYII=\n","text/plain":["<Figure size 720x720 with 1 Axes>"]},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAjwAAAAfCAYAAADjqBcrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABdUlEQVR4nO3bsYkCYRCG4dnzAhO7MBPMxAoMTQ03tBADG9EWxBbMNDS3AsF8reAumV+E4XlS4WMwWF4WthuGIQAAKvv59gEAAJ8meACA8gQPAFCe4AEAyhM8AEB5v//9eDqd0p9wXa/X7EREROz3+/RGqy/S7vd7eqPV//J6vdIbfd+nNzabTXojIuJ4PKY3brdbg0sittttemM8Hje4JOLxeKQ35vN5g0siuq5Lb0yn0/TGbrdLb0REHA6H9MZkMmlwSZvnwuVyaXBJxGq1Sm8sl8v0xmw2S29EtHlGjUajBpe0+V9aPJ8iIs7nc3pjvV43uCTi+XymNxaLxZ8PKG94AIDyBA8AUJ7gAQDKEzwAQHmCBwAoT/AAAOUJHgCgPMEDAJQneACA8gQPAFCe4AEAyhM8AEB5ggcAKE/wAADlCR4AoDzBAwCUJ3gAgPK6YRi+fQMAwEd5wwMAlCd4AIDyBA8AUJ7gAQDKEzwAQHmCBwAo7w1j7jc5RgUSngAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 720x720 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"ZQ3U5y7_G-Pz"},"source":["To apply `B_tilde` to these signals, we must augment each signal with batch and channel dimensions:"]},{"cell_type":"code","metadata":{"id":"IxesqTgVH_nN","executionInfo":{"status":"ok","timestamp":1635094482168,"user_tz":300,"elapsed":139,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}}},"source":["#collapse-hide\n","\n","X_1 = torch.as_tensor( X_1[ None, None, : ], dtype= torch.float32 )\n","X_2 = torch.as_tensor( X_2[ None, None, : ], dtype= torch.float32 )\n","X_3 = torch.as_tensor( X_3[ None, None, : ], dtype= torch.float32 )"],"execution_count":103,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uYl7uXZUIciJ"},"source":["Each output"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PL0YP2_qIZLF","executionInfo":{"status":"ok","timestamp":1635094483772,"user_tz":300,"elapsed":165,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"ec66650e-9c3e-4c3d-83fa-f366380334d7"},"source":["Y_1 = B_tilde( X_1 ).detach()\n","Y_2 = B_tilde( X_2 ).detach()\n","Y_3 = B_tilde( X_3 ).detach()\n","\n","#np.array( obj.size() )\n","\n","print( np.array( Y_1.size() ) )"],"execution_count":104,"outputs":[{"output_type":"stream","name":"stdout","text":["[ 1 16 15] [ 1 16 15] [ 1 16 15]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LJXO6rZxKwAy","executionInfo":{"status":"ok","timestamp":1635093934815,"user_tz":300,"elapsed":5,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"4af0196e-6254-47dd-f9e4-946902268890"},"source":["info(Y_1, \"Y 1\")"],"execution_count":100,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor     Y 1 \n","\n","    num. dims        3\n","    num. entries     240\n","    shape            [ 1 16 15]\n","\n","\n","┌──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┐\n","│ [-0. │ [-0. │ [-1. │ [0.6 │ [0.4 │ [0.0 │ [0.6 │ [1.8 │ [-1. │ [-0. │ [1.4 │ [-0. │ [0.0 │ [1.0 │ [1.3 │ [-2. │\n","└──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┘\n","\n","\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"srN-YFWvRZ53"},"source":["Recall that, in general, one has\n","\n","$$\n","\\tilde{\\theta} \\in \\mathcal{X} ( H \\backslash G / \\tilde{H}, \\mathcal{C} \\otimes \\tilde{\\mathcal{C}} ),\n","$$\n","\n","but let us reduce this further based on the assumptions made. Because $G \\equiv \\Omega \\equiv C_n$, subgroup $H$ must be the trivial subgroup $\\{ e \\}$. Moreover, having assumed that $\\dim{\\mathcal{C}} = 1$, one also has $\\mathcal{C} \\otimes \\tilde{\\mathcal{C}} \\cong \\tilde{\\mathcal{C}}$, so we may write\n","\n","$$\n","\\tilde{\\theta} \\in \\mathcal{X} ( G / \\tilde{H} ) , \\tilde{\\mathcal{C}} ),\n","$$\n","\n","with $C_{\\tilde{n}} \\cong G / \\tilde{H}$."]},{"cell_type":"markdown","metadata":{"id":"xRWrCHbGdXJZ"},"source":["## <font color=\"CornflowerBlue\">2.4 ... two-dimensional convolutions in torch</font>"]},{"cell_type":"markdown","metadata":{"id":"oLK3vk7KuBFQ"},"source":["____\n","\n","#### `torch.nn.Conv2d`"]},{"cell_type":"markdown","metadata":{"id":"njwaNoPCN09d"},"source":["\n","The arguments:\n","\n","* `in_channels`\n","\n","* `out_channels`\n","\n","* `kernel_size`\n","\n","* `stride` $\\quad$ controls the stride for the cross-correlation, a single number or a tuple. \n","\n","* `padding` $\\quad$ controls amount of padding applied to the input. It can either be a string, `\"valid\"` or `\"same\"` or a tuple of ints giving the amount of implicit padding applied on both sides. \n","\n","* `dilation` $\\quad$ controls the spacing between kernel points; \"also known as the a trous algorithm\n","\n","* `groups` $\\quad$ controls connections between inputs and outputs. The `in_channels` and `out_channels` must be divisible by `groups`. For example,\n","\n","    * At groups = 1, all inputs are convolved to all outputs\n","\n","    * At groups = 2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated. \n","\n","    * At groups = `in_channels`, each input channel is convolved with its own set of filters (of size `out_channels // in_channels`)\n","\n","* `bias`\n","\n","* `padding_mode`,\n","\n","* `device`,\n","\n","* `dtype`"]},{"cell_type":"markdown","metadata":{"id":"8AIKdP8WPa9i"},"source":["Let us now relate the shapes of the input and output to the parameters\n","\n","\n","| input parameter      | LaTeX symbol |\n","| ----------- | ----------- |\n","| `in_channels`      | $\\text{dim}(\\mathcal{C})$     |\n","| `out_channels`   | $\\text{dim}(\\mathcal{C}_1)$        |\n","| `kernel_size`      | $k$       |\n","| `stride`   | $\\lambda$        |\n","| `padding`   | $\\rho$        |\n","| `dilation`      | $\\delta$       |\n","| `groups`   | $M$        \n","\n"]},{"cell_type":"markdown","metadata":{"id":"b7jNhkTvPgoH"},"source":["Additionally, we use $N$ for the batch size of the input, `N`. We also let $(h,w)$ denote the height-width pair describing the shape of the input signal domain. \n","\n","Correspondingly, we write $(h_1, w_1)$ for the height-width pair describing the shape of the output signal domain. "]},{"cell_type":"markdown","metadata":{"id":"hTqm3oVlPuGI"},"source":["We remark that the stride can be either integer or a $2$-tuple, whose coordinates describe the vertical and horizontal stride respectively. We still write $\\lambda$ for the stride when it is a tuple, and use $\\lambda_h \\equiv \\lambda[0]$ and $\\lambda_w \\equiv \\lambda[1]$ to denote its first and second coordinate, in this case. Likewise, the padding and kernel size may be $2$-tuples as well, and we use similar notation to denote their entries.  "]},{"cell_type":"markdown","metadata":{"id":"RNqduAQrPwZh"},"source":["The full shape of the input to the layer includes the batch dimension, and is thus\n","\n","$$\n","(N, \\text{dim}(\\mathcal{C}), H, W) \\,,\n","$$\n","\n","while the shape of the output is\n","\n","$$\n","(N , \\text{dim}(\\mathcal{C}_1), H_1, W_1 )\n","$$\n","\n","These shapes, in particular the spatial dimensions of each, are related as follows: \n","\n","$\\begin{align}\n","H_1 &= \\left\\lfloor \\frac{\n","    H + 2 \\rho_h - \\delta_h ( k_h -1) -1 }{\\lambda_h}\n","\\right\\rfloor \\\\\n","W_1 &= \\left\\lfloor \\frac{\n","    W + 2 \\rho_w - \\delta_w ( k_w -1) -1 }{\\lambda_w}\n","\\right\\rfloor\n","\\end{align}$,\n","\n","in particular, the batch size does not have any bearing on how the shapes of tensors transform. "]},{"cell_type":"markdown","metadata":{"id":"h4qqcExeP3OC"},"source":["The parameters to be learned are the weights $w^1$ and biases $b^1$. These are both `Tensor` objects, accessed from the layer as `Conv2d.weight` and `Conv2d.bias`. The shape of the weight tensor is\n","\n","$$\n","\\textrm{shape}(w^1) =\n","\\left( \\, \\text{dim}(\\mathcal{C}_1),  \\, \\text{dim}(\\mathcal{C}) \\big/ M , k_h, k_w \\right)\n","$$\n","\n","The tensor $w^1$ thus has \n","\n","$$\n","\\textrm{size}(w^1) = \\textrm{dim}(\\mathcal{C}_1) \\textrm{dim} (\\mathcal{C}) k_h k_w \\big/ M\n","$$\n","\n","scalar entries. \n","\n","There is always the question of how to initialize weights. In the case of the `Conv2d` class, the weights are initialized to be i.i.d. $\\text{Unif}( - \\sqrt{ \\alpha_1}, \\sqrt{\\alpha_1} )$ random variables, where\n","\n","$$\n","\\alpha_1 := \\frac{ \\textrm{dim}(\\mathcal{C}_1) }{\\textrm{size}(w^1)}\n","$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zQDEqpPSP-Cd"},"source":["The bias tensor is a much smaller object, we have \n","\n","$\n","\\begin{align}\n","\\textrm{shape}(b^1) = (\\, \\textrm{dim}(\\mathcal{C}_1  ) \\,) \\, , \\quad \\textrm{size}(b^1) = \\textrm{dim}(\\mathcal{C}_1)\n","\\end{align}\n","$"]},{"cell_type":"markdown","metadata":{"id":"N9IpVQKjQCAl"},"source":["Despite this, we use the same initialization (with mutual independence of all random variables in discussion) for the bias entries as we did for the weights. "]},{"cell_type":"markdown","metadata":{"id":"HRn89dm915m7"},"source":["## ... A simple CNN"]},{"cell_type":"markdown","metadata":{"id":"_zxicBxC6VEw"},"source":["We consider possibly the simplest neural network that we can construct through the above blueprint. Suppose we have a binary classification problem, with the following hypothesis space. Let $\\textsf{H}_1$ denote the hypothesis space of functions $f : \\mathcal{X}( C_n, \\mathbb{R}) \\to \\{0,1\\}$ of the form \n","\n","$$\n","f = A \\circ P \\circ \\mathbf{a} \\circ B \\,,\n","$$\n","where the components of $f$ are "]},{"cell_type":"markdown","metadata":{"id":"g4DGJSDa6y2k"},"source":["where the components of $f$ are \n","\n","\n","* $B$  : $\\quad$ A $C_n$-equivariant function, to be learned. It is represented as a circulant matrix $\\mathbf{C}(\\theta)$, where $\\theta$ is a vector $\\theta \\equiv (\\theta_0, \\dots, \\theta_{n-1})$ whose entries $\\theta_j$ are parameters to be learned. \n","\n","* $ \\mathbf{a} $ : $\\quad$ We consider the ReLU activation function, $a : \\mathbb{R} \\to \\mathbb{R}_{\\geq\\, 0}$ defined by $a(w) = \\max(0,w)$, for $w \\in \\mathbb{R}$. The bold-face $\\mathbf{a}$ denotes the entry-wise action of this function on a given vector;for $y \\equiv (\\,y_1, \\,\\dots, \\, y_n \\, ) \\in \\mathcal{X}(C_n, \\mathbb{R})$, which we imagine as the output of $B(x)$ for some input signal $x$, we have $\\mathbf{a} (y ) = ( \\,  \\max(0,y_1), \\,  \\dots, \\, \\max(0,y_n) )$. There are no learned parameters in this layer. \n","\n","* $P$ : $\\quad$ A coarsening operator. In this case, let us say it is a _zero-padded group homomorphism_. \n","\n"," $P : C_n \\to C_{n / d }$ for some divisor $d \\mid n$ \\footnote{zero-padding} , and let us say that it operates through max-pooling on the signal, over the pre-images of each element of $C_{n / d}$. \n","\n","* $A$ : $\\quad$ A global-pooling layer. We assume this has the form of a fully-connected layer, followed by a softmax. Specifically,"]},{"cell_type":"code","metadata":{"id":"HjLRoZui8f2Y"},"source":[""],"execution_count":null,"outputs":[]}]}