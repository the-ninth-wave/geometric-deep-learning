{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2020-05-04-GDL4.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"}},"cells":[{"cell_type":"markdown","metadata":{"id":"XZuXYfmJPu3l"},"source":["# 4\n","> GDL\n","\n","- toc: true \n","- badges: true\n","- comments: false\n","- categories: [jupyter]\n"]},{"cell_type":"markdown","metadata":{"id":"-MK9npotqkY6"},"source":["## __1.4 $\\dots$ GDL blueprint (with fixed input domain)__"]},{"cell_type":"markdown","metadata":{"id":"OGYooKiLqqOG"},"source":["### $\\quad$ *__Setup__*"]},{"cell_type":"markdown","metadata":{"id":"nf_Dn2u0q2Ix"},"source":["Our formal treatment of a classification problem requires the following objects: \n","\n","* $\\Omega$ : A graph with $n$ vertices. This is the domain of the signals considered.\n","\n","* $\\mathcal{X}(\\Omega,\\mathbb{R}^s)$ :  The space of $s$-channel signals $x : \\Omega \\to \\mathbb{R}^s$. \n","\n","* $(e_j)_{j\\,=\\,1}^{\\,n}$ : A 'spatial' coordinate system for scalar-valued signals $\\mathcal{X} \\to \\mathbb{R}$. \n","\n","* $(z_c)_{c \\,=\\,1}^{\\,s}$ : A channel coordinate system. \n","\n","* $G$ : A group $G$ acting on $\\Omega$, via $\\Omega \\ni u \\mapsto g.u \\in \\Omega$. This induces the `pointwise' action on the space of signals $\\mathcal{X}(\\Omega,\\mathbb{R}^s)$, denoted $x \\mapsto \\mathbf{g}.x$. \n","\n","* $\\rho$ : The representation of the induced action of $G$, $x \\mapsto \\mathbf{g}.x$. \n","\n","The basis \n","$$\n","\\{ e_j \\otimes z_c : 1 \\leq j \\leq n,\\, 1 \\leq c \\leq s  \\}\n","$$ \n","\n","is the one in which signals are expressed to a computer. Given $x \\in \\mathcal{X}(\\Omega,\\mathbb{R}^s)$. In terms of this basis, one can write $x$ as\n","$$\n","x = \\mathbf{X}^{cj} e_j \\otimes z_c ,\n","$$\n","Note that $\\mathbf{X}$ is the so-called design matrix previously introduced. \n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YhPlpC0Hq_uW"},"source":["$\\vdots$\n","\n","Let $g \\in G$. In the basis $e_j \\otimes z_c$, we can then identify $\\rho(g)$ with a tensor... _(say more)_\n","\n","$\\vdots$"]},{"cell_type":"markdown","metadata":{"id":"Qnc4rF7SzpZz"},"source":["### $\\quad$ *__coarsening operator__*"]},{"cell_type":"markdown","metadata":{"id":"acray6hZy_ds"},"source":["Let $\\Omega$ and $\\Omega'$ be domains, $G$ a symmetry group over $\\Omega$, and write $\\Omega' \\prec \\Omega$ if $\\Omega'$ arises from $\\Omega$ through some coarsening operator (presumably this coarsening operator needs to commute with the group action)."]},{"cell_type":"markdown","metadata":{"id":"V-jGOrO2yt2c"},"source":["### $\\quad$ *__GDL block__*"]},{"cell_type":"markdown","metadata":{"id":"XrWaKZqkzu9g"},"source":["1. _linear $G$-equivariant layer_\n","\n","$$\n","B : \\mathcal{X}(\\Omega, \\mathcal{C}) \\to \\mathcal{X}(\\Omega' , \\mathcal{C}')\n","$$ \n","such that $B(g.x) = g.B(x)$ for all $g \\in G$ and $x \\in \\mathcal{X}(\\Omega, \\mathcal{C})$.\n","\n","2. _nonlinearity_, or _activation function_ \n","$$\n","a : \\mathcal{C} \\to \\mathcal{C}'\n","$$ \n","applied domain-pointwise as $(\\mathbf{a}(x))(u) = a(x(u))$.\n","\n","3. _local pooling operator_ or _coarsening operator_ \n","$$\n","P : \\mathcal{X}(\\Omega, \\mathcal{C}) \\to \\mathcal{X}(\\Omega', \\mathcal{C})\n","$$ \n","which gives us our notion $\\Omega' \\prec \\Omega$."]},{"cell_type":"markdown","metadata":{"id":"YSPpUeW20mzm"},"source":["The last ingredient is a global pooling layer applied last, compositionally. \n","\n","4. _$G$-invariant layer_, or _global pooling layer_\n","$$\n","A : \\mathcal{X} (\\Omega, \\mathcal{C}) \\to \\mathcal{Y}\n","$$ \n","satisfying $A(g.x) = A(x)$ for all $g \\in G$ and $x \\in \\mathcal{X}(\\Omega, \\mathcal{C})$. "]},{"cell_type":"markdown","metadata":{"id":"BKUg4pFJ01wd"},"source":["### $\\quad$ *__Hypothesis space__*"]},{"cell_type":"markdown","metadata":{"id":"Kl-PFkBY08_G"},"source":["These objects can be used to define a class of $G$-invariant functions $f: \\mathcal{X}(\\Omega, \\mathcal{C}) \\to \\mathcal{Y}$ of the form\n","$$\n","f = A \\circ \\mathbf{a}_J \\circ B_J \\circ P_{J-1} \\circ \\dots \\circ P_1 \\circ \\mathbf{a}_1 \\circ B_1 ,\n","$$\n","where the blocks are selected so that the output space of each block matches the input space of the next one. Different blocks may exploit different choices of symmetry groups $G$."]},{"cell_type":"markdown","metadata":{"id":"AT4Fjtgc1Gte"},"source":["### $\\quad$ *__Discussion__*"]},{"cell_type":"markdown","metadata":{"id":"3dPyxQy21M3W"},"source":["Shift-invariance arises naturally in vision and pattern recognition. In this case, the desired function $f \\in \\textsf{H}$, typically implemented as a CNN, inputs an image and outputs the probability of the image to contain an object from a certain class. It is often reasonably assumed that the classification result should not be affected by the position of the object in the image, i.e., the function $f$ must be shift-invariant.\n","\n","Multi-layer perceptrons lack this property, a reason why early (1970s) attempts to apply these architectures to pattern recognition problems failed. The development of NN architectures with local weight sharing, as epitomized by CNNs, was among other reasons motivated by the need for shift-invariant object classification. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"jhkHQknb1Zdy"},"source":["A prototypical application requiring shift-equivariance is image segmentation, where the output of $f$ is a pixel-wise image mask. This segmentation mask must follow shifts in the input image. In this example, the domains of the input and output are the same, but since the input has three color channels while the output has \\emph{one channel per class}, the representations $(\\rho, \\mathcal{X}(\\Omega, \\mathcal{C}) )$ and $(\\rho', \\mathcal{Y} \\equiv \\mathcal{X}(\\Omega, \\mathcal{C}'))$ are somewhat different. "]},{"cell_type":"markdown","metadata":{"id":"yGp03s9H1aoe"},"source":["When $f$ is implemented as a CNN, it may be written as a composition of $L$ functions, where $L$ is determined by the depth and other hyperparameters:\n","$$\n","f = f_L \\circ f_{L-1} \\circ \\dots \\circ f_2 \\circ f_1 .\n","$$\n","\n","Examining the individual layer functions making up CNN, one finds they are not shift-invariant in general but rather shift-equivariant. The last function applied, namely $f_L$, is typically a ``global-pooling\" function that is genuinely shift-invariant, causing $f$ to be shift-invariant, but to focus on this ignores the structure we will leverage for purposes of expressivity and regularity. "]}]}