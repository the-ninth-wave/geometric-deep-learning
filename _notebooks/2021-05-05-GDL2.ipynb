{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2021-05-05-GDL5.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"}},"cells":[{"cell_type":"markdown","metadata":{"id":"XZuXYfmJPu3l"},"source":["# 5\n","> GDL\n","\n","- toc: true \n","- badges: true\n","- comments: false\n","- categories: [jupyter]\n"]},{"cell_type":"markdown","metadata":{"id":"e6t_Gqemxl1t"},"source":["# __2 $\\quad$ Geometric deep learning models__  "]},{"cell_type":"markdown","metadata":{"id":"-MK9npotqkY6"},"source":["## __2.1 $\\quad$ Learning with scalar signals on a cyclic group__"]},{"cell_type":"markdown","metadata":{"id":"Gl2miszAx3ki"},"source":["In the simplest setting we consider, the underlying domain is a one-dimensional grid, and the signals only have a single channel. We can identify this grid $\\Omega$ with the Cayley graph of cyclic group\n","$$\n","C_n = \\langle \\, a : a^n = 1 \\, \\rangle \\equiv \\{ \\, 1, a, a^2, \\dots, a^{n-1} \\, \\}.\n","$$\n","It is convenient to parametrize the group, and hence the grid, through the exponent of the generator \n","$$\n","C_n \\equiv \\{ 0, 1, \\dots, n -1 \\}\n","$$\n","as this indexing is consistent with the way most programming languages index vectors, reinterpreting the group operation as addition modulo $n$."]},{"cell_type":"markdown","metadata":{"id":"y1QX0iPUyVZd"},"source":["The vector space of single-channeled (i.e. real-valued) signals\n","$$\n","\\mathcal{X}(C_n,\\mathbb{R}) = \\{ x : C_n \\to \\mathbb{R} \\} ,\n","$$\n","is finite dimensional, and each $x \\in \\mathcal{X}(C_n, \\mathbb{R})$ may be expressed as \n","$$\n","x = \n","\\left[ \n","\\begin{matrix}\n","x_0\\\\ \n","\\vdots\\\\\n","\\,x_{n-1}\\,\n","\\end{matrix}\n","\\right] \n","$$\n","with respect to some implicit coordinate system used by the computer, the _input coordinate system_. This is the same coordinate system used to express the representation $\\rho$ of translation group $G \\equiv C_n$, which we now describe."]},{"cell_type":"markdown","metadata":{"id":"H-bA8Ykqywt4"},"source":["Given a vector $\\theta = (\\theta_0 , \\dots, \\theta_{n-1})$, recall the associated _circulant matrix_ is the $n \\times n$ matrix with entries \n","$$\n","\\mathcal{C}(\\theta) := \\left( \\, \\theta_{ (u - v) \\mod n} \\right)_{ 0 \\, \\leq \\,u,\\,v \\, \\leq n-1 } \n","$$"]},{"cell_type":"markdown","metadata":{"id":"_Ilur6U0zVps"},"source":["Consider the case of $\\theta_S := (0,1,0,\\dots, 0)^T$, the associated circulant matrix, $\\mathbf{S} := \\mathbf{C}(\\theta_S)$ acts on vectors by shifting the entries of vectors to the right by one position, modulo $n$. This is a shift or translation operator, which we denote $\\mathbf{S}$. "]},{"cell_type":"markdown","metadata":{"id":"VACCLRh_zaU2"},"source":["___\n","__Lemma__ $\\quad$\n","A matrix is circulant if and only if it commutes with $\\mathbf{S}$. Moreover, given any two vectors $\\theta, \\eta \\in \\mathbb{R}^n$, one has $\\mathbf{C}(\\theta) \\mathbf{C}(\\eta) = \\mathbf{C}(\\eta) \\mathbf{C}(\\theta)$. \n","\n","___"]},{"cell_type":"markdown","metadata":{"id":"2BOxnG0Pz2YD"},"source":["The importance of $\\mathbf{S}$ to the present discussion is that it generates a group isomorphic to the one-dimensional translation group $C_n$. This is to say, a natural representation of $C_n = \\langle \\, a : a^n = 1 \\, \\rangle$ to consider is the group isomorphism induced by mapping the generator $a$ of $C_n$ to $\\mathbf{S}$. Specifically, the representation $\\rho$ of $G$ over $\\mathcal{X}( C_n, \\mathbb{R})$ is given by\n","$$\n","\\rho ( a^j ) := \\mathbf{S}^j \n","$$"]},{"cell_type":"markdown","metadata":{"id":"DTf8ASH40Qqb"},"source":["___\n","\n","__Corollary__ $\\quad$ Any $f : \\mathcal{X}(C_n, \\mathbb{R}) \\to \\mathcal{X}(C_n,\\mathbb{R})$ which is linear and $C_n$-equivariant can be expressed ( in the input coordinate system ) as an $n \\times n$ circulant matrix $\\mathbf{C}(\\theta)$ for some vector $\\theta$.\n","\n","___\n"]},{"cell_type":"markdown","metadata":{"id":"lu9LD2VC0nIT"},"source":["___\n","___\n","\n","__Example__ $\\quad$ \n","Our previous recipe for designing an equivariant function $F= \\Phi( \\mathbf{X}, \\mathbf{A})$ using a local aggregation function $\\varphi$. In this case, we can express\n","$$\n","\\varphi ( \\mathbf{x}_u, \\mathbf{X}_{\\mathcal{N}(u)} ) = \\varphi( \\mathbf{x}_{u-1}, \\, \\mathbf{x}_u, \\, \\mathbf{x}_{u+1} ),\n","$$\n","where the addition and subtraction in the indices above is understood to be modulo $n$. \n","\n"," If in addition, we insist that $\\varphi$ is linear, then it has the form \n","$$\n"," \\varphi( \\mathbf{x}_{u-1}, \\, \\mathbf{x}_u, \\, \\mathbf{x}_{u+1} ) = \\theta_{-1} \\mathbf{x}_{u-1} + \\theta_0 \\mathbf{x}_u + \\theta_1 \\mathbf{x}_{u+1},\n","$$\n","and in this case we can express $\\mathbf{F} = \\Phi (\\mathbf{X}, \\mathbf{A} )$ through the following matrix multiplication:\n","$$\n","\\left[\n","\\begin{matrix}\n","\\theta_0 & \\theta_1 & \\text{ } & \\text{ } & \\theta_{-1} \\\\\n","\\theta_{-1} & \\theta_0 & \\theta_1 & \\text{ } &   \\text{ } \\\\\n","\\text{} & \\ddots & \\ddots & \\ddots & \\text{ } \\\\\n","\\text{ } & \\text{ } & \\theta_{-1} & \\theta_0 & \\theta_1 \\\\\n","\\theta_1 & \\text{ } & \\text{ } & \\theta_{-1} & \\theta_0 \n","\\end{matrix} \n","\\right]\n","\\left[\n","\\begin{matrix}\n","\\mathbf{x}_0 \\\\\n","\\mathbf{x}_1 \\\\\n","\\vdots \\\\\n","\\,\\mathbf{x}_{n-2} \\, \\\\\n","\\mathbf{x}_{n-1}  \n","\\end{matrix}\n","\\right]\n","$$\n","This special multi-diagonal structure is sometimes referred to as ``weight sharing\" in the machine learning literature. \n","\n","___\n","___"]},{"cell_type":"markdown","metadata":{"id":"j0N6Rklm1Sn0"},"source":["Circulant matrices are synonymous with discrete convolutions; for $x \\in \\mathcal{X}(\\Omega,\\mathbb{R})$ and $\\theta \\in \\mathbb{R}^n$, their _convolution_ $x \\star \\theta$ is defined by \n","$$\n","( x \\star \\theta )_u := \\sum_{v = 0}^{n-1} x_{v \\mod n}\\, \\theta_{ (u-v) \\mod n}  \\, ,\n","$$\n","$$\n","\\equiv \\mathbf{C}(\\theta) x \n","$$\n"]},{"cell_type":"markdown","metadata":{"id":"Lmh-AZIM1jXB"},"source":["___\n","\n","__Rmk__ $\\quad$ This leads to an alternate, equivalent definition of convolution as a translation equivariant linear operation. Moreover by replacing translations by a more general group $G$, one can generalize convolution to settings whose domain has symmetry other than translational. \n","___ "]},{"cell_type":"markdown","metadata":{"id":"HRn89dm915m7"},"source":["## __2.2 $\\quad$ A simple CNN__"]},{"cell_type":"markdown","metadata":{"id":"_zxicBxC6VEw"},"source":["We consider possibly the simplest neural network that we can construct through the above blueprint. Suppose we have a binary classification problem, with the following hypothesis space. Let $\\textsf{H}_1$ denote the hypothesis space of functions $f : \\mathcal{X}( C_n, \\mathbb{R}) \\to \\{0,1\\}$ of the form \n","\n","$$\n","f = A \\circ P \\circ \\mathbf{a} \\circ B \\,,\n","$$\n","where the components of $f$ are "]},{"cell_type":"markdown","metadata":{"id":"g4DGJSDa6y2k"},"source":["where the components of $f$ are \n","\n","\n","* $B$  : $\\quad$ A $C_n$-equivariant function, to be learned. It is represented as a circulant matrix $\\mathbf{C}(\\theta)$, where $\\theta$ is a vector $\\theta \\equiv (\\theta_0, \\dots, \\theta_{n-1})$ whose entries $\\theta_j$ are parameters to be learned. \n","\n","* $ \\mathbf{a} $ : $\\quad$ We consider the ReLU activation function, $a : \\mathbb{R} \\to \\mathbb{R}_{\\geq\\, 0}$ defined by $a(w) = \\max(0,w)$, for $w \\in \\mathbb{R}$. The bold-face $\\mathbf{a}$ denotes the entry-wise action of this function on a given vector;for $y \\equiv (\\,y_1, \\,\\dots, \\, y_n \\, ) \\in \\mathcal{X}(C_n, \\mathbb{R})$, which we imagine as the output of $B(x)$ for some input signal $x$, we have $\\mathbf{a} (y ) = ( \\,  \\max(0,y_1), \\,  \\dots, \\, \\max(0,y_n) )$. There are no learned parameters in this layer. \n","\n","* $P$ : $\\quad$ A coarsening operator. In this case, let us say it is a _zero-padded group homomorphism_. \n","\n"," $P : C_n \\to C_{n / d }$ for some divisor $d \\mid n$ \\footnote{zero-padding} , and let us say that it operates through max-pooling on the signal, over the pre-images of each element of $C_{n / d}$. \n","\n","* $A$ : $\\quad$ A global-pooling layer. We assume this has the form of a fully-connected layer, followed by a softmax. Specifically,"]},{"cell_type":"code","metadata":{"id":"HjLRoZui8f2Y"},"source":[""],"execution_count":null,"outputs":[]}]}