{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2021-10-02-GDL2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"}},"cells":[{"cell_type":"markdown","metadata":{"id":"XZuXYfmJPu3l"},"source":["# 2\n","> GDL\n","\n","- toc: true \n","- badges: true\n","- comments: false\n","- categories: [jupyter]\n"]},{"cell_type":"markdown","metadata":{"id":"e6t_Gqemxl1t"},"source":["# __2 $\\quad$ Geometric deep learning models__  "]},{"cell_type":"markdown","metadata":{"id":"-MK9npotqkY6"},"source":["## __2.1 $\\quad$ Learning with scalar signals on a cyclic group__"]},{"cell_type":"markdown","metadata":{"id":"Gl2miszAx3ki"},"source":["In the simplest setting we consider, the underlying domain is a one-dimensional grid, and the signals only have a single channel. We can identify this grid $\\Omega$ with the Cayley graph of cyclic group\n","$$\n","C_n = \\langle \\, a : a^n = 1 \\, \\rangle \\equiv \\{ \\, 1, a, a^2, \\dots, a^{n-1} \\, \\}.\n","$$\n","It is convenient to parametrize the group, and hence the grid, through the exponent of the generator \n","$$\n","C_n \\equiv \\{ 0, 1, \\dots, n -1 \\}\n","$$\n","as this indexing is consistent with the way most programming languages index vectors, reinterpreting the group operation as addition modulo $n$."]},{"cell_type":"markdown","metadata":{"id":"y1QX0iPUyVZd"},"source":["The vector space of single-channeled (i.e. real-valued) signals\n","$$\n","\\mathcal{X}(C_n,\\mathbb{R}) = \\{ x : C_n \\to \\mathbb{R} \\} ,\n","$$\n","is finite dimensional, and each $x \\in \\mathcal{X}(C_n, \\mathbb{R})$ may be expressed as \n","$$\n","x = \n","\\left[ \n","\\begin{matrix}\n","x_0\\\\ \n","\\vdots\\\\\n","\\,x_{n-1}\\,\n","\\end{matrix}\n","\\right] \n","$$\n","with respect to some implicit coordinate system used by the computer, the _input coordinate system_. This is the same coordinate system used to express the representation $\\rho$ of translation group $G \\equiv C_n$, which we now describe."]},{"cell_type":"markdown","metadata":{"id":"H-bA8Ykqywt4"},"source":["Given a vector $\\theta = (\\theta_0 , \\dots, \\theta_{n-1})$, recall the associated _circulant matrix_ is the $n \\times n$ matrix with entries \n","$$\n","\\mathcal{C}(\\theta) := \\left( \\, \\theta_{ (u - v) \\mod n} \\right)_{ 0 \\, \\leq \\,u,\\,v \\, \\leq n-1 } \n","$$"]},{"cell_type":"markdown","metadata":{"id":"_Ilur6U0zVps"},"source":["Consider the case of $\\theta_S := (0,1,0,\\dots, 0)^T$, the associated circulant matrix, $\\mathbf{S} := \\mathbf{C}(\\theta_S)$ acts on vectors by shifting the entries of vectors to the right by one position, modulo $n$. This is a shift or translation operator, which we denote $\\mathbf{S}$. "]},{"cell_type":"markdown","metadata":{"id":"VACCLRh_zaU2"},"source":["___\n","__Lemma__ $\\quad$\n","A matrix is circulant if and only if it commutes with $\\mathbf{S}$. Moreover, given any two vectors $\\theta, \\eta \\in \\mathbb{R}^n$, one has $\\mathbf{C}(\\theta) \\mathbf{C}(\\eta) = \\mathbf{C}(\\eta) \\mathbf{C}(\\theta)$. \n","\n","___"]},{"cell_type":"markdown","metadata":{"id":"2BOxnG0Pz2YD"},"source":["The importance of $\\mathbf{S}$ to the present discussion is that it generates a group isomorphic to the one-dimensional translation group $C_n$. This is to say, a natural representation of $C_n = \\langle \\, a : a^n = 1 \\, \\rangle$ to consider is the group isomorphism induced by mapping the generator $a$ of $C_n$ to $\\mathbf{S}$. Specifically, the representation $\\rho$ of $G$ over $\\mathcal{X}( C_n, \\mathbb{R})$ is given by\n","$$\n","\\rho ( a^j ) := \\mathbf{S}^j \n","$$"]},{"cell_type":"markdown","metadata":{"id":"DTf8ASH40Qqb"},"source":["___\n","\n","__Corollary__ $\\quad$ Any $f : \\mathcal{X}(C_n, \\mathbb{R}) \\to \\mathcal{X}(C_n,\\mathbb{R})$ which is linear and $C_n$-equivariant can be expressed ( in the input coordinate system ) as an $n \\times n$ circulant matrix $\\mathbf{C}(\\theta)$ for some vector $\\theta$.\n","\n","___\n"]},{"cell_type":"markdown","metadata":{"id":"lu9LD2VC0nIT"},"source":["___\n","___\n","\n","__Example__ $\\quad$ \n","Our previous recipe for designing an equivariant function $F= \\Phi( \\mathbf{X}, \\mathbf{A})$ using a local aggregation function $\\varphi$. In this case, we can express\n","$$\n","\\varphi ( \\mathbf{x}_u, \\mathbf{X}_{\\mathcal{N}(u)} ) = \\varphi( \\mathbf{x}_{u-1}, \\, \\mathbf{x}_u, \\, \\mathbf{x}_{u+1} ),\n","$$\n","where the addition and subtraction in the indices above is understood to be modulo $n$. \n","\n"," If in addition, we insist that $\\varphi$ is linear, then it has the form \n","$$\n"," \\varphi( \\mathbf{x}_{u-1}, \\, \\mathbf{x}_u, \\, \\mathbf{x}_{u+1} ) = \\theta_{-1} \\mathbf{x}_{u-1} + \\theta_0 \\mathbf{x}_u + \\theta_1 \\mathbf{x}_{u+1},\n","$$\n","and in this case we can express $\\mathbf{F} = \\Phi (\\mathbf{X}, \\mathbf{A} )$ through the following matrix multiplication:\n","$$\n","\\left[\n","\\begin{matrix}\n","\\theta_0 & \\theta_1 & \\text{ } & \\text{ } & \\theta_{-1} \\\\\n","\\theta_{-1} & \\theta_0 & \\theta_1 & \\text{ } &   \\text{ } \\\\\n","\\text{} & \\ddots & \\ddots & \\ddots & \\text{ } \\\\\n","\\text{ } & \\text{ } & \\theta_{-1} & \\theta_0 & \\theta_1 \\\\\n","\\theta_1 & \\text{ } & \\text{ } & \\theta_{-1} & \\theta_0 \n","\\end{matrix} \n","\\right]\n","\\left[\n","\\begin{matrix}\n","\\mathbf{x}_0 \\\\\n","\\mathbf{x}_1 \\\\\n","\\vdots \\\\\n","\\,\\mathbf{x}_{n-2} \\, \\\\\n","\\mathbf{x}_{n-1}  \n","\\end{matrix}\n","\\right]\n","$$\n","This special multi-diagonal structure is sometimes referred to as ``weight sharing\" in the machine learning literature. \n","\n","___\n","___"]},{"cell_type":"markdown","metadata":{"id":"j0N6Rklm1Sn0"},"source":["Circulant matrices are synonymous with discrete convolutions; for $x \\in \\mathcal{X}(\\Omega,\\mathbb{R})$ and $\\theta \\in \\mathbb{R}^n$, their _convolution_ $x \\star \\theta$ is defined by \n","$$\n","( x \\star \\theta )_u := \\sum_{v = 0}^{n-1} x_{v \\mod n}\\, \\theta_{ (u-v) \\mod n}  \\, ,\n","$$\n","$$\n","\\equiv \\mathbf{C}(\\theta) x \n","$$\n"]},{"cell_type":"markdown","metadata":{"id":"Lmh-AZIM1jXB"},"source":["___\n","\n","__Rmk__ $\\quad$ This leads to an alternate, equivalent definition of convolution as a translation equivariant linear operation. Moreover by replacing translations by a more general group $G$, one can generalize convolution to settings whose domain has symmetry other than translational. \n","___ "]},{"cell_type":"markdown","metadata":{"id":"njwaNoPCN09d"},"source":["#### `torch.nn.Conv2d`\n","\n","The arguments:\n","\n","* `in_channels`\n","\n","* `out_channels`\n","\n","* `kernel_size`\n","\n","* `stride` $\\quad$ controls the stride for the cross-correlation, a single number or a tuple. \n","\n","* `padding` $\\quad$ controls amount of padding applied to the input. It can either be a string, `\"valid\"` or `\"same\"` or a tuple of ints giving the amount of implicit padding applied on both sides. \n","\n","* `dilation` $\\quad$ controls the spacing between kernel points; \"also known as the a trous algorithm\n","\n","* `groups` $\\quad$ controls connections between inputs and outputs. The `in_channels` and `out_channels` must be divisible by `groups`. For example,\n","\n","    * At groups = 1, all inputs are convolved to all outputs\n","\n","    * At groups = 2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated. \n","\n","    * At groups = `in_channels`, each input channel is convolved with its own set of filters (of size `out_channels // in_channels`)\n","\n","* `bias`\n","\n","* `padding_mode`,\n","\n","* `device`,\n","\n","* `dtype`"]},{"cell_type":"markdown","metadata":{"id":"8AIKdP8WPa9i"},"source":["Let us now relate the shapes of the input and output to the parameters\n","\n","\n","| input parameter      | LaTeX symbol |\n","| ----------- | ----------- |\n","| `in_channels`      | $\\text{dim}(\\mathcal{C})$     |\n","| `out_channels`   | $\\text{dim}(\\mathcal{C}_1)$        |\n","| `kernel_size`      | $k$       |\n","| `stride`   | $\\lambda$        |\n","| `padding`   | $\\rho$        |\n","| `dilation`      | $\\delta$       |\n","| `groups`   | $M$        \n","\n"]},{"cell_type":"markdown","metadata":{"id":"b7jNhkTvPgoH"},"source":["Additionally, we use $N$ for the batch size of the input, `N`. We also let $(h,w)$ denote the height-width pair describing the shape of the input signal domain. \n","\n","Correspondingly, we write $(h_1, w_1)$ for the height-width pair describing the shape of the output signal domain. "]},{"cell_type":"markdown","metadata":{"id":"hTqm3oVlPuGI"},"source":["We remark that the stride can be either integer or a $2$-tuple, whose coordinates describe the vertical and horizontal stride respectively. We still write $\\lambda$ for the stride when it is a tuple, and use $\\lambda_h \\equiv \\lambda[0]$ and $\\lambda_w \\equiv \\lambda[1]$ to denote its first and second coordinate, in this case. Likewise, the padding and kernel size may be $2$-tuples as well, and we use similar notation to denote their entries.  "]},{"cell_type":"markdown","metadata":{"id":"RNqduAQrPwZh"},"source":["The full shape of the input to the layer includes the batch dimension, and is thus\n","\n","$$\n","(N, \\text{dim}(\\mathcal{C}), H, W) \\,,\n","$$\n","\n","while the shape of the output is\n","\n","$$\n","(N , \\text{dim}(\\mathcal{C}_1), H_1, W_1 )\n","$$\n","\n","These shapes, in particular the spatial dimensions of each, are related as follows: \n","\n","$\\begin{align}\n","H_1 &= \\left\\lfloor \\frac{\n","    H + 2 \\rho_h - \\delta_h ( k_h -1) -1 }{\\lambda_h}\n","\\right\\rfloor \\\\\n","W_1 &= \\left\\lfloor \\frac{\n","    W + 2 \\rho_w - \\delta_w ( k_w -1) -1 }{\\lambda_w}\n","\\right\\rfloor\n","\\end{align}$,\n","\n","in particular, the batch size does not have any bearing on how the shapes of tensors transform. "]},{"cell_type":"markdown","metadata":{"id":"h4qqcExeP3OC"},"source":["The parameters to be learned are the weights $w^1$ and biases $b^1$. These are both `Tensor` objects, accessed from the layer as `Conv2d.weight` and `Conv2d.bias`. The shape of the weight tensor is\n","\n","$$\n","\\textrm{shape}(w^1) =\n","\\left( \\, \\text{dim}(\\mathcal{C}_1),  \\, \\text{dim}(\\mathcal{C}) \\big/ M , k_h, k_w \\right)\n","$$\n","\n","The tensor $w^1$ thus has \n","\n","$$\n","\\textrm{size}(w^1) = \\textrm{dim}(\\mathcal{C}_1) \\textrm{dim} (\\mathcal{C}) k_h k_w \\big/ M\n","$$\n","\n","scalar entries. \n","\n","There is always the question of how to initialize weights. In the case of the `Conv2d` class, the weights are initialized to be i.i.d. $\\text{Unif}( - \\sqrt{ \\alpha_1}, \\sqrt{\\alpha_1} )$ random variables, where\n","\n","$$\n","\\alpha_1 := \\frac{ \\textrm{dim}(\\mathcal{C}_1) }{\\textrm{size}(w^1)}\n","$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zQDEqpPSP-Cd"},"source":["The bias tensor is a much smaller object, we have \n","\n","$\n","\\begin{align}\n","\\textrm{shape}(b^1) = (\\, \\textrm{dim}(\\mathcal{C}_1  ) \\,) \\, , \\quad \\textrm{size}(b^1) = \\textrm{dim}(\\mathcal{C}_1)\n","\\end{align}\n","$"]},{"cell_type":"markdown","metadata":{"id":"N9IpVQKjQCAl"},"source":["Despite this, we use the same initialization (with mutual independence of all random variables in discussion) for the bias entries as we did for the weights. "]},{"cell_type":"markdown","metadata":{"id":"HRn89dm915m7"},"source":["## __2.2 $\\quad$ A simple CNN__"]},{"cell_type":"markdown","metadata":{"id":"_zxicBxC6VEw"},"source":["We consider possibly the simplest neural network that we can construct through the above blueprint. Suppose we have a binary classification problem, with the following hypothesis space. Let $\\textsf{H}_1$ denote the hypothesis space of functions $f : \\mathcal{X}( C_n, \\mathbb{R}) \\to \\{0,1\\}$ of the form \n","\n","$$\n","f = A \\circ P \\circ \\mathbf{a} \\circ B \\,,\n","$$\n","where the components of $f$ are "]},{"cell_type":"markdown","metadata":{"id":"g4DGJSDa6y2k"},"source":["where the components of $f$ are \n","\n","\n","* $B$  : $\\quad$ A $C_n$-equivariant function, to be learned. It is represented as a circulant matrix $\\mathbf{C}(\\theta)$, where $\\theta$ is a vector $\\theta \\equiv (\\theta_0, \\dots, \\theta_{n-1})$ whose entries $\\theta_j$ are parameters to be learned. \n","\n","* $ \\mathbf{a} $ : $\\quad$ We consider the ReLU activation function, $a : \\mathbb{R} \\to \\mathbb{R}_{\\geq\\, 0}$ defined by $a(w) = \\max(0,w)$, for $w \\in \\mathbb{R}$. The bold-face $\\mathbf{a}$ denotes the entry-wise action of this function on a given vector;for $y \\equiv (\\,y_1, \\,\\dots, \\, y_n \\, ) \\in \\mathcal{X}(C_n, \\mathbb{R})$, which we imagine as the output of $B(x)$ for some input signal $x$, we have $\\mathbf{a} (y ) = ( \\,  \\max(0,y_1), \\,  \\dots, \\, \\max(0,y_n) )$. There are no learned parameters in this layer. \n","\n","* $P$ : $\\quad$ A coarsening operator. In this case, let us say it is a _zero-padded group homomorphism_. \n","\n"," $P : C_n \\to C_{n / d }$ for some divisor $d \\mid n$ \\footnote{zero-padding} , and let us say that it operates through max-pooling on the signal, over the pre-images of each element of $C_{n / d}$. \n","\n","* $A$ : $\\quad$ A global-pooling layer. We assume this has the form of a fully-connected layer, followed by a softmax. Specifically,"]},{"cell_type":"code","metadata":{"id":"HjLRoZui8f2Y"},"source":[""],"execution_count":null,"outputs":[]}]}