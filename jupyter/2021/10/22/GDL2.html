<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>2 | geometric deep learning</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="2" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="GDL" />
<meta property="og:description" content="GDL" />
<link rel="canonical" href="https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2021/10/22/GDL2.html" />
<meta property="og:url" content="https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2021/10/22/GDL2.html" />
<meta property="og:site_name" content="geometric deep learning" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-10-22T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"GDL","url":"https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2021/10/22/GDL2.html","@type":"BlogPosting","headline":"2","dateModified":"2021-10-22T00:00:00-05:00","datePublished":"2021-10-22T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2021/10/22/GDL2.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/geometric-deep-learning/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://the-ninth-wave.github.io/geometric-deep-learning/feed.xml" title="geometric deep learning" /><link rel="shortcut icon" type="image/x-icon" href="/geometric-deep-learning/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/geometric-deep-learning/">geometric deep learning</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/geometric-deep-learning/about/">About Me</a><a class="page-link" href="/geometric-deep-learning/search/">Search</a><a class="page-link" href="/geometric-deep-learning/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">2</h1><p class="page-description">GDL</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-10-22T00:00:00-05:00" itemprop="datePublished">
        Oct 22, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      25 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/geometric-deep-learning/categories/#jupyter">jupyter</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/the-ninth-wave/geometric-deep-learning/tree/master/_notebooks/2021-10-22-GDL2.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/geometric-deep-learning/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/the-ninth-wave/geometric-deep-learning/master?filepath=_notebooks%2F2021-10-22-GDL2.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/geometric-deep-learning/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/the-ninth-wave/geometric-deep-learning/blob/master/_notebooks/2021-10-22-GDL2.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/geometric-deep-learning/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#geometric-deep-learning-models">geometric deep learning models </a>
<ul>
<li class="toc-entry toc-h2"><a href="#2.1-...-tensors-in-pytorch">2.1 ... tensors in pytorch </a>
<ul>
<li class="toc-entry toc-h3"><a href="#...-helper-classes-and-functions-">... helper classes and functions  </a>
<ul>
<li class="toc-entry toc-h4"><a href="#class-...-more_tens">class ... more_tens </a></li>
<li class="toc-entry toc-h4"><a href="#method-...-info">method ... info </a></li>
<li class="toc-entry toc-h4"><a href="#method-...-viz_tens">method ... viz_tens </a></li>
<li class="toc-entry toc-h4"><a href="#method-...-viz_tens_list">method ... viz_tens_list </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#...-initializing-tensors">... initializing tensors </a>
<ul>
<li class="toc-entry toc-h4"><a href="#...-1.---from-lists">... 1.   from lists </a></li>
<li class="toc-entry toc-h4"><a href="#...-2.-from-numpy">... 2. from numpy </a></li>
<li class="toc-entry toc-h4"><a href="#...-3.-(a)-ones-of-given-shape">... 3. (a) ones of given shape </a></li>
<li class="toc-entry toc-h4"><a href="#...-3.-(b)-zeros-of-given-shape">... 3. (b) zeros of given shape </a></li>
<li class="toc-entry toc-h4"><a href="#...-3.-(c)-i.i.d.-uniform-of-given-shape">... 3. (c) i.i.d. uniform of given shape </a></li>
<li class="toc-entry toc-h4"><a href="#...-3.-(d)-i.i.d.-standard-normal-of-given-shape">... 3. (d) i.i.d. standard normal of given shape </a></li>
<li class="toc-entry toc-h4"><a href="#...-3.-(f)-sequence">... 3. (f) sequence </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#numpy-objects-and-tensors">numpy objects and tensors </a></li>
<li class="toc-entry toc-h3"><a href="#...-tensor-operations">... tensor operations </a>
<ul>
<li class="toc-entry toc-h4"><a href="#...-adding-tensors">... adding tensors </a></li>
<li class="toc-entry toc-h4"><a href="#...-stacking-tensors">... stacking tensors </a></li>
<li class="toc-entry toc-h4"><a href="#...-in-place-operations">... in-place operations </a></li>
<li class="toc-entry toc-h4"><a href="#...-reshaping-tensors">... reshaping tensors </a></li>
<li class="toc-entry toc-h4"><a href="#...-transposing-tensors-(permuting-dimensions)">... transposing tensors (permuting dimensions) </a></li>
<li class="toc-entry toc-h4"><a href="#...-numpy-like-indexing-and-slicing">... numpy-like indexing and slicing </a></li>
<li class="toc-entry toc-h4"><a href="#...-other-operations">... other operations </a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#2.2-...-Learning-with-scalar-signals-on-a-cyclic-group">2.2 ... Learning with scalar signals on a cyclic group </a>
<ul>
<li class="toc-entry toc-h4"><a href="#torch.nn.Conv2d">torch.nn.Conv2d </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#2.2-$\quad$-A-simple-CNN">2.2 $\quad$ A simple CNN </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-10-22-GDL2.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="geometric-deep-learning-models">
<a class="anchor" href="#geometric-deep-learning-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>geometric deep learning models<a class="anchor-link" href="#geometric-deep-learning-models"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="2.1-...-tensors-in-pytorch">
<a class="anchor" href="#2.1-...-tensors-in-pytorch" aria-hidden="true"><span class="octicon octicon-link"></span></a><font color="green">2.1 ... tensors in pytorch</font><a class="anchor-link" href="#2.1-...-tensors-in-pytorch"> </a>
</h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">torch</span> <span class="o">--</span><span class="n">upgrade</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu111)
Collecting torch
  Downloading torch-1.10.0-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)
     |██████████████████████████████▎ | 834.1 MB 1.3 MB/s eta 0:00:38tcmalloc: large alloc 1147494400 bytes == 0x5560b83a0000 @  0x7fe0f2a0d615 0x55607f4604cc 0x55607f54047a 0x55607f4632ed 0x55607f554e1d 0x55607f4d6e99 0x55607f4d19ee 0x55607f464bda 0x55607f4d6d00 0x55607f4d19ee 0x55607f464bda 0x55607f4d3737 0x55607f555c66 0x55607f4d2daf 0x55607f555c66 0x55607f4d2daf 0x55607f555c66 0x55607f4d2daf 0x55607f465039 0x55607f4a8409 0x55607f463c52 0x55607f4d6c25 0x55607f4d19ee 0x55607f464bda 0x55607f4d3737 0x55607f4d19ee 0x55607f464bda 0x55607f4d2915 0x55607f464afa 0x55607f4d2c0d 0x55607f4d19ee
     |████████████████████████████████| 881.9 MB 18 kB/s 
Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)
Installing collected packages: torch
  Attempting uninstall: torch
    Found existing installation: torch 1.9.0+cu111
    Uninstalling torch-1.9.0+cu111:
      Successfully uninstalled torch-1.9.0+cu111
<span class="ansi-red-fg">ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
torchvision 0.10.0+cu111 requires torch==1.9.0, but you have torch 1.10.0 which is incompatible.
torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.10.0 which is incompatible.</span>
Successfully installed torch-1.10.0
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.utils.data</span> <span class="k">as</span> <span class="nn">data</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Using torch"</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Using torch 1.10.0+cu102
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="...-helper-classes-and-functions-">
<a class="anchor" href="#...-helper-classes-and-functions-" aria-hidden="true"><span class="octicon octicon-link"></span></a><font color="teal">... helper classes and functions </font><a class="anchor-link" href="#...-helper-classes-and-functions-"> </a>
</h3>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h4 id="class-...-more_tens">
<a class="anchor" href="#class-...-more_tens" aria-hidden="true"><span class="octicon octicon-link"></span></a>class ... <code>more_tens</code><a class="anchor-link" href="#class-...-more_tens"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">more_tens</span><span class="p">():</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> 
                 <span class="n">obj</span><span class="p">,</span> 
                 <span class="n">name</span><span class="p">,</span> 
                 <span class="n">has_RGB_shape</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> 
                 <span class="n">has_batch_dim</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">obj</span> <span class="o">=</span> <span class="n">obj</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ty</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span> <span class="bp">self</span><span class="o">.</span><span class="n">obj</span> <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">has_RGB_shape</span> <span class="o">=</span> <span class="n">has_RGB_shape</span> 

        <span class="bp">self</span><span class="o">.</span><span class="n">has_batch_dim</span> <span class="o">=</span> <span class="n">has_batch_dim</span>

    <span class="k">def</span> <span class="nf">display_rgb</span><span class="p">(</span> <span class="bp">self</span><span class="p">,</span> <span class="n">pil_image</span><span class="p">,</span> <span class="n">size</span> <span class="p">):</span>

        <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span> <span class="p">)</span>

        <span class="n">f_rows</span><span class="p">,</span> <span class="n">f_cols</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>

        <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span> <span class="n">f_rows</span><span class="p">,</span> <span class="n">f_cols</span><span class="p">,</span> <span class="mi">1</span> <span class="p">)</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span> <span class="n">left</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">bottom</span> <span class="o">=</span> <span class="kc">False</span> <span class="p">)</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">pil_image</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">display_tens</span><span class="p">(</span> <span class="bp">self</span><span class="p">,</span> <span class="n">tens</span><span class="p">,</span> <span class="n">size</span> <span class="p">):</span>

        <span class="n">display_size</span> <span class="o">=</span> <span class="n">size</span>

        <span class="k">if</span> <span class="n">tens</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>

            <span class="n">display_obj</span> <span class="o">=</span> <span class="n">tens</span><span class="p">[</span><span class="kc">None</span><span class="p">,:]</span>

            <span class="n">display_obj</span> <span class="o">=</span> <span class="n">display_obj</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

            <span class="n">viz_tens</span><span class="p">(</span><span class="n">display_obj</span><span class="p">,</span> <span class="n">display_size</span><span class="p">)</span>

        <span class="k">elif</span> <span class="n">tens</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>

            <span class="n">display_obj</span> <span class="o">=</span> <span class="n">tens</span>

            <span class="n">display_obj</span> <span class="o">=</span> <span class="n">display_obj</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

            <span class="n">viz_tens</span><span class="p">(</span><span class="n">display_obj</span><span class="p">,</span> <span class="n">display_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">display_np</span><span class="p">(</span> <span class="bp">self</span><span class="p">,</span> <span class="n">arr</span><span class="p">,</span> <span class="n">size</span> <span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">display_tens</span><span class="p">(</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span> <span class="n">arr</span> <span class="p">),</span> <span class="n">size</span> <span class="p">)</span>

    

    <span class="k">def</span> <span class="nf">scan_img</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image</span><span class="p">):</span>

        <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">image</span><span class="p">),</span> 
              <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">,</span> 
              <span class="s2">"format:  "</span><span class="p">,</span> <span class="n">image</span><span class="o">.</span><span class="n">format</span><span class="p">,</span> 
              <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">,</span> <span class="s2">"shape:  "</span><span class="p">,</span> <span class="n">image</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> 
              <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">,</span> <span class="s2">"mode:  "</span><span class="p">,</span> <span class="n">image</span><span class="o">.</span><span class="n">mode</span><span class="p">)</span>
        

    <span class="k">def</span> <span class="nf">set_GOE</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
        <span class="n">normalize</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">N</span><span class="p">)</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">N</span><span class="p">)</span>
        <span class="n">gaussian_matrix</span> <span class="o">=</span> <span class="n">normalize</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">return</span><span class="p">(</span> <span class="n">gaussian_matrix</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">gaussian_matrix</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> 
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>pip install tabletext
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Collecting tabletext
  Downloading tabletext-0.1.tar.gz (6.1 kB)
Building wheels for collected packages: tabletext
  Building wheel for tabletext (setup.py) ... done
  Created wheel for tabletext: filename=tabletext-0.1-py3-none-any.whl size=6022 sha256=384237e663e1e0614b9290fcd435e0e01872c35657dbc01661714520ea433992
  Stored in directory: /root/.cache/pip/wheels/cc/ae/ab/697f6cd9887c63663da889f796c2c7ea280bc407b16f6fd081
Successfully built tabletext
Installing collected packages: tabletext
Successfully installed tabletext-0.1
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tabletext</span> <span class="kn">import</span> <span class="n">to_text</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h4 id="method-...-info">
<a class="anchor" href="#method-...-info" aria-hidden="true"><span class="octicon octicon-link"></span></a>method ... <code>info</code><a class="anchor-link" href="#method-...-info"> </a>
</h4>
<p><em>can currently handle 1- and 2-tensors</em></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">info</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>

    <span class="n">ty_tens</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">])</span> <span class="p">)</span>

    <span class="n">ty_np</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">])</span> <span class="p">)</span>

    <span class="n">delim</span> <span class="o">=</span> <span class="s2">"   "</span>

    <span class="k">if</span> <span class="n">obj</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>

        <span class="n">display_obj</span> <span class="o">=</span> <span class="n">obj</span><span class="p">[</span><span class="kc">None</span><span class="p">,:]</span>

    <span class="k">elif</span> <span class="n">obj</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>

        <span class="n">display_obj</span> <span class="o">=</span> <span class="n">obj</span>

    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span> <span class="o">==</span> <span class="n">ty_tens</span><span class="p">:</span>

        <span class="nb">print</span><span class="p">(</span> <span class="s2">"tensor"</span><span class="p">,</span> <span class="n">delim</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span> <span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span> <span class="n">delim</span><span class="p">,</span> <span class="s2">"num. dims   "</span><span class="p">,</span> <span class="n">delim</span><span class="p">,</span> <span class="n">obj</span><span class="o">.</span><span class="n">ndim</span> <span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span> <span class="n">delim</span><span class="p">,</span> <span class="s2">"num. entries"</span><span class="p">,</span> <span class="n">delim</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span> <span class="n">obj</span> <span class="p">)</span><span class="o">.</span><span class="n">size</span> <span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span> <span class="n">delim</span><span class="p">,</span> <span class="s2">"shape       "</span><span class="p">,</span> <span class="n">delim</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span> <span class="n">obj</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="p">)</span> <span class="p">)</span>

    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span> <span class="o">==</span> <span class="n">ty_np</span><span class="p">:</span>

        <span class="nb">print</span><span class="p">(</span> <span class="s2">"np array"</span><span class="p">,</span> <span class="n">delim</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span> <span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span> <span class="n">delim</span><span class="p">,</span> <span class="s2">"number of dimensions"</span><span class="p">,</span> <span class="n">obj</span><span class="o">.</span><span class="n">ndim</span> <span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span> <span class="n">delim</span><span class="p">,</span> <span class="s2">"number of entries"</span><span class="p">,</span> <span class="n">delim</span><span class="p">,</span> <span class="n">obj</span><span class="o">.</span><span class="n">size</span> <span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span> <span class="n">delim</span><span class="p">,</span> <span class="s2">"shape"</span><span class="p">,</span> <span class="n">delim</span><span class="p">,</span> <span class="n">obj</span><span class="o">.</span><span class="n">shape</span> <span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span> <span class="n">to_text</span><span class="p">(</span> <span class="n">display_obj</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="p">)</span> <span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n\n</span><span class="s2">"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h4 id="method-...-viz_tens">
<a class="anchor" href="#method-...-viz_tens" aria-hidden="true"><span class="octicon octicon-link"></span></a>method ... <code>viz_tens</code><a class="anchor-link" href="#method-...-viz_tens"> </a>
</h4>
<p>args</p>
<ul>
<li>
<p><code>tens</code></p>
</li>
<li>
<p><code>display_size</code></p>
</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">viz_tens</span><span class="p">(</span> <span class="n">tens</span><span class="p">,</span> <span class="n">display_size</span> <span class="o">=</span> <span class="mi">2</span> <span class="p">):</span>

    <span class="n">size</span> <span class="o">=</span> <span class="n">display_size</span>

    <span class="k">if</span> <span class="n">tens</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>

        <span class="n">display_obj</span> <span class="o">=</span> <span class="n">tens</span><span class="p">[</span><span class="kc">None</span><span class="p">,:]</span>

        <span class="n">display_obj</span> <span class="o">=</span> <span class="n">display_obj</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

    <span class="k">elif</span> <span class="n">tens</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>

        <span class="n">display_obj</span> <span class="o">=</span> <span class="n">tens</span>

        <span class="n">display_obj</span> <span class="o">=</span> <span class="n">display_obj</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

    <span class="n">pil_image</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">ToPILImage</span><span class="p">()(</span> <span class="n">display_obj</span> <span class="p">)</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">"RGB"</span><span class="p">)</span>

    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span> <span class="p">)</span>

    <span class="n">f_rows</span><span class="p">,</span> <span class="n">f_cols</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>

    <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span> <span class="n">f_rows</span><span class="p">,</span> <span class="n">f_cols</span><span class="p">,</span> <span class="mi">1</span> <span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span> <span class="n">left</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">bottom</span> <span class="o">=</span> <span class="kc">False</span> <span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">pil_image</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h4 id="method-...-viz_tens_list">
<a class="anchor" href="#method-...-viz_tens_list" aria-hidden="true"><span class="octicon octicon-link"></span></a>method ... <code>viz_tens_list</code><a class="anchor-link" href="#method-...-viz_tens_list"> </a>
</h4>
<p>args</p>
<ul>
<li>
<p><code>list_of_tensors</code></p>
</li>
<li>
<p><code>display_size = 6</code></p>
</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">viz_tens_list</span><span class="p">(</span> <span class="n">list_of_tensors</span><span class="p">,</span> <span class="n">display_size</span> <span class="o">=</span> <span class="mi">6</span> <span class="p">):</span>

    <span class="n">size</span> <span class="o">=</span> <span class="n">display_size</span>

    <span class="n">image_list</span> <span class="o">=</span> <span class="p">[</span> <span class="n">transforms</span><span class="o">.</span><span class="n">ToPILImage</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">"RGB"</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">list_of_tensors</span> <span class="p">]</span>

    <span class="n">tensor_list</span> <span class="o">=</span> <span class="p">[</span> <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()(</span><span class="n">image</span><span class="p">)</span> <span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">image_list</span><span class="p">]</span>

    <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span> <span class="n">tensor_list</span><span class="p">,</span> <span class="n">padding</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">pad_value</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="p">)</span>

    <span class="n">grid_pil</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">ToPILImage</span><span class="p">()(</span><span class="n">grid</span><span class="p">)</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">"RGB"</span><span class="p">)</span>

    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span> <span class="p">)</span>

    <span class="n">f_rows</span><span class="p">,</span> <span class="n">f_cols</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>

    <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span> <span class="n">f_rows</span><span class="p">,</span> <span class="n">f_cols</span><span class="p">,</span> <span class="mi">1</span> <span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span> <span class="n">left</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">bottom</span> <span class="o">=</span> <span class="kc">False</span> <span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">grid_pil</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="...-initializing-tensors">
<a class="anchor" href="#...-initializing-tensors" aria-hidden="true"><span class="octicon octicon-link"></span></a><font color="teal">... initializing tensors</font><a class="anchor-link" href="#...-initializing-tensors"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Tensors are similar to NumPy's <code>ndarrays</code>, except that tensors can run on GPUs or other hardware accelerators. Tensors and NumPy arrays can often share the same underlying memory, eliminating the need to copy data. Tensors are also optimized for automatic differentiation</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here are some ways they can be initialized in torch.</p>
<ol>
<li>
<p>from lists: $\quad$ <code>X_1 = torch.tensor( list_object )</code></p>
</li>
<li>
<p>from a numpy array $\quad$ <code>X_2 = torch.from_numpy( array_object )</code></p>
</li>
<li>
<p>with random or constant values, of a given shape. For example,</p>
<p>a. entries all ones: $\quad$ <code>X_3_a = torch.ones( shape )</code></p>
<p>b. entries all zeros $\quad$ <code>X_3_b = torch.zeros( shape )</code></p>
<p>c. entries are i.i.d. $\text{Unif}(0,1)$ $\quad$ <code>X_3_c = torch.rand( shape )</code></p>
<p>d. entries are i.i.d. standard normal $\quad$ <code>X_3_d = torch.randn( shape )</code></p>
<p>e. from values stored in memory $\quad$ <code>X_3_e = torch.Tensor( shape )</code></p>
<p>f. a list of consecutive integers between $N$ and $M$, inclusive, as tensor object $\quad$ <code>X_3_f = torch.arange(N,M)</code></p>
</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-1.---from-lists">
<a class="anchor" href="#...-1.---from-lists" aria-hidden="true"><span class="octicon octicon-link"></span></a>... 1.   from lists<a class="anchor-link" href="#...-1.---from-lists"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span> <span class="p">[</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span> <span class="p">]</span> <span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">X_1</span><span class="p">,</span> <span class="s2">"X1"</span><span class="p">)</span>
<span class="n">viz_tens</span><span class="p">(</span> <span class="mi">25</span> <span class="o">*</span> <span class="n">X_1</span> <span class="p">)</span> <span class="c1"># the factor there to help distinguish values</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     X1 

    num. dims        1
    num. entries     2
    shape            [2]


┌───┬───┐
│ 2 │ 3 │
└───┴───┘



</pre>
</div>
</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAABGCAYAAAAKCiBIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAA2klEQVR4nO3dsQ0CIBBAUTFOx/KMwRg4gXaGxP9ee80lP1dQMc45D3qetxfgDuGjhI8SPkr4KOGjXt+Ga62/fuvtvW+v8FNzzvFp5uKjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPmr4RrzJxUcJHyV8lPBRwkcJH/UGmigNh5tk3yoAAAAASUVORK5CYII=%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-2.-from-numpy">
<a class="anchor" href="#...-2.-from-numpy" aria-hidden="true"><span class="octicon octicon-link"></span></a>... 2. from numpy<a class="anchor-link" href="#...-2.-from-numpy"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span> <span class="p">[</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span> <span class="p">]</span> <span class="p">)</span> <span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">X_2</span><span class="p">,</span> <span class="s2">"'X two'"</span><span class="p">)</span>
<span class="n">viz_tens</span><span class="p">(</span> <span class="mi">25</span> <span class="o">*</span> <span class="n">X_2</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     'X two' 

    num. dims        1
    num. entries     3
    shape            [3]


┌───┬───┬───┐
│ 2 │ 3 │ 4 │
└───┴───┴───┘



</pre>
</div>
</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAAAzCAYAAABR5bw6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAsklEQVR4nO3csQ0DIRAAQWO5UwqmDMrgG7Cd/ks7k15y0uoCEsY550XP++4FuIfwUcJHCR8lfJTwUZ9/w7XW4956e++7V/jqiXvNOcevmYuPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4a/bJtcfJTwUcJHCR8lfJTwURc1fBBhoRLH+gAAAABJRU5ErkJggg==%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The next examples allow a shape to be provided as an argument.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">shape_dict</span> <span class="o">=</span> <span class="p">{}</span>

<span class="n">shape_dict</span><span class="p">[</span><span class="s2">"i"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span> <span class="mi">3</span> <span class="p">)</span>

<span class="n">shape_dict</span><span class="p">[</span><span class="s2">"ii"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span> <span class="p">)</span>

<span class="n">shape_dict</span><span class="p">[</span><span class="s2">"iii"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-3.-(a)-ones-of-given-shape">
<a class="anchor" href="#...-3.-(a)-ones-of-given-shape" aria-hidden="true"><span class="octicon octicon-link"></span></a>... 3. (a) ones of given shape<a class="anchor-link" href="#...-3.-(a)-ones-of-given-shape"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">shape_dict</span><span class="p">:</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">key</span>
    <span class="n">X_3_a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape_dict</span><span class="p">[</span><span class="n">s</span><span class="p">])</span>
    <span class="n">info</span><span class="p">(</span><span class="n">X_3_a</span><span class="p">,</span> <span class="s2">"'X three (a)'"</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s2">" "</span> <span class="o">+</span> <span class="n">s</span> <span class="o">+</span> <span class="s2">"'"</span> <span class="p">)</span>
    <span class="n">viz_tens</span><span class="p">(</span> <span class="mi">25</span> <span class="o">*</span> <span class="n">X_3_a</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     'X three (a) i' 

    num. dims        1
    num. entries     3
    shape            [3]


┌─────┬─────┬─────┐
│ 1.0 │ 1.0 │ 1.0 │
└─────┴─────┴─────┘



tensor     'X three (a) ii' 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌─────┬─────┬─────┐
│ 1.0 │ 1.0 │ 1.0 │
├─────┼─────┼─────┤
│ 1.0 │ 1.0 │ 1.0 │
└─────┴─────┴─────┘



tensor     'X three (a) iii' 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌─────┬─────┬─────┐
│ 1.0 │ 1.0 │ 1.0 │
├─────┼─────┼─────┤
│ 1.0 │ 1.0 │ 1.0 │
└─────┴─────┴─────┘



</pre>
</div>
</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAAAzCAYAAABR5bw6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAqElEQVR4nO3csQ2AMAwAQYIYIvtPly3CBNAi9Hetm0gvF24y9t4HPefXD+AbwkcJHyV8lPBRwkddb8O1llvvx+ac42lm46OEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svio4S/bJhsfJXyU8FHCRwkfJXzUDUneCmEkcikWAAAAAElFTkSuQmCC%0A">
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAA/UlEQVR4nO3WwQkDMQwAwTikCPdfnbvQVZB8TdiZrz6CRaA1My963rcX4A7ho4SPEj5K+KjPr+E5x8v/x/be69vMxUcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJH7Vm5vYOXODio4SPEj5K+Cjho4SPegDe4Qqr0aiDOwAAAABJRU5ErkJggg==%0A">
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAA/UlEQVR4nO3WwQkDMQwAwTikCPdfnbvQVZB8TdiZrz6CRaA1My963rcX4A7ho4SPEj5K+KjPr+E5x8v/x/be69vMxUcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJH7Vm5vYOXODio4SPEj5K+Cjho4SPegDe4Qqr0aiDOwAAAABJRU5ErkJggg==%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-3.-(b)-zeros-of-given-shape">
<a class="anchor" href="#...-3.-(b)-zeros-of-given-shape" aria-hidden="true"><span class="octicon octicon-link"></span></a>... 3. (b) zeros of given shape<a class="anchor-link" href="#...-3.-(b)-zeros-of-given-shape"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">shape_dict</span><span class="p">:</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">key</span>
    <span class="n">X_3_b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape_dict</span><span class="p">[</span><span class="n">s</span><span class="p">])</span>
    <span class="n">info</span><span class="p">(</span><span class="n">X_3_b</span><span class="p">,</span> <span class="s2">"'X three (a)'"</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s2">" "</span> <span class="o">+</span> <span class="n">s</span> <span class="o">+</span> <span class="s2">"'"</span> <span class="p">)</span>
    <span class="n">viz_tens</span><span class="p">(</span> <span class="mi">25</span> <span class="o">*</span> <span class="n">X_3_b</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     'X three (a) i' 

    num. dims        1
    num. entries     3
    shape            [3]


┌─────┬─────┬─────┐
│ 0.0 │ 0.0 │ 0.0 │
└─────┴─────┴─────┘



tensor     'X three (a) ii' 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌─────┬─────┬─────┐
│ 0.0 │ 0.0 │ 0.0 │
├─────┼─────┼─────┤
│ 0.0 │ 0.0 │ 0.0 │
└─────┴─────┴─────┘



tensor     'X three (a) iii' 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌─────┬─────┬─────┐
│ 0.0 │ 0.0 │ 0.0 │
├─────┼─────┼─────┤
│ 0.0 │ 0.0 │ 0.0 │
└─────┴─────┴─────┘



</pre>
</div>
</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAAAzCAYAAABR5bw6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAo0lEQVR4nO3cMQqAQAwAQSP+/8vxBdqK7Eyb5mBJkeZmdw96zq8fwDeEjxI+Svgo4aOEj7rehjPj1vux3Z2nmY2PEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho8Zftk02Pkr4KOGjhI8SPkr4qBuM0wphyOWPswAAAABJRU5ErkJggg==%0A">
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAA+ElEQVR4nO3WsQnEQAwAwdfj/luWK7DTw+xMqkSwCDS7+6Pnf3oBzhA+Svgo4aOEj7rehjPj5f+w3Z2nmYuPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5qdvf0Dhzg4qOEjxI+Svgo4aOEj7oBIeUKq3KMmaAAAAAASUVORK5CYII=%0A">
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAA+ElEQVR4nO3WsQnEQAwAwdfj/luWK7DTw+xMqkSwCDS7+6Pnf3oBzhA+Svgo4aOEj7rehjPj5f+w3Z2nmYuPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5qdvf0Dhzg4qOEjxI+Svgo4aOEj7oBIeUKq3KMmaAAAAAASUVORK5CYII=%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-3.-(c)-i.i.d.-uniform-of-given-shape">
<a class="anchor" href="#...-3.-(c)-i.i.d.-uniform-of-given-shape" aria-hidden="true"><span class="octicon octicon-link"></span></a>... 3. (c) i.i.d. uniform of given shape<a class="anchor-link" href="#...-3.-(c)-i.i.d.-uniform-of-given-shape"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">shape_dict</span><span class="p">:</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">key</span>
    <span class="n">X_3_c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">shape_dict</span><span class="p">[</span><span class="n">s</span><span class="p">])</span>
    <span class="n">info</span><span class="p">(</span><span class="n">X_3_c</span><span class="p">,</span> <span class="s2">"'X three (c)'"</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s2">" "</span> <span class="o">+</span> <span class="n">s</span> <span class="o">+</span> <span class="s2">"'"</span> <span class="p">)</span>
    <span class="n">viz_tens</span><span class="p">(</span> <span class="mi">25</span> <span class="o">*</span> <span class="n">X_3_c</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     'X three (c) i' 

    num. dims        1
    num. entries     3
    shape            [3]


┌─────────────────────┬─────────────────────┬─────────────────────┐
│ 0.20829910039901733 │ 0.32885128259658813 │ 0.10535955429077148 │
└─────────────────────┴─────────────────────┴─────────────────────┘



tensor     'X three (c) ii' 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌────────────────────┬─────────────────────┬────────────────────┐
│ 0.9192349314689636 │   0.400767982006073 │ 0.9301983714103699 │
├────────────────────┼─────────────────────┼────────────────────┤
│ 0.6557910442352295 │ 0.07660150527954102 │  0.846017599105835 │
└────────────────────┴─────────────────────┴────────────────────┘



tensor     'X three (c) iii' 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌───────────────────────┬────────────────────┬─────────────────────┐
│   0.36242759227752686 │ 0.3083369731903076 │ 0.08496475219726562 │
├───────────────────────┼────────────────────┼─────────────────────┤
│ 0.0029196739196777344 │ 0.6430553197860718 │  0.3907780647277832 │
└───────────────────────┴────────────────────┴─────────────────────┘



</pre>
</div>
</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAAAzCAYAAABR5bw6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAtklEQVR4nO3cuw0DIRBAQWO5IEqig+s/4grwJ8XSm0lJVjxtQMLYez/oeZ4egDOEjxI+Svgo4aOEj3r9Opxz/t1bb4xxeoSPrus6PcKbtdbXy7LxUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfNTwl22TjY8SPkr4KOGjhI8SPuoGR7kKYfGVs1YAAAAASUVORK5CYII=%0A">
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABKUlEQVR4nO3cwWmEUBhG0RimBuuwNIuxNUsQy3gpIJMhs8kL3HO2b/PB5QdXLmOMD3o+Zw9gDuGjhI8SPkr4qMerx+u6/t0n/7qusyc8tW3b7AnfnOe5/PTm4qOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Sviolz8/2vf9r3b82nEcsyc8dd/37AlvcfFRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRyxhj9gYmcPFRwkcJHyV8lPBRwkd9AfMzE272ZFcdAAAAAElFTkSuQmCC%0A">
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABKUlEQVR4nO3csWnEQBRFUcssKFF9qkNdqQXVo0bGBXi9eBOP4Z6TTvLg8kGRljHGBz2fswcwh/BRwkcJHyV81OPV47qu/+6T/zzP2ROeOo5j9oRv7vtefnpz8VHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXzUy58fbdv2Vzt+bd/32ROeuq5r9oS3uPgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+SvioZYwxewMTuPgo4aOEjxI+Svgo4aO+ALVwEPvUyuneAAAAAElFTkSuQmCC%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-3.-(d)-i.i.d.-standard-normal-of-given-shape">
<a class="anchor" href="#...-3.-(d)-i.i.d.-standard-normal-of-given-shape" aria-hidden="true"><span class="octicon octicon-link"></span></a>... 3. (d) i.i.d. standard normal of given shape<a class="anchor-link" href="#...-3.-(d)-i.i.d.-standard-normal-of-given-shape"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">shape_dict</span><span class="p">:</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">key</span>
    <span class="n">X_3_d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape_dict</span><span class="p">[</span><span class="n">s</span><span class="p">])</span>
    <span class="n">info</span><span class="p">(</span><span class="n">X_3_d</span><span class="p">,</span> <span class="s2">"'X three (d)'"</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s2">" "</span> <span class="o">+</span> <span class="n">s</span> <span class="o">+</span> <span class="s2">"'"</span> <span class="p">)</span>
    <span class="n">viz_tens</span><span class="p">(</span> <span class="mi">25</span> <span class="o">*</span> <span class="n">X_3_d</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     'X three (d) i' 

    num. dims        1
    num. entries     3
    shape            [3]


┌────────────────────┬────────────────────┬────────────────────┐
│ 1.3849544525146484 │ -2.475163221359253 │ -0.931602954864502 │
└────────────────────┴────────────────────┴────────────────────┘



tensor     'X three (d) ii' 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌──────────────────────┬─────────────────────┬──────────────────────┐
│ -0.13348686695098877 │ 0.34148848056793213 │ -0.07157137989997864 │
├──────────────────────┼─────────────────────┼──────────────────────┤
│ -0.09089037775993347 │ -1.3296922445297241 │   -0.542582631111145 │
└──────────────────────┴─────────────────────┴──────────────────────┘



tensor     'X three (d) iii' 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌─────────────────────┬──────────────────────┬─────────────────────┐
│  0.5470984578132629 │   0.6430637240409851 │ -0.7904810905456543 │
├─────────────────────┼──────────────────────┼─────────────────────┤
│ -0.9058369994163513 │ -0.26072561740875244 │ -0.5465103983879089 │
└─────────────────────┴──────────────────────┴─────────────────────┘



</pre>
</div>
</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAAAzCAYAAABR5bw6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAuElEQVR4nO3cuQ3DMBAAQdNwQyxNDahWdUIX4CelgJ1JLzlgcQETjrXWg57n7gXYQ/go4aOEjxI+Svio17/heZ63e+td17V7ha+O49i9woc55/g1c/FRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV81PCXbZOLjxI+Svgo4aOEjxI+6g3N2g1hWjRdJgAAAABJRU5ErkJggg==%0A">
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABJ0lEQVR4nO3csW2EQBRFUWNtTpP0R0oH5BTEuACvV97EY+mek07ypKsvEbGMMT7o+Zw9gDmEjxI+Svgo4aMerx73ff93n/zXdc2e8NRxHLMnfHOe5/LTm4uPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho17+/Ghd17/a8Wvbts2e8NR937MnvMXFRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRy1jjNkbmMDFRwkfJXyU8FHCRwkf9QVdARUAL1SkoAAAAABJRU5ErkJggg==%0A">
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABKklEQVR4nO3cwW2EQBQFQWNtfERBHqRCBCRDBiQyDsDrlffisdRV17k8qfUlTixjjA96PmcPYA7ho4SPEj5K+KjHq8fjOP7dJ/+2bbMnPLWu6+wJ35znufz05uKjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4qJc/P7qu6692/Nq+77MnPHXf9+wJb3HxUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcsYY/YGJnDxUcJHCR8lfJTwUcJHfQHghBRJGPVeCAAAAABJRU5ErkJggg==%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-3.-(f)-sequence">
<a class="anchor" href="#...-3.-(f)-sequence" aria-hidden="true"><span class="octicon octicon-link"></span></a>... 3. (f) sequence<a class="anchor-link" href="#...-3.-(f)-sequence"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_3_f</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">)</span> 
<span class="n">info</span><span class="p">(</span><span class="n">X_3_f</span><span class="p">,</span> <span class="s2">"'X three (f)'"</span><span class="p">)</span>
<span class="n">viz_tens</span><span class="p">(</span> <span class="mi">25</span> <span class="o">*</span> <span class="n">X_3_f</span> <span class="p">)</span> <span class="c1"># note the modular arith. being performed automatically</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     'X three (f)' 

    num. dims        1
    num. entries     10
    shape            [10]


┌────┬────┬────┬────┬────┬────┬────┬────┬────┬────┐
│ 10 │ 11 │ 12 │ 13 │ 14 │ 15 │ 16 │ 17 │ 18 │ 19 │
└────┴────┴────┴────┴────┴────┴────┴────┴────┴────┘



</pre>
</div>
</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAAAZCAYAAAD30ppqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAf0lEQVRoge3awQnAIBAAwVwIXP/t2cCVcakgwTzEwO58FRUWH4LR3Yd4zt0H0B6GhzI8lOGhDA9leKjrbTAzp996VTW96Ze5K9f+w9yVa48x4mnMGw9leCjDQxkeyvBQhocyPJThoQwPZXio8AcOkzceyvBQhocyPJThoQwPdQM4ZCIt7yfBxgAAAABJRU5ErkJggg==%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="numpy-objects-and-tensors">
<a class="anchor" href="#numpy-objects-and-tensors" aria-hidden="true"><span class="octicon octicon-link"></span></a><font color="teal">numpy objects and tensors</font><a class="anchor-link" href="#numpy-objects-and-tensors"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Tensors can be converted to numpy arrays, and numpy arrays back to tensors.
To transform a numpy array into a tensor, we can use the function <code>torch.from_numpy</code>, and we use <code>np.array</code> for the other direction.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The conversion of tensors to numpy require the tensor to be on the CPU, and not the GPU.</p>
<p>In case you have a tensor on GPU, you need to call <code>.cpu()</code> on the tensor beforehand.
Hence, you get a line like <code>np_arr = tensor.cpu().numpy()</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"t: </span><span class="si">{</span><span class="n">t</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"n: </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>t: tensor([1., 1., 1., 1., 1.])
n: [1. 1. 1. 1. 1.]
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">t</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"t: </span><span class="si">{</span><span class="n">t</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"n: </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>t: tensor([2., 2., 2., 2., 2.])
n: [2. 2. 2. 2. 2.]
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="...-tensor-operations">
<a class="anchor" href="#...-tensor-operations" aria-hidden="true"><span class="octicon octicon-link"></span></a><font color="teal">... tensor operations</font><a class="anchor-link" href="#...-tensor-operations"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Most operations existing in numpy also exist in PyTorch. A full list of operations can be found in the <a href="https://pytorch.org/docs/stable/tensors.html#">PyTorch documentation</a>.</p>
<ul>
<li>
<p>Each torch operation can be run on the GPU.</p>
</li>
<li>
<p>By default, tensors are created on the CPU. Unless we are using a package like <code>pytorch-lightning</code>, we need to explicitly move tensors to the GPU using the <code>.to</code> method, after checking GPU availability.</p>
</li>
<li>
<p>Copying large tensors across devices can be expensive in terms of time and memory.</p>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-adding-tensors">
<a class="anchor" href="#...-adding-tensors" aria-hidden="true"><span class="octicon octicon-link"></span></a>... adding tensors<a class="anchor-link" href="#...-adding-tensors"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_1</span><span class="p">,</span> <span class="n">X_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>

<span class="n">viz_tens_list</span><span class="p">(</span> <span class="p">[</span> <span class="n">X_1</span><span class="p">,</span> <span class="n">X_2</span> <span class="p">]</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAV0AAAC1CAYAAAD86CzsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAC90lEQVR4nO3asW3CYBhF0Tiy2IKSFWAzxqCmgGEYwCuYUcwCAZTmOgrntF/xXF39hYdlWb4AaHyv/QEAn0R0AUKiCxASXYCQ6AKExjd3vzYA/N7w7OClCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQGhc+wPWdLlc0r3r9ZrubTabdO92u6V7pcPhkO5tt9t0b5qmdO9+v6d7f4mXLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQGtf+gDXN85zunU6ndO98Pqd7/9nxeEz39vt9urfb7dK9T+alCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKEhmVZXt1fHgH40fDs4KULEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5AaHxzH5KvAPgQXroAIdEFCIkuQEh0AUKiCxASXYDQA8kyHevFJ1f6AAAAAElFTkSuQmCC%0A">
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Y</span> <span class="o">=</span> <span class="n">X_1</span> <span class="o">+</span> <span class="n">X_2</span>
<span class="n">viz_tens</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> 
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABJ0lEQVR4nO3csWnEQBRFUctsiapIlagOtTGZmhkX4PViJzuGe046yYPLB0Xa5pwf9HyuHsAawkcJHyV8lPBRj1ePY4x/98l/XdfqCU+NMVZP+OY8z+2nNxcfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCR738+dG+7+/a8WvHcaye8NR936sn/ImLjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEj9rmnKs3sICLjxI+Svgo4aOEjxI+6gu2JhW9RmsJUQAAAABJRU5ErkJggg==%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-stacking-tensors">
<a class="anchor" href="#...-stacking-tensors" aria-hidden="true"><span class="octicon octicon-link"></span></a>... stacking tensors<a class="anchor-link" href="#...-stacking-tensors"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_1</span><span class="p">,</span> <span class="n">X_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">15</span><span class="p">)</span>

<span class="n">info</span><span class="p">(</span><span class="n">X_1</span><span class="p">,</span> <span class="s2">"X 1"</span><span class="p">)</span>

<span class="n">info</span><span class="p">(</span><span class="n">X_2</span><span class="p">,</span> <span class="s2">"X 2"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     X 1 

    num. dims        1
    num. entries     5
    shape            [5]


┌───┬───┬───┬───┬───┐
│ 5 │ 6 │ 7 │ 8 │ 9 │
└───┴───┴───┴───┴───┘



tensor     X 2 

    num. dims        1
    num. entries     5
    shape            [5]


┌────┬────┬────┬────┬────┐
│ 10 │ 11 │ 12 │ 13 │ 14 │
└────┴────┴────┴────┴────┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">X_1</span><span class="p">,</span> <span class="n">X_2</span><span class="p">],</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="s2">"Y"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     Y 

    num. dims        2
    num. entries     10
    shape            [2 5]


┌────┬────┬────┬────┬────┐
│  5 │  6 │  7 │  8 │  9 │
├────┼────┼────┼────┼────┤
│ 10 │ 11 │ 12 │ 13 │ 14 │
└────┴────┴────┴────┴────┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-in-place-operations">
<a class="anchor" href="#...-in-place-operations" aria-hidden="true"><span class="octicon octicon-link"></span></a>... in-place operations<a class="anchor-link" href="#...-in-place-operations"> </a>
</h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Operations that store the result into the operand are called <em>in-place</em>. They are  usually marked with a underscore postfix, e.g. "<code>add_</code>" instead of "<code>add</code>". The operation <code>X.copy_(Y)</code> will change <code>X</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Calling <code>x1 + x2</code> creates a new tensor containing the sum of the two inputs.
However, we can also use in-place operations that are applied directly on the memory of a tensor.
We therefore change the values of <code>x2</code> without the chance to re-accessing the values of <code>x2</code> before the operation.
An example is shown below:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_1</span><span class="p">,</span> <span class="n">X_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\t</span><span class="s2">"</span><span class="p">,</span><span class="s2">"before"</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">X_1</span><span class="p">,</span> <span class="s2">"'X one'"</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">X_2</span><span class="p">,</span> <span class="s2">"'X two'"</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n\n</span><span class="s2">"</span><span class="p">)</span>

<span class="n">X_2</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">X_1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\t</span><span class="s2">"</span><span class="p">,</span><span class="s2">"after"</span><span class="p">,</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">X_1</span><span class="p">,</span> <span class="s2">"'X one'"</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">X_2</span><span class="p">,</span> <span class="s2">"'X two'"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>	 before 

tensor     'X one' 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌────────────────────┬────────────────────┬────────────────────┐
│ 0.6161253452301025 │ 0.7582958936691284 │ 0.5906646847724915 │
├────────────────────┼────────────────────┼────────────────────┤
│ 0.3219376802444458 │ 0.7609710693359375 │ 0.7627565860748291 │
└────────────────────┴────────────────────┴────────────────────┘



tensor     'X two' 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌────────────────────┬─────────────────────┬─────────────────────┐
│ 0.6869636178016663 │ 0.41213929653167725 │ 0.36759936809539795 │
├────────────────────┼─────────────────────┼─────────────────────┤
│ 0.5534904599189758 │  0.4116729497909546 │ 0.35099947452545166 │
└────────────────────┴─────────────────────┴─────────────────────┘






	 after 

tensor     'X one' 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌────────────────────┬────────────────────┬────────────────────┐
│ 0.6161253452301025 │ 0.7582958936691284 │ 0.5906646847724915 │
├────────────────────┼────────────────────┼────────────────────┤
│ 0.3219376802444458 │ 0.7609710693359375 │ 0.7627565860748291 │
└────────────────────┴────────────────────┴────────────────────┘



tensor     'X two' 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌────────────────────┬────────────────────┬────────────────────┐
│  1.303088903427124 │ 1.1704351902008057 │ 0.9582640528678894 │
├────────────────────┼────────────────────┼────────────────────┤
│ 0.8754281401634216 │  1.172644019126892 │ 1.1137560606002808 │
└────────────────────┴────────────────────┴────────────────────┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>In-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss of history. Hence, their use is discouraged.</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-reshaping-tensors">
<a class="anchor" href="#...-reshaping-tensors" aria-hidden="true"><span class="octicon octicon-link"></span></a>... reshaping tensors<a class="anchor-link" href="#...-reshaping-tensors"> </a>
</h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Another common operation aims at changing the shape of a tensor.
A tensor of size <code>(2,3)</code> can be re-organized to any other shape with the same number of elements (e.g. a tensor of size <code>(6)</code>, or <code>(3,2)</code>, ...).</p>
<p>In PyTorch, this reshaping operation is called <code>view</code>:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="s2">"X"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     X 

    num. dims        1
    num. entries     6
    shape            [6]


┌───┬───┬───┬───┬───┬───┐
│ 0 │ 1 │ 2 │ 3 │ 4 │ 5 │
└───┴───┴───┴───┴───┴───┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="s2">"X"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     X 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌───┬───┬───┐
│ 0 │ 1 │ 2 │
├───┼───┼───┤
│ 3 │ 4 │ 5 │
└───┴───┴───┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-transposing-tensors-(permuting-dimensions)">
<a class="anchor" href="#...-transposing-tensors-(permuting-dimensions)" aria-hidden="true"><span class="octicon octicon-link"></span></a>... transposing tensors (permuting dimensions)<a class="anchor-link" href="#...-transposing-tensors-(permuting-dimensions)"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="s2">"X with 0th and 1st dimensions permuted"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     X with 0th and 1st dimensions permuted 

    num. dims        2
    num. entries     6
    shape            [3 2]


┌───┬───┐
│ 0 │ 3 │
├───┼───┤
│ 1 │ 4 │
├───┼───┤
│ 2 │ 5 │
└───┴───┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span>
<span class="n">info</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="s2">"X transposed again"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     X transposed again 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌───┬───┬───┐
│ 0 │ 1 │ 2 │
├───┼───┼───┤
│ 3 │ 4 │ 5 │
└───┴───┴───┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-numpy-like-indexing-and-slicing">
<a class="anchor" href="#...-numpy-like-indexing-and-slicing" aria-hidden="true"><span class="octicon octicon-link"></span></a>... numpy-like indexing and slicing<a class="anchor-link" href="#...-numpy-like-indexing-and-slicing"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span> <span class="mi">4</span><span class="p">,</span><span class="mi">4</span> <span class="p">)</span>
<span class="n">info</span><span class="p">(</span> <span class="n">X</span><span class="p">,</span> <span class="s2">"X"</span> <span class="p">)</span>
<span class="n">info</span><span class="p">(</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">"first row of X"</span> <span class="p">)</span>
<span class="n">info</span><span class="p">(</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="s2">"first column of X"</span> <span class="p">)</span>
<span class="n">info</span><span class="p">(</span> <span class="n">X</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s2">"last column of X"</span> <span class="p">)</span>
<span class="n">info</span><span class="p">(</span> <span class="n">X</span><span class="p">[:</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s2">"First two rows, last column"</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span> <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">,</span> <span class="p">:],</span> <span class="s2">"Middle two rows"</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     X 

    num. dims        2
    num. entries     16
    shape            [4 4]


┌───────────────────────┬──────────────────────┬─────────────────────┬──────────────────────┐
│ 0.0025978684425354004 │    0.834635317325592 │  0.8788173198699951 │   0.6822240948677063 │
├───────────────────────┼──────────────────────┼─────────────────────┼──────────────────────┤
│   0.15136289596557617 │ 0.006530046463012695 │ 0.09391051530838013 │   0.8728501200675964 │
├───────────────────────┼──────────────────────┼─────────────────────┼──────────────────────┤
│    0.7400528788566589 │    0.920752227306366 │  0.7619349360466003 │   0.6265460848808289 │
├───────────────────────┼──────────────────────┼─────────────────────┼──────────────────────┤
│     0.495103657245636 │  0.11974698305130005 │ 0.07161390781402588 │ 0.032325685024261475 │
└───────────────────────┴──────────────────────┴─────────────────────┴──────────────────────┘



tensor     first row of X 

    num. dims        1
    num. entries     4
    shape            [4]


┌───────────────────────┬───────────────────┬────────────────────┬────────────────────┐
│ 0.0025978684425354004 │ 0.834635317325592 │ 0.8788173198699951 │ 0.6822240948677063 │
└───────────────────────┴───────────────────┴────────────────────┴────────────────────┘



tensor     first column of X 

    num. dims        1
    num. entries     4
    shape            [4]


┌───────────────────────┬─────────────────────┬────────────────────┬───────────────────┐
│ 0.0025978684425354004 │ 0.15136289596557617 │ 0.7400528788566589 │ 0.495103657245636 │
└───────────────────────┴─────────────────────┴────────────────────┴───────────────────┘



tensor     last column of X 

    num. dims        1
    num. entries     4
    shape            [4]


┌────────────────────┬────────────────────┬────────────────────┬──────────────────────┐
│ 0.6822240948677063 │ 0.8728501200675964 │ 0.6265460848808289 │ 0.032325685024261475 │
└────────────────────┴────────────────────┴────────────────────┴──────────────────────┘



tensor     First two rows, last column 

    num. dims        1
    num. entries     2
    shape            [2]


┌────────────────────┬────────────────────┐
│ 0.6822240948677063 │ 0.8728501200675964 │
└────────────────────┴────────────────────┘



tensor     Middle two rows 

    num. dims        2
    num. entries     8
    shape            [2 4]


┌─────────────────────┬──────────────────────┬─────────────────────┬────────────────────┐
│ 0.15136289596557617 │ 0.006530046463012695 │ 0.09391051530838013 │ 0.8728501200675964 │
├─────────────────────┼──────────────────────┼─────────────────────┼────────────────────┤
│  0.7400528788566589 │    0.920752227306366 │  0.7619349360466003 │ 0.6265460848808289 │
└─────────────────────┴──────────────────────┴─────────────────────┴────────────────────┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">info</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="s2">"modified X"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     modified X 

    num. dims        2
    num. entries     16
    shape            [4 4]


┌─────────────────────┬─────┬────────────────────┬─────────────────────┐
│ 0.33701878786087036 │ 0.0 │ 0.8188108205795288 │  0.7308486700057983 │
├─────────────────────┼─────┼────────────────────┼─────────────────────┤
│ 0.05802798271179199 │ 0.0 │ 0.4210916757583618 │  0.9836747646331787 │
├─────────────────────┼─────┼────────────────────┼─────────────────────┤
│  0.5723287463188171 │ 0.0 │ 0.7068576216697693 │  0.3095592260360718 │
├─────────────────────┼─────┼────────────────────┼─────────────────────┤
│ 0.17637217044830322 │ 0.0 │ 0.2726491093635559 │ 0.39976662397384644 │
└─────────────────────┴─────┴────────────────────┴─────────────────────┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-other-operations">
<a class="anchor" href="#...-other-operations" aria-hidden="true"><span class="octicon octicon-link"></span></a>... other operations<a class="anchor-link" href="#...-other-operations"> </a>
</h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here are some ways to perform matrix multiplication:</p>
<ul>
<li>
<p><code>torch.matmul</code> $\quad$ Performs the matrix product over two tensors, where the specific behavior depends on the dimensions.
If both inputs are matrices (2-dimensional tensors), it performs the standard matrix product.
For higher dimensional inputs, the function supports broadcasting (for details see the <a href="https://pytorch.org/docs/stable/generated/torch.matmul.html?highlight=matmul#torch.matmul">documentation</a>).</p>
<p>It can also be written as <code>a @ b</code>, similar to numpy.</p>
</li>
<li>
<p><code>torch.mm</code> $\quad$ Performs the matrix product over two matrices, but doesn't support broadcasting (see <a href="https://pytorch.org/docs/stable/generated/torch.mm.html?highlight=torch%20mm#torch.mm">documentation</a>)</p>
</li>
<li>
<p><code>torch.bmm</code> $\quad$ Performs the matrix product with a support batch dimension. Let <code>T</code> be a tensor of shape <code>(b,  n, m)</code>, and <code>R</code> a tensor of shape <code>(b, m, p)</code>, the output tensor is of shape <code>(b, n , p)</code>, obtained by "entry-wise" matrix multiplication along the batch dimension.</p>
</li>
<li>
<p><code>torch.einsum</code> $\quad$ Performs matrix multiplications and more (i.e. sums of products) using the Einstein summation convention.</p>
</li>
</ul>
<p>Usually, we use <code>torch.matmul</code> or <code>torch.bmm</code>.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">9</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="n">info</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="s2">"X"</span><span class="p">)</span>

<span class="n">info</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="s2">"Y"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     X 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌───┬───┬───┐
│ 0 │ 1 │ 2 │
├───┼───┼───┤
│ 3 │ 4 │ 5 │
└───┴───┴───┘



tensor     Y 

    num. dims        2
    num. entries     9
    shape            [3 3]


┌───┬───┬───┐
│ 0 │ 1 │ 2 │
├───┼───┼───┤
│ 3 │ 4 │ 5 │
├───┼───┼───┤
│ 6 │ 7 │ 8 │
└───┴───┴───┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="s2">"Z"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     Z 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌────┬────┬────┐
│ 15 │ 18 │ 21 │
├────┼────┼────┤
│ 42 │ 54 │ 66 │
└────┴────┴────┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Given a tensor <code>X</code>, the tensors <code>Y_1</code>,<code>Y_2</code>, <code>Y_3</code> computed below all have the same value:</p>

<pre><code>Y_1 = X @ X.T
Y_2 = X.matmul(X.T)
Y_3 = torch.rand_like(X)
torch.matmul(X, X.T, out = Y_3)</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>On the other hand, <code>*</code> denotes the entrywise product of two tensors.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="s2">"X"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     X 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌───┬───┬───┐
│ 0 │ 1 │ 2 │
├───┼───┼───┤
│ 3 │ 4 │ 5 │
└───┴───┴───┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">info</span><span class="p">(</span> <span class="n">X</span> <span class="o">*</span> <span class="n">Y</span><span class="p">,</span> <span class="s2">"(a)"</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span> <span class="n">X</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">Y</span><span class="p">),</span> <span class="s2">"(b)"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     (a) 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌───┬────┬────┐
│ 0 │  1 │  4 │
├───┼────┼────┤
│ 9 │ 16 │ 25 │
└───┴────┴────┘



tensor     (b) 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌───┬────┬────┐
│ 0 │  1 │  4 │
├───┼────┼────┤
│ 9 │ 16 │ 25 │
└───┴────┴────┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You can use <code>torch.cat</code> to concatenate a sequence of tensors along a given dimension. See also <code>torch.stack</code>, another tensor joining op that is subtly different from <code>torch.cat</code></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span> <span class="p">[</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">],</span> <span class="n">dim</span> <span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="s2">"Y"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     Y 

    num. dims        2
    num. entries     18
    shape            [2 9]


┌───┬───┬───┬───┬───┬───┬───┬───┬───┐
│ 0 │ 1 │ 2 │ 0 │ 1 │ 2 │ 0 │ 1 │ 2 │
├───┼───┼───┼───┼───┼───┼───┼───┼───┤
│ 3 │ 4 │ 5 │ 3 │ 4 │ 5 │ 3 │ 4 │ 5 │
└───┴───┴───┴───┴───┴───┴───┴───┴───┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If you have a one-element tensor, for example obtained by aggregating all values of a given tensor into a single value, you can convert it to a Python numerical value using <code>item()</code>:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">agg</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">agg_item</span> <span class="o">=</span> <span class="n">agg</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> 
<span class="nb">print</span><span class="p">(</span><span class="n">agg_item</span><span class="p">,</span> <span class="s2">"</span><span class="se">\t</span><span class="s2">"</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">agg_item</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>15 	 &lt;class 'int'&gt;
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="2.2-...-Learning-with-scalar-signals-on-a-cyclic-group">
<a class="anchor" href="#2.2-...-Learning-with-scalar-signals-on-a-cyclic-group" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.2 ... Learning with scalar signals on a cyclic group<a class="anchor-link" href="#2.2-...-Learning-with-scalar-signals-on-a-cyclic-group"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the simplest setting we consider, the underlying domain is a one-dimensional grid, and the signals only have a single channel. We can identify this grid $\Omega$ with the Cayley graph of cyclic group
$$
C_n = \langle \, a : a^n = 1 \, \rangle \equiv \{ \, 1, a, a^2, \dots, a^{n-1} \, \}.
$$
It is convenient to parametrize the group, and hence the grid, through the exponent of the generator 
$$
C_n \equiv \{ 0, 1, \dots, n -1 \}
$$
as this indexing is consistent with the way most programming languages index vectors, reinterpreting the group operation as addition modulo $n$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The vector space of single-channeled (i.e. real-valued) signals
$$
\mathcal{X}(C_n,\mathbb{R}) = \{ x : C_n \to \mathbb{R} \} ,
$$
is finite dimensional, and each $x \in \mathcal{X}(C_n, \mathbb{R})$ may be expressed as 
$$
x = 
\left[ 
\begin{matrix}
x_0\\ 
\vdots\\
\,x_{n-1}\,
\end{matrix}
\right] 
$$
with respect to some implicit coordinate system used by the computer, the <em>input coordinate system</em>. This is the same coordinate system used to express the representation $\rho$ of translation group $G \equiv C_n$, which we now describe.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Given a vector $\theta = (\theta_0 , \dots, \theta_{n-1})$, recall the associated <em>circulant matrix</em> is the $n \times n$ matrix with entries 
$$
\mathcal{C}(\theta) := \left( \, \theta_{ (u - v) \mod n} \right)_{ 0 \, \leq \,u,\,v \, \leq n-1 } 
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Consider the case of $\theta_S := (0,1,0,\dots, 0)^T$, the associated circulant matrix, $\mathbf{S} := \mathbf{C}(\theta_S)$ acts on vectors by shifting the entries of vectors to the right by one position, modulo $n$. This is a shift or translation operator, which we denote $\mathbf{S}$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>Lemma</strong> $\quad$
A matrix is circulant if and only if it commutes with $\mathbf{S}$. Moreover, given any two vectors $\theta, \eta \in \mathbb{R}^n$, one has $\mathbf{C}(\theta) \mathbf{C}(\eta) = \mathbf{C}(\eta) \mathbf{C}(\theta)$.</p>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The importance of $\mathbf{S}$ to the present discussion is that it generates a group isomorphic to the one-dimensional translation group $C_n$. This is to say, a natural representation of $C_n = \langle \, a : a^n = 1 \, \rangle$ to consider is the group isomorphism induced by mapping the generator $a$ of $C_n$ to $\mathbf{S}$. Specifically, the representation $\rho$ of $G$ over $\mathcal{X}( C_n, \mathbb{R})$ is given by
$$
\rho ( a^j ) := \mathbf{S}^j 
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>Corollary</strong> $\quad$ Any $f : \mathcal{X}(C_n, \mathbb{R}) \to \mathcal{X}(C_n,\mathbb{R})$ which is linear and $C_n$-equivariant can be expressed ( in the input coordinate system ) as an $n \times n$ circulant matrix $\mathbf{C}(\theta)$ for some vector $\theta$.</p>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<hr>
<p><strong>Example</strong> $\quad$ 
Our previous recipe for designing an equivariant function $F= \Phi( \mathbf{X}, \mathbf{A})$ using a local aggregation function $\varphi$. In this case, we can express
$$
\varphi ( \mathbf{x}_u, \mathbf{X}_{\mathcal{N}(u)} ) = \varphi( \mathbf{x}_{u-1}, \, \mathbf{x}_u, \, \mathbf{x}_{u+1} ),
$$
where the addition and subtraction in the indices above is understood to be modulo $n$.</p>
<p>If in addition, we insist that $\varphi$ is linear, then it has the form 
$$
 \varphi( \mathbf{x}_{u-1}, \, \mathbf{x}_u, \, \mathbf{x}_{u+1} ) = \theta_{-1} \mathbf{x}_{u-1} + \theta_0 \mathbf{x}_u + \theta_1 \mathbf{x}_{u+1},
$$
and in this case we can express $\mathbf{F} = \Phi (\mathbf{X}, \mathbf{A} )$ through the following matrix multiplication:
$$
\left[
\begin{matrix}
\theta_0 &amp; \theta_1 &amp; \text{ } &amp; \text{ } &amp; \theta_{-1} \\
\theta_{-1} &amp; \theta_0 &amp; \theta_1 &amp; \text{ } &amp;   \text{ } \\
\text{} &amp; \ddots &amp; \ddots &amp; \ddots &amp; \text{ } \\
\text{ } &amp; \text{ } &amp; \theta_{-1} &amp; \theta_0 &amp; \theta_1 \\
\theta_1 &amp; \text{ } &amp; \text{ } &amp; \theta_{-1} &amp; \theta_0 
\end{matrix} 
\right]
\left[
\begin{matrix}
\mathbf{x}_0 \\
\mathbf{x}_1 \\
\vdots \\
\,\mathbf{x}_{n-2} \, \\
\mathbf{x}_{n-1}  
\end{matrix}
\right]
$$
This special multi-diagonal structure is sometimes referred to as ``weight sharing" in the machine learning literature.</p>
<hr>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Circulant matrices are synonymous with discrete convolutions; for $x \in \mathcal{X}(\Omega,\mathbb{R})$ and $\theta \in \mathbb{R}^n$, their <em>convolution</em> $x \star \theta$ is defined by 
$$
( x \star \theta )_u := \sum_{v = 0}^{n-1} x_{v \mod n}\, \theta_{ (u-v) \mod n}  \, ,
$$
$$
\equiv \mathbf{C}(\theta) x 
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>Rmk</strong> $\quad$ This leads to an alternate, equivalent definition of convolution as a translation equivariant linear operation. Moreover by replacing translations by a more general group $G$, one can generalize convolution to settings whose domain has symmetry other than translational.</p>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="torch.nn.Conv2d">
<a class="anchor" href="#torch.nn.Conv2d" aria-hidden="true"><span class="octicon octicon-link"></span></a><code>torch.nn.Conv2d</code><a class="anchor-link" href="#torch.nn.Conv2d"> </a>
</h4>
<p>The arguments:</p>
<ul>
<li>
<p><code>in_channels</code></p>
</li>
<li>
<p><code>out_channels</code></p>
</li>
<li>
<p><code>kernel_size</code></p>
</li>
<li>
<p><code>stride</code> $\quad$ controls the stride for the cross-correlation, a single number or a tuple.</p>
</li>
<li>
<p><code>padding</code> $\quad$ controls amount of padding applied to the input. It can either be a string, <code>"valid"</code> or <code>"same"</code> or a tuple of ints giving the amount of implicit padding applied on both sides.</p>
</li>
<li>
<p><code>dilation</code> $\quad$ controls the spacing between kernel points; "also known as the a trous algorithm</p>
</li>
<li>
<p><code>groups</code> $\quad$ controls connections between inputs and outputs. The <code>in_channels</code> and <code>out_channels</code> must be divisible by <code>groups</code>. For example,</p>
<ul>
<li>
<p>At groups = 1, all inputs are convolved to all outputs</p>
</li>
<li>
<p>At groups = 2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated.</p>
</li>
<li>
<p>At groups = <code>in_channels</code>, each input channel is convolved with its own set of filters (of size <code>out_channels // in_channels</code>)</p>
</li>
</ul>
</li>
<li>
<p><code>bias</code></p>
</li>
<li>
<p><code>padding_mode</code>,</p>
</li>
<li>
<p><code>device</code>,</p>
</li>
<li>
<p><code>dtype</code></p>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let us now relate the shapes of the input and output to the parameters</p>
<table>
<thead>
<tr>
<th>input parameter</th>
<th>LaTeX symbol</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>in_channels</code></td>
<td>$\text{dim}(\mathcal{C})$</td>
</tr>
<tr>
<td><code>out_channels</code></td>
<td>$\text{dim}(\mathcal{C}_1)$</td>
</tr>
<tr>
<td><code>kernel_size</code></td>
<td>$k$</td>
</tr>
<tr>
<td><code>stride</code></td>
<td>$\lambda$</td>
</tr>
<tr>
<td><code>padding</code></td>
<td>$\rho$</td>
</tr>
<tr>
<td><code>dilation</code></td>
<td>$\delta$</td>
</tr>
<tr>
<td><code>groups</code></td>
<td>$M$        </td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Additionally, we use $N$ for the batch size of the input, <code>N</code>. We also let $(h,w)$ denote the height-width pair describing the shape of the input signal domain.</p>
<p>Correspondingly, we write $(h_1, w_1)$ for the height-width pair describing the shape of the output signal domain.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We remark that the stride can be either integer or a $2$-tuple, whose coordinates describe the vertical and horizontal stride respectively. We still write $\lambda$ for the stride when it is a tuple, and use $\lambda_h \equiv \lambda[0]$ and $\lambda_w \equiv \lambda[1]$ to denote its first and second coordinate, in this case. Likewise, the padding and kernel size may be $2$-tuples as well, and we use similar notation to denote their entries.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The full shape of the input to the layer includes the batch dimension, and is thus</p>
$$
(N, \text{dim}(\mathcal{C}), H, W) \,,
$$<p>while the shape of the output is</p>
$$
(N , \text{dim}(\mathcal{C}_1), H_1, W_1 )
$$<p>These shapes, in particular the spatial dimensions of each, are related as follows:</p>
<p>$\begin{align}
H_1 &amp;= \left\lfloor \frac{
    H + 2 \rho_h - \delta_h ( k_h -1) -1 }{\lambda_h}
\right\rfloor \\
W_1 &amp;= \left\lfloor \frac{
    W + 2 \rho_w - \delta_w ( k_w -1) -1 }{\lambda_w}
\right\rfloor
\end{align}$,</p>
<p>in particular, the batch size does not have any bearing on how the shapes of tensors transform.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The parameters to be learned are the weights $w^1$ and biases $b^1$. These are both <code>Tensor</code> objects, accessed from the layer as <code>Conv2d.weight</code> and <code>Conv2d.bias</code>. The shape of the weight tensor is</p>
$$
\textrm{shape}(w^1) =
\left( \, \text{dim}(\mathcal{C}_1),  \, \text{dim}(\mathcal{C}) \big/ M , k_h, k_w \right)
$$<p>The tensor $w^1$ thus has</p>
$$
\textrm{size}(w^1) = \textrm{dim}(\mathcal{C}_1) \textrm{dim} (\mathcal{C}) k_h k_w \big/ M
$$<p>scalar entries.</p>
<p>There is always the question of how to initialize weights. In the case of the <code>Conv2d</code> class, the weights are initialized to be i.i.d. $\text{Unif}( - \sqrt{ \alpha_1}, \sqrt{\alpha_1} )$ random variables, where</p>
$$
\alpha_1 := \frac{ \textrm{dim}(\mathcal{C}_1) }{\textrm{size}(w^1)}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The bias tensor is a much smaller object, we have</p>
<p>$
\begin{align}
\textrm{shape}(b^1) = (\, \textrm{dim}(\mathcal{C}_1  ) \,) \, , \quad \textrm{size}(b^1) = \textrm{dim}(\mathcal{C}_1)
\end{align}
$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Despite this, we use the same initialization (with mutual independence of all random variables in discussion) for the bias entries as we did for the weights.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="2.2-$\quad$-A-simple-CNN">
<a class="anchor" href="#2.2-%24%5Cquad%24-A-simple-CNN" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>2.2 $\quad$ A simple CNN</strong><a class="anchor-link" href="#2.2-%24%5Cquad%24-A-simple-CNN"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We consider possibly the simplest neural network that we can construct through the above blueprint. Suppose we have a binary classification problem, with the following hypothesis space. Let $\textsf{H}_1$ denote the hypothesis space of functions $f : \mathcal{X}( C_n, \mathbb{R}) \to \{0,1\}$ of the form</p>
$$
f = A \circ P \circ \mathbf{a} \circ B \,,
$$<p>where the components of $f$ are</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>where the components of $f$ are</p>
<ul>
<li>
<p>$B$  : $\quad$ A $C_n$-equivariant function, to be learned. It is represented as a circulant matrix $\mathbf{C}(\theta)$, where $\theta$ is a vector $\theta \equiv (\theta_0, \dots, \theta_{n-1})$ whose entries $\theta_j$ are parameters to be learned.</p>
</li>
<li>
<p>$ \mathbf{a} $ : $\quad$ We consider the ReLU activation function, $a : \mathbb{R} \to \mathbb{R}_{\geq\, 0}$ defined by $a(w) = \max(0,w)$, for $w \in \mathbb{R}$. The bold-face $\mathbf{a}$ denotes the entry-wise action of this function on a given vector;for $y \equiv (\,y_1, \,\dots, \, y_n \, ) \in \mathcal{X}(C_n, \mathbb{R})$, which we imagine as the output of $B(x)$ for some input signal $x$, we have $\mathbf{a} (y ) = ( \,  \max(0,y_1), \,  \dots, \, \max(0,y_n) )$. There are no learned parameters in this layer.</p>
</li>
<li>
<p>$P$ : $\quad$ A coarsening operator. In this case, let us say it is a <em>zero-padded group homomorphism</em>.</p>
<p>$P : C_n \to C_{n / d }$ for some divisor $d \mid n$ \footnote{zero-padding} , and let us say that it operates through max-pooling on the signal, over the pre-images of each element of $C_{n / d}$.</p>
</li>
<li>
<p>$A$ : $\quad$ A global-pooling layer. We assume this has the form of a fully-connected layer, followed by a softmax. Specifically,</p>
</li>
</ul>

</div>
</div>
</div>
</div>



  </div><a class="u-url" href="/geometric-deep-learning/jupyter/2021/10/22/GDL2.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/geometric-deep-learning/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/geometric-deep-learning/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/geometric-deep-learning/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>...</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" title="fastai"><svg class="svg-icon grey"><use xlink:href="/geometric-deep-learning/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/geometric-deep-learning/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
