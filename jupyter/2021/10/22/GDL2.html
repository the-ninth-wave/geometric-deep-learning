<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>2 | geometric deep learning</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="2" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="GDL" />
<meta property="og:description" content="GDL" />
<link rel="canonical" href="https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2021/10/22/GDL2.html" />
<meta property="og:url" content="https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2021/10/22/GDL2.html" />
<meta property="og:site_name" content="geometric deep learning" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-10-22T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"GDL","url":"https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2021/10/22/GDL2.html","@type":"BlogPosting","headline":"2","dateModified":"2021-10-22T00:00:00-05:00","datePublished":"2021-10-22T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2021/10/22/GDL2.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/geometric-deep-learning/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://the-ninth-wave.github.io/geometric-deep-learning/feed.xml" title="geometric deep learning" /><link rel="shortcut icon" type="image/x-icon" href="/geometric-deep-learning/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/geometric-deep-learning/">geometric deep learning</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/geometric-deep-learning/about/">About Me</a><a class="page-link" href="/geometric-deep-learning/search/">Search</a><a class="page-link" href="/geometric-deep-learning/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">2</h1><p class="page-description">GDL</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-10-22T00:00:00-05:00" itemprop="datePublished">
        Oct 22, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      31 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/geometric-deep-learning/categories/#jupyter">jupyter</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/the-ninth-wave/geometric-deep-learning/tree/master/_notebooks/2021-10-22-GDL2.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/geometric-deep-learning/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/the-ninth-wave/geometric-deep-learning/master?filepath=_notebooks%2F2021-10-22-GDL2.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/geometric-deep-learning/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/the-ninth-wave/geometric-deep-learning/blob/master/_notebooks/2021-10-22-GDL2.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/geometric-deep-learning/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#geometric-deep-learning-models">geometric deep learning models </a>
<ul>
<li class="toc-entry toc-h2"><a href="#2.1-...-tensors-in-pytorch">2.1 ... tensors in pytorch </a>
<ul>
<li class="toc-entry toc-h3"><a href="#...-helper-classes-and-functions-">... helper classes and functions  </a>
<ul>
<li class="toc-entry toc-h4"><a href="#method-...-info">method ... info </a></li>
<li class="toc-entry toc-h4"><a href="#method-...-viz_tens">method ... viz_tens </a></li>
<li class="toc-entry toc-h4"><a href="#method-...-viz_tens_list">method ... viz_tens_list </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#...-initializing-tensors">... initializing tensors </a>
<ul>
<li class="toc-entry toc-h4"><a href="#...-1.---from-lists">... 1.   from lists </a></li>
<li class="toc-entry toc-h4"><a href="#...-2.-from-numpy">... 2. from numpy </a></li>
<li class="toc-entry toc-h4"><a href="#...-3.-(a)-ones-of-given-shape">... 3. (a) ones of given shape </a></li>
<li class="toc-entry toc-h4"><a href="#...-3.-(b)-zeros-of-given-shape">... 3. (b) zeros of given shape </a></li>
<li class="toc-entry toc-h4"><a href="#...-3.-(c)-i.i.d.-uniform-of-given-shape">... 3. (c) i.i.d. uniform of given shape </a></li>
<li class="toc-entry toc-h4"><a href="#...-3.-(d)-i.i.d.-standard-normal-of-given-shape">... 3. (d) i.i.d. standard normal of given shape </a></li>
<li class="toc-entry toc-h4"><a href="#...-3.-(f)-sequence">... 3. (f) sequence </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#numpy-objects-and-tensors">numpy objects and tensors </a></li>
<li class="toc-entry toc-h3"><a href="#...-tensor-operations">... tensor operations </a>
<ul>
<li class="toc-entry toc-h4"><a href="#...-adding-tensors">... adding tensors </a></li>
<li class="toc-entry toc-h4"><a href="#...-stacking-tensors">... stacking tensors </a></li>
<li class="toc-entry toc-h4"><a href="#...-in-place-operations">... in-place operations </a></li>
<li class="toc-entry toc-h4"><a href="#...-reshaping-tensors">... reshaping tensors </a></li>
<li class="toc-entry toc-h4"><a href="#...-transposing-tensors-(permuting-dimensions)">... transposing tensors (permuting dimensions) </a></li>
<li class="toc-entry toc-h4"><a href="#...-numpy-like-indexing-and-slicing">... numpy-like indexing and slicing </a></li>
<li class="toc-entry toc-h4"><a href="#...-other-operations">... other operations </a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#2.2-...-one-dimensional-convolutions-of-scalar-valued-signals">2.2 ... one-dimensional convolutions of scalar-valued signals </a>
<ul>
<li class="toc-entry toc-h3"><a href="#...example:-local-averaging-as-a-circulant-matrix">...example: local averaging as a circulant matrix </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#2.3-...-one-dimensional-convolutions-in-torch">2.3 ... one-dimensional convolutions in torch </a>
<ul>
<li class="toc-entry toc-h4"><a href="#class-...-torch.nn.Conv1d">class ... torch.nn.Conv1d </a></li>
<li class="toc-entry toc-h3"><a href="#method-...-compute_out_size">method ... compute_out_size </a></li>
<li class="toc-entry toc-h3"><a href="#-example-concrete-layerh3divdivdivfor-now-we-will-instantiate-a-parameters-dictionary-with-dilation--1-and-padding--0-let-us-use-n--32-so-thattilden--leftlfloor-fracn-klambda--1-rightrfloor-equiv-leftlfloor-frac32-klambda--1-rightrfloorfor-our-example-we-specialize-to-kernel_size--4-and-stride--2-in-which-case-tilden--15------------odcp----one-dimensional-conv-parametersodcpin_size--32odcpin_channels--1odcppadding--0odcpdilation--1odcpkernel_size--4odcpstride--2odcpgroups--1odcpout_channels--16----------------------------------odcpout_size--compute_out_size----in_size--odcpin_size----padding--odcppadding----dilation--odcpdilation----kernel_size--odcpkernel_size----stride--odcpstride----print-odcpout_size---------150----one-of-the-important-free-parameters-we-can-choose-to-initialize-a-one-dimensional-convolutional-layer-is-the-number-of-out_channels-we-choose-out_channels--16-in-our-first-examplein-many-vision-related-cnns-the-first-layer-takes-in-a-grayscale-or-rgb-image-outputting-features-with-a-much-larger-number-of-channelsi-am-thinking-of-each-output-channel-as-a-separate-lint-roller-to-run-over-the-imagefrom-the-docs-the-parameter-groups-controls-the-connections-between-inputs-and-outputsboth-in_channels-and-out_channels-must-be-divisible-by-groupsat-groups--1-all-inputs-are-convolved-to-all-outputsat-groups--2-the-operation-becomes-equivalent-to-having-two-convolutional-layers-side-by-side-each-seeing-half-the-input-channels-and-producing-half-the-output-channels-and-both-subsequently-concatenatedat-groups--in_channels-each-input-channel-is-convolved-with-its-own-set-of-filters-of-size-out_channels--in_channelsby-taking-in_channels--1-we-must-have-groups--1-alsothe-learned-parameters-of-layer-tildeb-are-of-two-types-those-in-the-weight-tensor-tildetheta-and-those-in-the-bias-vector-tildeb-at-this-point-for-simplicity-we-set-tildeb-equiv-0-via-bias--false-so-that-the-only-learned-parameters-are-those-in-tildetheta------------from-torchnn-import-conv1d--------we-now-initialize-the-convolutional-layer------------b_tilde--conv1din_channels--odcpin_channels-----------------out_channels--odcpout_channels-----------------kernel_size--odcpkernel_size-----------------stride--odcpstride-----------------padding--odcppadding-----------------dilation--odcpdilation-----------------groups--odcpgroups-----------------bias--false------------------------let-us-initialize-a-few-possible-input-signals-x_1-x_2-x_3--------------------------x_1--torcharange032infox_1-x-1viz_tens-x_1-display_size--10-x_2--torchrand-32--infox_2-x-2viz_tens-x_2-display_size--10-x_3--torchrandn-32-infox_3-x-3viz_tens-x_3-display_size--10---------tensor-----x-1-----num-dims--------1----num-entries-----32----shape------------32-0--1--2--3--4--5--6--7--8--9--10--11--12--13--14--15--16--17--18--19--20--21--22--23--24--25--26--27--28--29--30--31-tensor-----x-2-----num-dims--------1----num-entries-----32----shape------------32-025--069--089--036--029--004--024--006--038--060--003--093--081--001--026--066--039--044--027--090--022--091--053--060--089--041--021--041--090--012--061--000-tensor-----x-3-----num-dims--------1----num-entries-----32----shape------------32--02--151--003--103--088---13---03--101---16---00---12--119---07--111--029--029---05--194---04---03--144---07--249---03--087--021--122---05---01---04---01---02-----to-apply-b_tilde-to-these-signals-we-must-augment-each-signal-with-batch-and-channel-dimensions--------------------------x_1--torchas_tensor-x_1-none-none---dtype-torchfloat32-x_2--torchas_tensor-x_2-none-none---dtype-torchfloat32-x_3--torchas_tensor-x_3-none-none---dtype-torchfloat32-------------each-output------------y_1--b_tilde-x_1-detachy_2--b_tilde-x_2-detachy_3--b_tilde-x_3-detachnparray-objsize-print-nparray-y_1size-------1-16-15--1-16-15--1-16-15----------------infoy_1-y-1----tensor-----y-1-----num-dims--------3----num-entries-----240----shape-------------1-16-15--0---0---1--06--04--00--06--18---1---0--14---0--00--10--13---2-----recall-that-in-general-one-hastildetheta-in-mathcalx--h-backslash-g--tildeh-mathcalc-otimes-tildemathcalc-but-let-us-reduce-this-further-based-on-the-assumptions-made-because-g-equiv-omega-equiv-c_n-subgroup-h-must-be-the-trivial-subgroup--e--moreover-having-assumed-that-dimmathcalc--1-one-also-has-mathcalc-otimes-tildemathcalc-cong-tildemathcalc-so-we-may-writetildetheta-in-mathcalx--g--tildeh---tildemathcalc-with-c_tilden-cong-g--tildeh24--two-dimensional-convolutions-in-torch-torchnnconv2d-the-argumentsin_channelsout_channelskernel_sizestride-quad-controls-the-stride-for-the-cross-correlation-a-single-number-or-a-tuplepadding-quad-controls-amount-of-padding-applied-to-the-input-it-can-either-be-a-string-valid-or-same-or-a-tuple-of-ints-giving-the-amount-of-implicit-padding-applied-on-both-sidesdilation-quad-controls-the-spacing-between-kernel-points-also-known-as-the-a-trous-algorithmgroups-quad-controls-connections-between-inputs-and-outputs-the-in_channels-and-out_channels-must-be-divisible-by-groups-for-exampleat-groups--1-all-inputs-are-convolved-to-all-outputsat-groups--2-the-operation-becomes-equivalent-to-having-two-conv-layers-side-by-side-each-seeing-half-the-input-channels-and-producing-half-the-output-channels-and-both-subsequently-concatenatedat-groups--in_channels-each-input-channel-is-convolved-with-its-own-set-of-filters-of-size-out_channels--in_channelsbiaspadding_modedevicedtypelet-us-now-relate-the-shapes-of-the-input-and-output-to-the-parametersinput-parameterlatex-symbolin_channelstextdimmathcalcout_channelstextdimmathcalc_1kernel_sizekstridelambdapaddingrhodilationdeltagroupsm--------additionally-we-use-n-for-the-batch-size-of-the-input-n-we-also-let-hw-denote-the-height-width-pair-describing-the-shape-of-the-input-signal-domaincorrespondingly-we-write-h_1-w_1-for-the-height-width-pair-describing-the-shape-of-the-output-signal-domainwe-remark-that-the-stride-can-be-either-integer-or-a-2-tuple-whose-coordinates-describe-the-vertical-and-horizontal-stride-respectively-we-still-write-lambda-for-the-stride-when-it-is-a-tuple-and-use-lambda_h-equiv-lambda0-and-lambda_w-equiv-lambda1-to-denote-its-first-and-second-coordinate-in-this-case-likewise-the-padding-and-kernel-size-may-be-2-tuples-as-well-and-we-use-similar-notation-to-denote-their-entriesthe-full-shape-of-the-input-to-the-layer-includes-the-batch-dimension-and-is-thusn-textdimmathcalc-h-w-while-the-shape-of-the-output-isn--textdimmathcalc_1-h_1-w_1-these-shapes-in-particular-the-spatial-dimensions-of-each-are-related-as-followsbeginalignh_1--leftlfloor-frac----h--2-rho_h---delta_h--k_h--1--1-lambda_hrightrfloor-w_1--leftlfloor-frac----w--2-rho_w---delta_w--k_w--1--1-lambda_wrightrfloorendalignin-particular-the-batch-size-does-not-have-any-bearing-on-how-the-shapes-of-tensors-transformthe-parameters-to-be-learned-are-the-weights-w1-and-biases-b1-these-are-both-tensor-objects-accessed-from-the-layer-as-conv2dweight-and-conv2dbias-the-shape-of-the-weight-tensor-istextrmshapew1-left--textdimmathcalc_1---textdimmathcalc-big-m--k_h-k_w-rightthe-tensor-w1-thus-hastextrmsizew1--textrmdimmathcalc_1-textrmdim-mathcalc-k_h-k_w-big-mscalar-entriesthere-is-always-the-question-of-how-to-initialize-weights-in-the-case-of-the-conv2d-class-the-weights-are-initialized-to-be-iid-textunif---sqrt-alpha_1-sqrtalpha_1--random-variables-wherealpha_1--frac-textrmdimmathcalc_1-textrmsizew1the-bias-tensor-is-a-much-smaller-object-we-havebeginaligntextrmshapeb1---textrmdimmathcalc_1------quad-textrmsizeb1--textrmdimmathcalc_1endaligndespite-this-we-use-the-same-initialization-with-mutual-independence-of-all-random-variables-in-discussion-for-the-bias-entries-as-we-did-for-the-weights-a-simple-cnn-we-consider-possibly-the-simplest-neural-network-that-we-can-construct-through-the-above-blueprint-suppose-we-have-a-binary-classification-problem-with-the-following-hypothesis-space-let-textsfh_1-denote-the-hypothesis-space-of-functions-f--mathcalx-c_n-mathbbr-to-01-of-the-formf--a-circ-p-circ-mathbfa-circ-b-where-the-components-of-f-arewhere-the-components-of-f-areb---quad-a-c_n-equivariant-function-to-be-learned-it-is-represented-as-a-circulant-matrix-mathbfctheta-where-theta-is-a-vector-theta-equiv-theta_0-dots-theta_n-1-whose-entries-theta_j-are-parameters-to-be-learned-mathbfa---quad-we-consider-the-relu-activation-function-a--mathbbr-to-mathbbr_geq-0-defined-by-aw--max0w-for-w-in-mathbbr-the-bold-face-mathbfa-denotes-the-entry-wise-action-of-this-function-on-a-given-vectorfor-y-equiv-y_1-dots--y_n---in-mathcalxc_n-mathbbr-which-we-imagine-as-the-output-of-bx-for-some-input-signal-x-we-have-mathbfa-y------max0y_1---dots--max0y_n--there-are-no-learned-parameters-in-this-layerp--quad-a-coarsening-operator-in-this-case-let-us-say-it-is-a-zero-padded-group-homomorphismp--c_n-to-c_n--d--for-some-divisor-d-mid-n-footnotezero-padding--and-let-us-say-that-it-operates-through-max-pooling-on-the-signal-over-the-pre-images-of-each-element-of-c_n--da--quad-a-global-pooling-layer-we-assume-this-has-the-form-of-a-fully-connected-layer-followed-by-a-softmax-specificallydiv-">... example: concrete layer&lt;/h3&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;


For now, we will instantiate a parameters dictionary with dilation = 1 and padding = 0. Let us use $n = 32$, so that
$$
\tilde{n} = \left\lfloor \frac{n-k}{\lambda} + 1 \right\rfloor \equiv \left\lfloor \frac{32-k}{\lambda} + 1 \right\rfloor
$$For our example, we specialize to kernel_size = 4 and stride = 2, in which case $\tilde{n} = 15$.




    
    




    
odcp = {} # one dimensional conv. parameters

odcp[&quot;in_size&quot;] = 32
odcp[&quot;in_channels&quot;] = 1
odcp[&quot;padding&quot;] = 0
odcp[&quot;dilation&quot;] = 1
odcp[&quot;kernel_size&quot;] = 4
odcp[&quot;stride&quot;] = 2
odcp[&quot;groups&quot;] = 1
odcp[&quot;out_channels&quot;] = 16


    




    

    
    


      
        


    
odcp[&quot;out_size&quot;] = compute_out_size(
    in_size = odcp[&quot;in_size&quot;],
    padding = odcp[&quot;padding&quot;],
    dilation = odcp[&quot;dilation&quot;],
    kernel_size = odcp[&quot;kernel_size&quot;],
    stride = odcp[&quot;stride&quot;]
    )

print( odcp[&quot;out_size&quot;] )


    



    






15.0








    



One of the important free parameters we can choose to initialize a one-dimensional convolutional layer is the number of out_channels. We choose out_channels = 16 in our first example.
In many vision related CNNs, the first layer takes in a grayscale or RGB image, outputting features with a much larger number of channels.
I am thinking of each output channel as a separate &quot;lint roller&quot; to run over the image.







From the docs, the parameter groups &quot;controls the connections between inputs and outputs.&quot;
Both in_channels and out_channels must be divisible by groups:

At groups = 1, all inputs are convolved to all outputs.

At groups = 2, the operation becomes equivalent to having two convolutional layers side by side, each seeing half the input channels and producing half the output channels, and both subsequently concatenated.

At groups = in_channels, each input channel is convolved with its own set of filters (of size out_channels / in_channels).


By taking in_channels = 1, we must have groups = 1 also.






The learned parameters of layer $\tilde{B}$ are of two types: those in the weight tensor $\tilde{\theta}$, and those in the bias vector $\tilde{b}$. At this point, for simplicity, we set $\tilde{b} \equiv 0$ via bias = False, so that the only learned parameters are those in $\tilde{\theta}$.




    
    




    
from torch.nn import Conv1d


    




    



We now initialize the convolutional layer.




    
    




    
B_tilde = Conv1d(in_channels = odcp[&quot;in_channels&quot;],
                 out_channels = odcp[&quot;out_channels&quot;],
                 kernel_size = odcp[&quot;kernel_size&quot;],
                 stride = odcp[&quot;stride&quot;],
                 padding = odcp[&quot;padding&quot;],
                 dilation = odcp[&quot;dilation&quot;],
                 groups = odcp[&quot;groups&quot;],
                 bias = False
                )


    




    



Let us initialize a few possible input signals: X_1, X_2, X_3.




    
    


      
        


    
X_1 = torch.arange(0,32)
info(X_1, &quot;X 1&quot;)
viz_tens( X_1, display_size = 10 )

X_2 = torch.rand( (32) ) 
info(X_2, &quot;X 2&quot;)
viz_tens( X_2, display_size = 10 )

X_3 = torch.randn( (32) )
info(X_3, &quot;X 3&quot;)
viz_tens( X_3, display_size = 10 )


    



    






tensor     X 1 

    num. dims        1
    num. entries     32
    shape            [32]


┌───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┐
│ 0 │ 1 │ 2 │ 3 │ 4 │ 5 │ 6 │ 7 │ 8 │ 9 │ 10 │ 11 │ 12 │ 13 │ 14 │ 15 │ 16 │ 17 │ 18 │ 19 │ 20 │ 21 │ 22 │ 23 │ 24 │ 25 │ 26 │ 27 │ 28 │ 29 │ 30 │ 31 │
└───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┘



tensor     X 2 

    num. dims        1
    num. entries     32
    shape            [32]


┌──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┐
│ 0.25 │ 0.69 │ 0.89 │ 0.36 │ 0.29 │ 0.04 │ 0.24 │ 0.06 │ 0.38 │ 0.60 │ 0.03 │ 0.93 │ 0.81 │ 0.01 │ 0.26 │ 0.66 │ 0.39 │ 0.44 │ 0.27 │ 0.90 │ 0.22 │ 0.91 │ 0.53 │ 0.60 │ 0.89 │ 0.41 │ 0.21 │ 0.41 │ 0.90 │ 0.12 │ 0.61 │ 0.00 │
└──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┘



tensor     X 3 

    num. dims        1
    num. entries     32
    shape            [32]


┌──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┐
│ -0.2 │ 1.51 │ 0.03 │ 1.03 │ 0.88 │ -1.3 │ -0.3 │ 1.01 │ -1.6 │ -0.0 │ -1.2 │ 1.19 │ -0.7 │ 1.11 │ 0.29 │ 0.29 │ -0.5 │ 1.94 │ -0.4 │ -0.3 │ 1.44 │ -0.7 │ 2.49 │ -0.3 │ 0.87 │ 0.21 │ 1.22 │ -0.5 │ -0.1 │ -0.4 │ -0.1 │ -0.2 │
└──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┘









































    



To apply B_tilde to these signals, we must augment each signal with batch and channel dimensions:




    
    


      
        


    
X_1 = torch.as_tensor( X_1[ None, None, : ], dtype= torch.float32 )
X_2 = torch.as_tensor( X_2[ None, None, : ], dtype= torch.float32 )
X_3 = torch.as_tensor( X_3[ None, None, : ], dtype= torch.float32 )


    



    

    



Each output




    
    




    
Y_1 = B_tilde( X_1 ).detach()
Y_2 = B_tilde( X_2 ).detach()
Y_3 = B_tilde( X_3 ).detach()

#np.array( obj.size() )

print( np.array( Y_1.size() ) )


    









[ 1 16 15] [ 1 16 15] [ 1 16 15]








    

    
    




    
info(Y_1, &quot;Y 1&quot;)


    









tensor     Y 1 

    num. dims        3
    num. entries     240
    shape            [ 1 16 15]


┌──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┐
│ [-0. │ [-0. │ [-1. │ [0.6 │ [0.4 │ [0.0 │ [0.6 │ [1.8 │ [-1. │ [-0. │ [1.4 │ [-0. │ [0.0 │ [1.0 │ [1.3 │ [-2. │
└──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┘











    



Recall that, in general, one has
$$
\tilde{\theta} \in \mathcal{X} ( H \backslash G / \tilde{H}, \mathcal{C} \otimes \tilde{\mathcal{C}} ),
$$but let us reduce this further based on the assumptions made. Because $G \equiv \Omega \equiv C_n$, subgroup $H$ must be the trivial subgroup $\{ e \}$. Moreover, having assumed that $\dim{\mathcal{C}} = 1$, one also has $\mathcal{C} \otimes \tilde{\mathcal{C}} \cong \tilde{\mathcal{C}}$, so we may write
$$
\tilde{\theta} \in \mathcal{X} ( G / \tilde{H} ) , \tilde{\mathcal{C}} ),
$$with $C_{\tilde{n}} \cong G / \tilde{H}$.






2.4 ... two-dimensional convolutions in torch 






torch.nn.Conv2d 





The arguments:

in_channels

out_channels

kernel_size

stride $\quad$ controls the stride for the cross-correlation, a single number or a tuple.

padding $\quad$ controls amount of padding applied to the input. It can either be a string, &quot;valid&quot; or &quot;same&quot; or a tuple of ints giving the amount of implicit padding applied on both sides.

dilation $\quad$ controls the spacing between kernel points; &quot;also known as the a trous algorithm

groups $\quad$ controls connections between inputs and outputs. The in_channels and out_channels must be divisible by groups. For example,

At groups = 1, all inputs are convolved to all outputs

At groups = 2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated.

At groups = in_channels, each input channel is convolved with its own set of filters (of size out_channels // in_channels)



bias

padding_mode,

device,

dtype








Let us now relate the shapes of the input and output to the parameters


input parameter
LaTeX symbol




in_channels
$\text{dim}(\mathcal{C})$


out_channels
$\text{dim}(\mathcal{C}_1)$


kernel_size
$k$


stride
$\lambda$


padding
$\rho$


dilation
$\delta$


groups
$M$        









Additionally, we use $N$ for the batch size of the input, N. We also let $(h,w)$ denote the height-width pair describing the shape of the input signal domain.
Correspondingly, we write $(h_1, w_1)$ for the height-width pair describing the shape of the output signal domain.






We remark that the stride can be either integer or a $2$-tuple, whose coordinates describe the vertical and horizontal stride respectively. We still write $\lambda$ for the stride when it is a tuple, and use $\lambda_h \equiv \lambda[0]$ and $\lambda_w \equiv \lambda[1]$ to denote its first and second coordinate, in this case. Likewise, the padding and kernel size may be $2$-tuples as well, and we use similar notation to denote their entries.






The full shape of the input to the layer includes the batch dimension, and is thus
$$
(N, \text{dim}(\mathcal{C}), H, W) \,,
$$while the shape of the output is
$$
(N , \text{dim}(\mathcal{C}_1), H_1, W_1 )
$$These shapes, in particular the spatial dimensions of each, are related as follows:
$\begin{align}
H_1 &amp;= \left\lfloor \frac{
    H + 2 \rho_h - \delta_h ( k_h -1) -1 }{\lambda_h}
\right\rfloor \\
W_1 &amp;= \left\lfloor \frac{
    W + 2 \rho_w - \delta_w ( k_w -1) -1 }{\lambda_w}
\right\rfloor
\end{align}$,
in particular, the batch size does not have any bearing on how the shapes of tensors transform.






The parameters to be learned are the weights $w^1$ and biases $b^1$. These are both Tensor objects, accessed from the layer as Conv2d.weight and Conv2d.bias. The shape of the weight tensor is
$$
\textrm{shape}(w^1) =
\left( \, \text{dim}(\mathcal{C}_1),  \, \text{dim}(\mathcal{C}) \big/ M , k_h, k_w \right)
$$The tensor $w^1$ thus has
$$
\textrm{size}(w^1) = \textrm{dim}(\mathcal{C}_1) \textrm{dim} (\mathcal{C}) k_h k_w \big/ M
$$scalar entries.
There is always the question of how to initialize weights. In the case of the Conv2d class, the weights are initialized to be i.i.d. $\text{Unif}( - \sqrt{ \alpha_1}, \sqrt{\alpha_1} )$ random variables, where
$$
\alpha_1 := \frac{ \textrm{dim}(\mathcal{C}_1) }{\textrm{size}(w^1)}
$$





The bias tensor is a much smaller object, we have
$
\begin{align}
\textrm{shape}(b^1) = (\, \textrm{dim}(\mathcal{C}_1  ) \,) \, , \quad \textrm{size}(b^1) = \textrm{dim}(\mathcal{C}_1)
\end{align}
$






Despite this, we use the same initialization (with mutual independence of all random variables in discussion) for the bias entries as we did for the weights.






... A simple CNN 





We consider possibly the simplest neural network that we can construct through the above blueprint. Suppose we have a binary classification problem, with the following hypothesis space. Let $\textsf{H}_1$ denote the hypothesis space of functions $f : \mathcal{X}( C_n, \mathbb{R}) \to \{0,1\}$ of the form
$$
f = A \circ P \circ \mathbf{a} \circ B \,,
$$where the components of $f$ are






where the components of $f$ are

$B$  : $\quad$ A $C_n$-equivariant function, to be learned. It is represented as a circulant matrix $\mathbf{C}(\theta)$, where $\theta$ is a vector $\theta \equiv (\theta_0, \dots, \theta_{n-1})$ whose entries $\theta_j$ are parameters to be learned.

$ \mathbf{a} $ : $\quad$ We consider the ReLU activation function, $a : \mathbb{R} \to \mathbb{R}_{\geq\, 0}$ defined by $a(w) = \max(0,w)$, for $w \in \mathbb{R}$. The bold-face $\mathbf{a}$ denotes the entry-wise action of this function on a given vector;for $y \equiv (\,y_1, \,\dots, \, y_n \, ) \in \mathcal{X}(C_n, \mathbb{R})$, which we imagine as the output of $B(x)$ for some input signal $x$, we have $\mathbf{a} (y ) = ( \,  \max(0,y_1), \,  \dots, \, \max(0,y_n) )$. There are no learned parameters in this layer.

$P$ : $\quad$ A coarsening operator. In this case, let us say it is a zero-padded group homomorphism.
$P : C_n \to C_{n / d }$ for some divisor $d \mid n$ \footnote{zero-padding} , and let us say that it operates through max-pooling on the signal, over the pre-images of each element of $C_{n / d}$.

$A$ : $\quad$ A global-pooling layer. We assume this has the form of a fully-connected layer, followed by a softmax. Specifically,






&lt;/div&gt;
 

</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#2.4-...-two-dimensional-convolutions-in-torch">2.4 ... two-dimensional convolutions in torch </a>
<ul>
<li class="toc-entry toc-h4"><a href="#torch.nn.Conv2d">torch.nn.Conv2d </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#...-A-simple-CNN">... A simple CNN </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-10-22-GDL2.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="geometric-deep-learning-models">
<a class="anchor" href="#geometric-deep-learning-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>geometric deep learning models<a class="anchor-link" href="#geometric-deep-learning-models"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="2.1-...-tensors-in-pytorch">
<a class="anchor" href="#2.1-...-tensors-in-pytorch" aria-hidden="true"><span class="octicon octicon-link"></span></a><font color="CornflowerBlue">2.1 ... tensors in pytorch</font><a class="anchor-link" href="#2.1-...-tensors-in-pytorch"> </a>
</h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">torch</span> <span class="o">--</span><span class="n">upgrade</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu111)
Collecting torch
  Downloading torch-1.10.0-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)
     |██████████████████████████████▎ | 834.1 MB 1.2 MB/s eta 0:00:42tcmalloc: large alloc 1147494400 bytes == 0x55e98d60e000 @  0x7f82bf648615 0x55e9534e44cc 0x55e9535c447a 0x55e9534e72ed 0x55e9535d8e1d 0x55e95355ae99 0x55e9535559ee 0x55e9534e8bda 0x55e95355ad00 0x55e9535559ee 0x55e9534e8bda 0x55e953557737 0x55e9535d9c66 0x55e953556daf 0x55e9535d9c66 0x55e953556daf 0x55e9535d9c66 0x55e953556daf 0x55e9534e9039 0x55e95352c409 0x55e9534e7c52 0x55e95355ac25 0x55e9535559ee 0x55e9534e8bda 0x55e953557737 0x55e9535559ee 0x55e9534e8bda 0x55e953556915 0x55e9534e8afa 0x55e953556c0d 0x55e9535559ee
     |████████████████████████████████| 881.9 MB 16 kB/s 
Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)
Installing collected packages: torch
  Attempting uninstall: torch
    Found existing installation: torch 1.9.0+cu111
    Uninstalling torch-1.9.0+cu111:
      Successfully uninstalled torch-1.9.0+cu111
<span class="ansi-red-fg">ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
torchvision 0.10.0+cu111 requires torch==1.9.0, but you have torch 1.10.0 which is incompatible.
torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.10.0 which is incompatible.</span>
Successfully installed torch-1.10.0
</pre>
</div>
</div>

<div class="output_area">




</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.utils.data</span> <span class="k">as</span> <span class="nn">data</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Using torch"</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Using torch 1.9.0+cu111
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="...-helper-classes-and-functions-">
<a class="anchor" href="#...-helper-classes-and-functions-" aria-hidden="true"><span class="octicon octicon-link"></span></a><font color="teal">... helper classes and functions </font><a class="anchor-link" href="#...-helper-classes-and-functions-"> </a>
</h3>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>pip install tabletext
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Collecting tabletext
  Downloading tabletext-0.1.tar.gz (6.1 kB)
Building wheels for collected packages: tabletext
  Building wheel for tabletext (setup.py) ... done
  Created wheel for tabletext: filename=tabletext-0.1-py3-none-any.whl size=6022 sha256=aa9d9579b7a4b73606e8bfcc3de7c5b82bdac6ff7e33c90d90e28fad443d8760
  Stored in directory: /root/.cache/pip/wheels/cc/ae/ab/697f6cd9887c63663da889f796c2c7ea280bc407b16f6fd081
Successfully built tabletext
Installing collected packages: tabletext
Successfully installed tabletext-0.1
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tabletext</span> <span class="kn">import</span> <span class="n">to_text</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h4 id="method-...-info">
<a class="anchor" href="#method-...-info" aria-hidden="true"><span class="octicon octicon-link"></span></a>method ... <code>info</code><a class="anchor-link" href="#method-...-info"> </a>
</h4>
<p><em>can currently handle 1- and 2-tensors</em></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">info</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>

    <span class="n">ty_tens</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">])</span> <span class="p">)</span>

    <span class="n">ty_np</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">])</span> <span class="p">)</span>

    <span class="n">delim</span> <span class="o">=</span> <span class="s2">"   "</span>

    <span class="k">if</span> <span class="n">obj</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>

        <span class="n">display_obj</span> <span class="o">=</span> <span class="n">obj</span><span class="p">[</span><span class="kc">None</span><span class="p">,:]</span>

    <span class="k">elif</span> <span class="n">obj</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>

        <span class="n">display_obj</span> <span class="o">=</span> <span class="n">obj</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">display_obj</span> <span class="o">=</span> <span class="n">obj</span>

    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span> <span class="o">==</span> <span class="n">ty_tens</span><span class="p">:</span>

        <span class="nb">print</span><span class="p">(</span> <span class="s2">"tensor"</span><span class="p">,</span> <span class="n">delim</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span> <span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span> <span class="n">delim</span><span class="p">,</span> <span class="s2">"num. dims   "</span><span class="p">,</span> <span class="n">delim</span><span class="p">,</span> <span class="n">obj</span><span class="o">.</span><span class="n">ndim</span> <span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span> <span class="n">delim</span><span class="p">,</span> <span class="s2">"num. entries"</span><span class="p">,</span> <span class="n">delim</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span> <span class="n">obj</span> <span class="p">)</span><span class="o">.</span><span class="n">size</span> <span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span> <span class="n">delim</span><span class="p">,</span> <span class="s2">"shape       "</span><span class="p">,</span> <span class="n">delim</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span> <span class="n">obj</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="p">)</span> <span class="p">)</span>

    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span> <span class="o">==</span> <span class="n">ty_np</span><span class="p">:</span>

        <span class="nb">print</span><span class="p">(</span> <span class="s2">"np array"</span><span class="p">,</span> <span class="n">delim</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span> <span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span> <span class="n">delim</span><span class="p">,</span> <span class="s2">"number of dimensions"</span><span class="p">,</span> <span class="n">obj</span><span class="o">.</span><span class="n">ndim</span> <span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span> <span class="n">delim</span><span class="p">,</span> <span class="s2">"number of entries"</span><span class="p">,</span> <span class="n">delim</span><span class="p">,</span> <span class="n">obj</span><span class="o">.</span><span class="n">size</span> <span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span> <span class="n">delim</span><span class="p">,</span> <span class="s2">"shape"</span><span class="p">,</span> <span class="n">delim</span><span class="p">,</span> <span class="n">obj</span><span class="o">.</span><span class="n">shape</span> <span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>

    <span class="n">display_list</span> <span class="o">=</span> <span class="n">display_obj</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="n">J</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span> <span class="n">display_list</span> <span class="p">)</span>
    <span class="n">K</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span> <span class="n">display_list</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">outer_list</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">J</span><span class="p">):</span>

        <span class="n">inner_list</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>

            <span class="n">inner_list</span> <span class="o">+=</span> <span class="p">[</span> <span class="nb">str</span><span class="p">(</span> <span class="n">display_list</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="p">)[:</span><span class="mi">4</span><span class="p">]</span> <span class="p">]</span>

        <span class="n">outer_list</span> <span class="o">+=</span> <span class="p">[</span> <span class="n">inner_list</span> <span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span> <span class="n">to_text</span><span class="p">(</span> <span class="n">outer_list</span> <span class="p">)</span> <span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n\n</span><span class="s2">"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h4 id="method-...-viz_tens">
<a class="anchor" href="#method-...-viz_tens" aria-hidden="true"><span class="octicon octicon-link"></span></a>method ... <code>viz_tens</code><a class="anchor-link" href="#method-...-viz_tens"> </a>
</h4>
<p>args</p>
<ul>
<li>
<p><code>tens</code></p>
</li>
<li>
<p><code>display_size</code></p>
</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">viz_tens</span><span class="p">(</span> <span class="n">tens</span><span class="p">,</span> <span class="n">display_size</span> <span class="o">=</span> <span class="mi">2</span> <span class="p">):</span>

    <span class="n">size</span> <span class="o">=</span> <span class="n">display_size</span>

    <span class="k">if</span> <span class="n">tens</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>

        <span class="n">display_obj</span> <span class="o">=</span> <span class="n">tens</span><span class="p">[</span><span class="kc">None</span><span class="p">,:]</span>

        <span class="n">display_obj</span> <span class="o">=</span> <span class="n">display_obj</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

    <span class="k">elif</span> <span class="n">tens</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>

        <span class="n">display_obj</span> <span class="o">=</span> <span class="n">tens</span>

        <span class="n">display_obj</span> <span class="o">=</span> <span class="n">display_obj</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

    <span class="n">pil_image</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">ToPILImage</span><span class="p">()(</span> <span class="n">display_obj</span> <span class="p">)</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">"RGB"</span><span class="p">)</span>

    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span> <span class="p">)</span>

    <span class="n">f_rows</span><span class="p">,</span> <span class="n">f_cols</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>

    <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span> <span class="n">f_rows</span><span class="p">,</span> <span class="n">f_cols</span><span class="p">,</span> <span class="mi">1</span> <span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span> <span class="n">left</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">bottom</span> <span class="o">=</span> <span class="kc">False</span> <span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">pil_image</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h4 id="method-...-viz_tens_list">
<a class="anchor" href="#method-...-viz_tens_list" aria-hidden="true"><span class="octicon octicon-link"></span></a>method ... <code>viz_tens_list</code><a class="anchor-link" href="#method-...-viz_tens_list"> </a>
</h4>
<p>args</p>
<ul>
<li>
<p><code>list_of_tensors</code></p>
</li>
<li>
<p><code>display_size = 6</code></p>
</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">viz_tens_list</span><span class="p">(</span> <span class="n">list_of_tensors</span><span class="p">,</span> <span class="n">display_size</span> <span class="o">=</span> <span class="mi">6</span> <span class="p">):</span>

    <span class="n">size</span> <span class="o">=</span> <span class="n">display_size</span>

    <span class="n">image_list</span> <span class="o">=</span> <span class="p">[</span> <span class="n">transforms</span><span class="o">.</span><span class="n">ToPILImage</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">"RGB"</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">list_of_tensors</span> <span class="p">]</span>

    <span class="n">tensor_list</span> <span class="o">=</span> <span class="p">[</span> <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()(</span><span class="n">image</span><span class="p">)</span> <span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">image_list</span><span class="p">]</span>

    <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span> <span class="n">tensor_list</span><span class="p">,</span> <span class="n">padding</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">pad_value</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="p">)</span>

    <span class="n">grid_pil</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">ToPILImage</span><span class="p">()(</span><span class="n">grid</span><span class="p">)</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">"RGB"</span><span class="p">)</span>

    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span> <span class="p">)</span>

    <span class="n">f_rows</span><span class="p">,</span> <span class="n">f_cols</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>

    <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span> <span class="n">f_rows</span><span class="p">,</span> <span class="n">f_cols</span><span class="p">,</span> <span class="mi">1</span> <span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span> <span class="n">left</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">bottom</span> <span class="o">=</span> <span class="kc">False</span> <span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">grid_pil</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="...-initializing-tensors">
<a class="anchor" href="#...-initializing-tensors" aria-hidden="true"><span class="octicon octicon-link"></span></a><font color="teal">... initializing tensors</font><a class="anchor-link" href="#...-initializing-tensors"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Tensors are similar to NumPy's <code>ndarrays</code>, except that tensors can run on GPUs or other hardware accelerators. Tensors and NumPy arrays can often share the same underlying memory, eliminating the need to copy data. Tensors are also optimized for automatic differentiation</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here are some ways they can be initialized in torch.</p>
<ol>
<li>
<p>from lists: $\quad$ <code>X_1 = torch.tensor( list_object )</code></p>
</li>
<li>
<p>from a numpy array $\quad$ <code>X_2 = torch.from_numpy( array_object )</code></p>
</li>
<li>
<p>with random or constant values, of a given shape. For example,</p>
<p>a. entries all ones: $\quad$ <code>X_3_a = torch.ones( shape )</code></p>
<p>b. entries all zeros $\quad$ <code>X_3_b = torch.zeros( shape )</code></p>
<p>c. entries are i.i.d. $\text{Unif}(0,1)$ $\quad$ <code>X_3_c = torch.rand( shape )</code></p>
<p>d. entries are i.i.d. standard normal $\quad$ <code>X_3_d = torch.randn( shape )</code></p>
<p>e. from values stored in memory $\quad$ <code>X_3_e = torch.Tensor( shape )</code></p>
<p>f. a list of consecutive integers between $N$ and $M$, inclusive, as tensor object $\quad$ <code>X_3_f = torch.arange(N,M)</code></p>
</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-1.---from-lists">
<a class="anchor" href="#...-1.---from-lists" aria-hidden="true"><span class="octicon octicon-link"></span></a>... 1.   from lists<a class="anchor-link" href="#...-1.---from-lists"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span> <span class="p">[</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span> <span class="p">]</span> <span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">X_1</span><span class="p">,</span> <span class="s2">"X1"</span><span class="p">)</span>
<span class="n">viz_tens</span><span class="p">(</span> <span class="mi">25</span> <span class="o">*</span> <span class="n">X_1</span> <span class="p">)</span> <span class="c1"># the factor there to help distinguish values</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     X1 

    num. dims        1
    num. entries     2
    shape            [2]


┌───┬───┐
│ 2 │ 3 │
└───┴───┘



</pre>
</div>
</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAABGCAYAAAAKCiBIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAA2klEQVR4nO3dsQ0CIBBAUTFOx/KMwRg4gXaGxP9ee80lP1dQMc45D3qetxfgDuGjhI8SPkr4KOGjXt+Ga62/fuvtvW+v8FNzzvFp5uKjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPmr4RrzJxUcJHyV8lPBRwkcJH/UGmigNh5tk3yoAAAAASUVORK5CYII=%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-2.-from-numpy">
<a class="anchor" href="#...-2.-from-numpy" aria-hidden="true"><span class="octicon octicon-link"></span></a>... 2. from numpy<a class="anchor-link" href="#...-2.-from-numpy"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span> <span class="p">[</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span> <span class="p">]</span> <span class="p">)</span> <span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">X_2</span><span class="p">,</span> <span class="s2">"'X two'"</span><span class="p">)</span>
<span class="n">viz_tens</span><span class="p">(</span> <span class="mi">25</span> <span class="o">*</span> <span class="n">X_2</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     'X two' 

    num. dims        1
    num. entries     3
    shape            [3]


┌───┬───┬───┐
│ 2 │ 3 │ 4 │
└───┴───┴───┘



</pre>
</div>
</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAAAzCAYAAABR5bw6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAsklEQVR4nO3csQ0DIRAAQWO5UwqmDMrgG7Cd/ks7k15y0uoCEsY550XP++4FuIfwUcJHCR8lfJTwUZ9/w7XW4956e++7V/jqiXvNOcevmYuPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4a/bJtcfJTwUcJHCR8lfJTwURc1fBBhoRLH+gAAAABJRU5ErkJggg==%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The next examples allow a shape to be provided as an argument.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">shape_dict</span> <span class="o">=</span> <span class="p">{}</span>

<span class="n">shape_dict</span><span class="p">[</span><span class="s2">"i"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span> <span class="mi">3</span> <span class="p">)</span>

<span class="n">shape_dict</span><span class="p">[</span><span class="s2">"ii"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span> <span class="p">)</span>

<span class="n">shape_dict</span><span class="p">[</span><span class="s2">"iii"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-3.-(a)-ones-of-given-shape">
<a class="anchor" href="#...-3.-(a)-ones-of-given-shape" aria-hidden="true"><span class="octicon octicon-link"></span></a>... 3. (a) ones of given shape<a class="anchor-link" href="#...-3.-(a)-ones-of-given-shape"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">shape_dict</span><span class="p">:</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">key</span>
    <span class="n">X_3_a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape_dict</span><span class="p">[</span><span class="n">s</span><span class="p">])</span>
    <span class="n">info</span><span class="p">(</span><span class="n">X_3_a</span><span class="p">,</span> <span class="s2">"'X three (a)'"</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s2">" "</span> <span class="o">+</span> <span class="n">s</span> <span class="o">+</span> <span class="s2">"'"</span> <span class="p">)</span>
    <span class="n">viz_tens</span><span class="p">(</span> <span class="mi">25</span> <span class="o">*</span> <span class="n">X_3_a</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     'X three (a) i' 

    num. dims        1
    num. entries     3
    shape            [3]


┌─────┬─────┬─────┐
│ 1.0 │ 1.0 │ 1.0 │
└─────┴─────┴─────┘



tensor     'X three (a) ii' 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌─────┬─────┬─────┐
│ 1.0 │ 1.0 │ 1.0 │
├─────┼─────┼─────┤
│ 1.0 │ 1.0 │ 1.0 │
└─────┴─────┴─────┘



tensor     'X three (a) iii' 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌─────┬─────┬─────┐
│ 1.0 │ 1.0 │ 1.0 │
├─────┼─────┼─────┤
│ 1.0 │ 1.0 │ 1.0 │
└─────┴─────┴─────┘



</pre>
</div>
</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAAAzCAYAAABR5bw6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAqElEQVR4nO3csQ2AMAwAQYIYIvtPly3CBNAi9Hetm0gvF24y9t4HPefXD+AbwkcJHyV8lPBRwkddb8O1llvvx+ac42lm46OEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svio4S/bJhsfJXyU8FHCRwkfJXzUDUneCmEkcikWAAAAAElFTkSuQmCC%0A">
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAA/UlEQVR4nO3WwQkDMQwAwTikCPdfnbvQVZB8TdiZrz6CRaA1My963rcX4A7ho4SPEj5K+KjPr+E5x8v/x/be69vMxUcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJH7Vm5vYOXODio4SPEj5K+Cjho4SPegDe4Qqr0aiDOwAAAABJRU5ErkJggg==%0A">
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAA/UlEQVR4nO3WwQkDMQwAwTikCPdfnbvQVZB8TdiZrz6CRaA1My963rcX4A7ho4SPEj5K+KjPr+E5x8v/x/be69vMxUcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJH7Vm5vYOXODio4SPEj5K+Cjho4SPegDe4Qqr0aiDOwAAAABJRU5ErkJggg==%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-3.-(b)-zeros-of-given-shape">
<a class="anchor" href="#...-3.-(b)-zeros-of-given-shape" aria-hidden="true"><span class="octicon octicon-link"></span></a>... 3. (b) zeros of given shape<a class="anchor-link" href="#...-3.-(b)-zeros-of-given-shape"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">shape_dict</span><span class="p">:</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">key</span>
    <span class="n">X_3_b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape_dict</span><span class="p">[</span><span class="n">s</span><span class="p">])</span>
    <span class="n">info</span><span class="p">(</span><span class="n">X_3_b</span><span class="p">,</span> <span class="s2">"'X three (a)'"</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s2">" "</span> <span class="o">+</span> <span class="n">s</span> <span class="o">+</span> <span class="s2">"'"</span> <span class="p">)</span>
    <span class="n">viz_tens</span><span class="p">(</span> <span class="mi">25</span> <span class="o">*</span> <span class="n">X_3_b</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     'X three (a) i' 

    num. dims        1
    num. entries     3
    shape            [3]


┌─────┬─────┬─────┐
│ 0.0 │ 0.0 │ 0.0 │
└─────┴─────┴─────┘



tensor     'X three (a) ii' 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌─────┬─────┬─────┐
│ 0.0 │ 0.0 │ 0.0 │
├─────┼─────┼─────┤
│ 0.0 │ 0.0 │ 0.0 │
└─────┴─────┴─────┘



tensor     'X three (a) iii' 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌─────┬─────┬─────┐
│ 0.0 │ 0.0 │ 0.0 │
├─────┼─────┼─────┤
│ 0.0 │ 0.0 │ 0.0 │
└─────┴─────┴─────┘



</pre>
</div>
</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAAAzCAYAAABR5bw6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAo0lEQVR4nO3cMQqAQAwAQSP+/8vxBdqK7Eyb5mBJkeZmdw96zq8fwDeEjxI+Svgo4aOEj7rehjPj1vux3Z2nmY2PEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho8Zftk02Pkr4KOGjhI8SPkr4qBuM0wphyOWPswAAAABJRU5ErkJggg==%0A">
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAA+ElEQVR4nO3WsQnEQAwAwdfj/luWK7DTw+xMqkSwCDS7+6Pnf3oBzhA+Svgo4aOEj7rehjPj5f+w3Z2nmYuPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5qdvf0Dhzg4qOEjxI+Svgo4aOEj7oBIeUKq3KMmaAAAAAASUVORK5CYII=%0A">
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAA+ElEQVR4nO3WsQnEQAwAwdfj/luWK7DTw+xMqkSwCDS7+6Pnf3oBzhA+Svgo4aOEj7rehjPj5f+w3Z2nmYuPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5qdvf0Dhzg4qOEjxI+Svgo4aOEj7oBIeUKq3KMmaAAAAAASUVORK5CYII=%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-3.-(c)-i.i.d.-uniform-of-given-shape">
<a class="anchor" href="#...-3.-(c)-i.i.d.-uniform-of-given-shape" aria-hidden="true"><span class="octicon octicon-link"></span></a>... 3. (c) i.i.d. uniform of given shape<a class="anchor-link" href="#...-3.-(c)-i.i.d.-uniform-of-given-shape"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">shape_dict</span><span class="p">:</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">key</span>
    <span class="n">X_3_c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">shape_dict</span><span class="p">[</span><span class="n">s</span><span class="p">])</span>
    <span class="n">info</span><span class="p">(</span><span class="n">X_3_c</span><span class="p">,</span> <span class="s2">"X 3 (c) "</span> <span class="o">+</span> <span class="s2">" "</span> <span class="o">+</span> <span class="n">s</span> <span class="p">)</span>
    <span class="n">viz_tens</span><span class="p">(</span> <span class="mi">25</span> <span class="o">*</span> <span class="n">X_3_c</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     X 3 (c)  i 

    num. dims        1
    num. entries     3
    shape            [3]


┌──────┬──────┬──────┐
│ 0.88 │ 0.91 │ 0.38 │
└──────┴──────┴──────┘



tensor     X 3 (c)  ii 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌──────┬──────┬──────┐
│ 0.95 │ 0.39 │ 0.60 │
├──────┼──────┼──────┤
│ 0.25 │ 0.79 │ 0.94 │
└──────┴──────┴──────┘



tensor     X 3 (c)  iii 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌──────┬──────┬──────┐
│ 0.13 │ 0.93 │ 0.59 │
├──────┼──────┼──────┤
│ 0.86 │ 0.56 │ 0.74 │
└──────┴──────┴──────┘



</pre>
</div>
</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAAAzCAYAAABR5bw6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAt0lEQVR4nO3cuQ3DMBAAQdNwzyxPNagcRXQBflIK2Jn0kgMWFzDhWGs96HnuXoA9hI8SPkr4KOGjhI96/Rte13W7t955nrtX+Oo4jt0rfJhzjl8zFx8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHDX/ZNrn4KOGjhI8SPkr4KOGj3n9EEGE9itKdAAAAAElFTkSuQmCC%0A">
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABLElEQVR4nO3csW2EQBBAUWM5oSh6uYCKKIOCqIA+cGzdHeme/N9LNxnpa6SNZrqu64ue79EDMIbwUcJHCR8lfNTP3eN5nh/35T+OY/QILy3LMnqEJ/M8T+/ebHyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfNd2dNF3X9eOOHz0ej9EjvLTv++gRnmzb5vgRfwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCR91eveL/svFRwkcJHyV8lPBRwkf9AphsF6frkzUoAAAAAElFTkSuQmCC%0A">
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABK0lEQVR4nO3cu2nFQBRFUcu4J/UhUDlKVI/aUzwu4H14kcew10onObC5oEjLGOOLnu/ZA5hD+Cjho4SPEj7q593jtm3/7pP/vu/ZE546z3P2hAfrui6v3lx8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJH/X250f7vv/Vjo8dxzF7wlPXdc2e8GBd15dvLj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5qGWPM3sAELj5K+Cjho4SPEj5K+KhfR1ASzMQA6osAAAAASUVORK5CYII=%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-3.-(d)-i.i.d.-standard-normal-of-given-shape">
<a class="anchor" href="#...-3.-(d)-i.i.d.-standard-normal-of-given-shape" aria-hidden="true"><span class="octicon octicon-link"></span></a>... 3. (d) i.i.d. standard normal of given shape<a class="anchor-link" href="#...-3.-(d)-i.i.d.-standard-normal-of-given-shape"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">shape_dict</span><span class="p">:</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">key</span>
    <span class="n">X_3_d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape_dict</span><span class="p">[</span><span class="n">s</span><span class="p">])</span>
    <span class="n">info</span><span class="p">(</span><span class="n">X_3_d</span><span class="p">,</span> <span class="s2">"X 3 (d)"</span> <span class="o">+</span> <span class="s2">" "</span> <span class="o">+</span> <span class="n">s</span> <span class="p">)</span>
    <span class="n">viz_tens</span><span class="p">(</span> <span class="mi">25</span> <span class="o">*</span> <span class="n">X_3_d</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     X 3 (d) i 

    num. dims        1
    num. entries     3
    shape            [3]


┌──────┬──────┬──────┐
│ -1.2 │ 0.52 │ 1.22 │
└──────┴──────┴──────┘



tensor     X 3 (d) ii 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌──────┬──────┬──────┐
│ 0.15 │ -0.3 │ -0.4 │
├──────┼──────┼──────┤
│ -0.2 │ -0.1 │ -1.1 │
└──────┴──────┴──────┘



tensor     X 3 (d) iii 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌──────┬──────┬──────┐
│ -0.8 │ -0.7 │ 0.12 │
├──────┼──────┼──────┤
│ -0.1 │ -2.2 │ -0.6 │
└──────┴──────┴──────┘



</pre>
</div>
</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAAAzCAYAAABR5bw6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAuElEQVR4nO3cuQ3DMBAAQdNwVSxJDaki1UUX4CelgJ1JLzlgcQETjrXWg57n7gXYQ/go4aOEjxI+Svio17/hcRy3e+td17V7ha/mnLtX+HCe5/g1c/FRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV81PCXbZOLjxI+Svgo4aOEjxI+6g2mew1hx7zLoQAAAABJRU5ErkJggg==%0A">
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABKUlEQVR4nO3csW2EQBRFUWNtFXRGTIX0QDO0QDIuwOuVN/FYuuekkzzp6ktELGOMD3o+Zw9gDuGjhI8SPkr4qMerx/M8/90n/33fsyc8tW3b7AnfXNe1/PTm4qOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Sviolz8/Oo7jr3b82rqusyc8te/77AlvcfFRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRyxhj9gYmcPFRwkcJHyV8lPBRwkd9AbVVFDcCSemUAAAAAElFTkSuQmCC%0A">
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABJ0lEQVR4nO3csW2EQBRFUWNtB6R0Q/9QCwWgcQFer7yJx9I9J53kSVdfImIZY3zQ8zl7AHMIHyV8lPBRwkc9Xj1e1/XvPvn3fZ894an7vmdP+OY8z+WnNxcfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCR738+dG6rn+149eO45g94alt22ZPeIuLjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEj1rGGLM3MIGLjxI+Svgo4aOEjxI+6gsRMxOkcly2jgAAAABJRU5ErkJggg==%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-3.-(f)-sequence">
<a class="anchor" href="#...-3.-(f)-sequence" aria-hidden="true"><span class="octicon octicon-link"></span></a>... 3. (f) sequence<a class="anchor-link" href="#...-3.-(f)-sequence"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_3_f</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">)</span> 
<span class="n">info</span><span class="p">(</span><span class="n">X_3_f</span><span class="p">,</span> <span class="s2">"X 3 (f)"</span><span class="p">)</span>
<span class="n">viz_tens</span><span class="p">(</span> <span class="mi">25</span> <span class="o">*</span> <span class="n">X_3_f</span> <span class="p">)</span> <span class="c1"># note the modular arith. being performed automatically</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     X 3 (f) 

    num. dims        1
    num. entries     10
    shape            [10]


┌────┬────┬────┬────┬────┬────┬────┬────┬────┬────┐
│ 10 │ 11 │ 12 │ 13 │ 14 │ 15 │ 16 │ 17 │ 18 │ 19 │
└────┴────┴────┴────┴────┴────┴────┴────┴────┴────┘



</pre>
</div>
</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAAAZCAYAAAD30ppqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAf0lEQVRoge3awQnAIBAAwVwIXP/t2cCVcakgwTzEwO58FRUWH4LR3Yd4zt0H0B6GhzI8lOGhDA9leKjrbTAzp996VTW96Ze5K9f+w9yVa48x4mnMGw9leCjDQxkeyvBQhocyPJThoQwPZXio8AcOkzceyvBQhocyPJThoQwPdQM4ZCIt7yfBxgAAAABJRU5ErkJggg==%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="numpy-objects-and-tensors">
<a class="anchor" href="#numpy-objects-and-tensors" aria-hidden="true"><span class="octicon octicon-link"></span></a><font color="teal">numpy objects and tensors</font><a class="anchor-link" href="#numpy-objects-and-tensors"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Tensors can be converted to numpy arrays, and numpy arrays back to tensors.
To transform a numpy array into a tensor, we can use the function <code>torch.from_numpy</code>, and we use <code>np.array</code> for the other direction.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The conversion of tensors to numpy require the tensor to be on the CPU, and not the GPU.</p>
<p>In case you have a tensor on GPU, you need to call <code>.cpu()</code> on the tensor beforehand.
Hence, you get a line like <code>np_arr = tensor.cpu().numpy()</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"t: </span><span class="si">{</span><span class="n">t</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"n: </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>t: tensor([1., 1., 1., 1., 1.])
n: [1. 1. 1. 1. 1.]
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">t</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"t: </span><span class="si">{</span><span class="n">t</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"n: </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>t: tensor([2., 2., 2., 2., 2.])
n: [2. 2. 2. 2. 2.]
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="...-tensor-operations">
<a class="anchor" href="#...-tensor-operations" aria-hidden="true"><span class="octicon octicon-link"></span></a><font color="teal">... tensor operations</font><a class="anchor-link" href="#...-tensor-operations"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Most operations existing in numpy also exist in PyTorch. A full list of operations can be found in the <a href="https://pytorch.org/docs/stable/tensors.html#">PyTorch documentation</a>.</p>
<ul>
<li>
<p>Each torch operation can be run on the GPU.</p>
</li>
<li>
<p>By default, tensors are created on the CPU. Unless we are using a package like <code>pytorch-lightning</code>, we need to explicitly move tensors to the GPU using the <code>.to</code> method, after checking GPU availability.</p>
</li>
<li>
<p>Copying large tensors across devices can be expensive in terms of time and memory.</p>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-adding-tensors">
<a class="anchor" href="#...-adding-tensors" aria-hidden="true"><span class="octicon octicon-link"></span></a>... adding tensors<a class="anchor-link" href="#...-adding-tensors"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_1</span><span class="p">,</span> <span class="n">X_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">viz_tens_list</span><span class="p">(</span> <span class="p">[</span> <span class="n">X_1</span><span class="p">,</span> <span class="n">X_2</span> <span class="p">]</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAV0AAAC1CAYAAAD86CzsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAC+klEQVR4nO3asW3CYBhF0TjyAMzBKog1aKiYhmWQ2IDeI1hiB7NAAKW5jsI57Vc8V1d/4WFZli8AGt9rfwDAJxFdgJDoAoREFyAkugCh8c3drw0Avzc8O3jpAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBrX/oA1nc/ndG+73aZ7p9Mp3bvdbuleaZ7ndO9yuaR7m80m3dvtduneX+KlCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKExrU/YE3H4zHd2+/36d79fk/3/rPr9ZruTdOU7h0Oh3Tvk3npAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugChYVmWV/eXRwB+NDw7eOkChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQGt/ch+QrAD6Ely5ASHQBQqILEBJdgJDoAoREFyD0AKc9IFJT4QG+AAAAAElFTkSuQmCC%0A">
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Y</span> <span class="o">=</span> <span class="n">X_1</span> <span class="o">+</span> <span class="n">X_2</span>
<span class="n">viz_tens</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> 
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABKklEQVR4nO3cwWnDQBBA0Si4CakF1aKaBCpWbSgF2BY5ZU3+e9e9DHwG9jTTdV1f9HyPHoAxhI8SPkr4KOGjHneP+75/3Jf/PM/RI7x0HMfoEZ7M8zy9e7PxUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfNTt8aN1Xf9qjl/btm30CC8tyzJ6hCd352ptfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfNR0dxmJ/8vGRwkfJXyU8FHCRwkf9QPQnxJgHxHSmQAAAABJRU5ErkJggg==%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-stacking-tensors">
<a class="anchor" href="#...-stacking-tensors" aria-hidden="true"><span class="octicon octicon-link"></span></a>... stacking tensors<a class="anchor-link" href="#...-stacking-tensors"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_1</span><span class="p">,</span> <span class="n">X_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">15</span><span class="p">)</span>

<span class="n">info</span><span class="p">(</span><span class="n">X_1</span><span class="p">,</span> <span class="s2">"X 1"</span><span class="p">)</span>

<span class="n">info</span><span class="p">(</span><span class="n">X_2</span><span class="p">,</span> <span class="s2">"X 2"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     X 1 

    num. dims        1
    num. entries     5
    shape            [5]


┌───┬───┬───┬───┬───┐
│ 5 │ 6 │ 7 │ 8 │ 9 │
└───┴───┴───┴───┴───┘



tensor     X 2 

    num. dims        1
    num. entries     5
    shape            [5]


┌────┬────┬────┬────┬────┐
│ 10 │ 11 │ 12 │ 13 │ 14 │
└────┴────┴────┴────┴────┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">X_1</span><span class="p">,</span> <span class="n">X_2</span><span class="p">],</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="s2">"Y"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     Y 

    num. dims        2
    num. entries     10
    shape            [2 5]


┌────┬────┬────┬────┬────┐
│ 5  │ 6  │ 7  │ 8  │ 9  │
├────┼────┼────┼────┼────┤
│ 10 │ 11 │ 12 │ 13 │ 14 │
└────┴────┴────┴────┴────┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-in-place-operations">
<a class="anchor" href="#...-in-place-operations" aria-hidden="true"><span class="octicon octicon-link"></span></a>... in-place operations<a class="anchor-link" href="#...-in-place-operations"> </a>
</h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Operations that store the result into the operand are called <em>in-place</em>. They are  usually marked with a underscore postfix, e.g. "<code>add_</code>" instead of "<code>add</code>". The operation <code>X.copy_(Y)</code> will change <code>X</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Calling <code>x1 + x2</code> creates a new tensor containing the sum of the two inputs.
However, we can also use in-place operations that are applied directly on the memory of a tensor.
We therefore change the values of <code>x2</code> without the chance to re-accessing the values of <code>x2</code> before the operation.
An example is shown below:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_1</span><span class="p">,</span> <span class="n">X_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\t</span><span class="s2">"</span><span class="p">,</span><span class="s2">"before"</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">X_1</span><span class="p">,</span> <span class="s2">"'X one'"</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">X_2</span><span class="p">,</span> <span class="s2">"'X two'"</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n\n</span><span class="s2">"</span><span class="p">)</span>

<span class="n">X_2</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">X_1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\t</span><span class="s2">"</span><span class="p">,</span><span class="s2">"after"</span><span class="p">,</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">X_1</span><span class="p">,</span> <span class="s2">"'X one'"</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">X_2</span><span class="p">,</span> <span class="s2">"'X two'"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>	 before 

tensor     'X one' 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌──────┬──────┬──────┐
│ 0.14 │ 0.53 │ 0.15 │
├──────┼──────┼──────┤
│ 0.65 │ 0.32 │ 0.65 │
└──────┴──────┴──────┘



tensor     'X two' 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌──────┬──────┬──────┐
│ 0.39 │ 0.91 │ 0.20 │
├──────┼──────┼──────┤
│ 0.20 │ 0.20 │ 0.94 │
└──────┴──────┴──────┘






	 after 

tensor     'X one' 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌──────┬──────┬──────┐
│ 0.14 │ 0.53 │ 0.15 │
├──────┼──────┼──────┤
│ 0.65 │ 0.32 │ 0.65 │
└──────┴──────┴──────┘



tensor     'X two' 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌──────┬──────┬──────┐
│ 0.54 │ 1.44 │ 0.36 │
├──────┼──────┼──────┤
│ 0.85 │ 0.52 │ 1.60 │
└──────┴──────┴──────┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>In-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss of history. Hence, their use is discouraged.</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-reshaping-tensors">
<a class="anchor" href="#...-reshaping-tensors" aria-hidden="true"><span class="octicon octicon-link"></span></a>... reshaping tensors<a class="anchor-link" href="#...-reshaping-tensors"> </a>
</h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Another common operation aims at changing the shape of a tensor.
A tensor of size <code>(2,3)</code> can be re-organized to any other shape with the same number of elements (e.g. a tensor of size <code>(6)</code>, or <code>(3,2)</code>, ...).</p>
<p>In PyTorch, this reshaping operation is called <code>view</code>:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="s2">"X"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     X 

    num. dims        1
    num. entries     6
    shape            [6]


┌───┬───┬───┬───┬───┬───┐
│ 0 │ 1 │ 2 │ 3 │ 4 │ 5 │
└───┴───┴───┴───┴───┴───┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="s2">"X"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     X 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌───┬───┬───┐
│ 0 │ 1 │ 2 │
├───┼───┼───┤
│ 3 │ 4 │ 5 │
└───┴───┴───┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-transposing-tensors-(permuting-dimensions)">
<a class="anchor" href="#...-transposing-tensors-(permuting-dimensions)" aria-hidden="true"><span class="octicon octicon-link"></span></a>... transposing tensors (permuting dimensions)<a class="anchor-link" href="#...-transposing-tensors-(permuting-dimensions)"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="s2">"X with 0th and 1st dimensions permuted"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     X with 0th and 1st dimensions permuted 

    num. dims        2
    num. entries     6
    shape            [3 2]


┌───┬───┐
│ 0 │ 3 │
├───┼───┤
│ 1 │ 4 │
├───┼───┤
│ 2 │ 5 │
└───┴───┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span>
<span class="n">info</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="s2">"X transposed again"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     X transposed again 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌───┬───┬───┐
│ 0 │ 1 │ 2 │
├───┼───┼───┤
│ 3 │ 4 │ 5 │
└───┴───┴───┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-numpy-like-indexing-and-slicing">
<a class="anchor" href="#...-numpy-like-indexing-and-slicing" aria-hidden="true"><span class="octicon octicon-link"></span></a>... numpy-like indexing and slicing<a class="anchor-link" href="#...-numpy-like-indexing-and-slicing"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span> <span class="mi">4</span><span class="p">,</span><span class="mi">4</span> <span class="p">)</span>
<span class="n">info</span><span class="p">(</span> <span class="n">X</span><span class="p">,</span> <span class="s2">"X"</span> <span class="p">)</span>
<span class="n">info</span><span class="p">(</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">"first row of X"</span> <span class="p">)</span>
<span class="n">info</span><span class="p">(</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="s2">"first column of X"</span> <span class="p">)</span>
<span class="n">info</span><span class="p">(</span> <span class="n">X</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s2">"last column of X"</span> <span class="p">)</span>
<span class="n">info</span><span class="p">(</span> <span class="n">X</span><span class="p">[:</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s2">"First two rows, last column"</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span> <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">,</span> <span class="p">:],</span> <span class="s2">"Middle two rows"</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     X 

    num. dims        2
    num. entries     16
    shape            [4 4]


┌──────┬──────┬──────┬──────┐
│ 0.66 │ 0.98 │ 0.08 │ 0.00 │
├──────┼──────┼──────┼──────┤
│ 0.10 │ 0.16 │ 0.70 │ 0.67 │
├──────┼──────┼──────┼──────┤
│ 0.91 │ 0.24 │ 0.15 │ 0.76 │
├──────┼──────┼──────┼──────┤
│ 0.29 │ 0.80 │ 0.38 │ 0.78 │
└──────┴──────┴──────┴──────┘



tensor     first row of X 

    num. dims        1
    num. entries     4
    shape            [4]


┌──────┬──────┬──────┬──────┐
│ 0.66 │ 0.98 │ 0.08 │ 0.00 │
└──────┴──────┴──────┴──────┘



tensor     first column of X 

    num. dims        1
    num. entries     4
    shape            [4]


┌──────┬──────┬──────┬──────┐
│ 0.66 │ 0.10 │ 0.91 │ 0.29 │
└──────┴──────┴──────┴──────┘



tensor     last column of X 

    num. dims        1
    num. entries     4
    shape            [4]


┌──────┬──────┬──────┬──────┐
│ 0.00 │ 0.67 │ 0.76 │ 0.78 │
└──────┴──────┴──────┴──────┘



tensor     First two rows, last column 

    num. dims        1
    num. entries     2
    shape            [2]


┌──────┬──────┐
│ 0.00 │ 0.67 │
└──────┴──────┘



tensor     Middle two rows 

    num. dims        2
    num. entries     8
    shape            [2 4]


┌──────┬──────┬──────┬──────┐
│ 0.10 │ 0.16 │ 0.70 │ 0.67 │
├──────┼──────┼──────┼──────┤
│ 0.91 │ 0.24 │ 0.15 │ 0.76 │
└──────┴──────┴──────┴──────┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">info</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="s2">"modified X"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     modified X 

    num. dims        2
    num. entries     16
    shape            [4 4]


┌──────┬─────┬──────┬──────┐
│ 0.66 │ 0.0 │ 0.08 │ 0.00 │
├──────┼─────┼──────┼──────┤
│ 0.10 │ 0.0 │ 0.70 │ 0.67 │
├──────┼─────┼──────┼──────┤
│ 0.91 │ 0.0 │ 0.15 │ 0.76 │
├──────┼─────┼──────┼──────┤
│ 0.29 │ 0.0 │ 0.38 │ 0.78 │
└──────┴─────┴──────┴──────┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-other-operations">
<a class="anchor" href="#...-other-operations" aria-hidden="true"><span class="octicon octicon-link"></span></a>... other operations<a class="anchor-link" href="#...-other-operations"> </a>
</h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here are some ways to perform matrix multiplication:</p>
<ul>
<li>
<p><code>torch.matmul</code> $\quad$ Performs the matrix product over two tensors, where the specific behavior depends on the dimensions.
If both inputs are matrices (2-dimensional tensors), it performs the standard matrix product.
For higher dimensional inputs, the function supports broadcasting (for details see the <a href="https://pytorch.org/docs/stable/generated/torch.matmul.html?highlight=matmul#torch.matmul">documentation</a>).</p>
<p>It can also be written as <code>a @ b</code>, similar to numpy.</p>
</li>
<li>
<p><code>torch.mm</code> $\quad$ Performs the matrix product over two matrices, but doesn't support broadcasting (see <a href="https://pytorch.org/docs/stable/generated/torch.mm.html?highlight=torch%20mm#torch.mm">documentation</a>)</p>
</li>
<li>
<p><code>torch.bmm</code> $\quad$ Performs the matrix product with a support batch dimension. Let <code>T</code> be a tensor of shape <code>(b,  n, m)</code>, and <code>R</code> a tensor of shape <code>(b, m, p)</code>, the output tensor is of shape <code>(b, n , p)</code>, obtained by "entry-wise" matrix multiplication along the batch dimension.</p>
</li>
<li>
<p><code>torch.einsum</code> $\quad$ Performs matrix multiplications and more (i.e. sums of products) using the Einstein summation convention.</p>
</li>
</ul>
<p>Usually, we use <code>torch.matmul</code> or <code>torch.bmm</code>.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">9</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="n">info</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="s2">"X"</span><span class="p">)</span>

<span class="n">info</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="s2">"Y"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     X 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌───┬───┬───┐
│ 0 │ 1 │ 2 │
├───┼───┼───┤
│ 3 │ 4 │ 5 │
└───┴───┴───┘



tensor     Y 

    num. dims        2
    num. entries     9
    shape            [3 3]


┌───┬───┬───┐
│ 0 │ 1 │ 2 │
├───┼───┼───┤
│ 3 │ 4 │ 5 │
├───┼───┼───┤
│ 6 │ 7 │ 8 │
└───┴───┴───┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="s2">"Z"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     Z 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌────┬────┬────┐
│ 15 │ 18 │ 21 │
├────┼────┼────┤
│ 42 │ 54 │ 66 │
└────┴────┴────┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Given a tensor <code>X</code>, the tensors <code>Y_1</code>,<code>Y_2</code>, <code>Y_3</code> computed below all have the same value:</p>

<pre><code>Y_1 = X @ X.T
Y_2 = X.matmul(X.T)
Y_3 = torch.rand_like(X)
torch.matmul(X, X.T, out = Y_3)</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>On the other hand, <code>*</code> denotes the entrywise product of two tensors.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="s2">"X"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     X 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌───┬───┬───┐
│ 0 │ 1 │ 2 │
├───┼───┼───┤
│ 3 │ 4 │ 5 │
└───┴───┴───┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">info</span><span class="p">(</span> <span class="n">X</span> <span class="o">*</span> <span class="n">X</span><span class="p">,</span> <span class="s2">"(a)"</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span> <span class="n">X</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="s2">"(b)"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     (a) 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌───┬────┬────┐
│ 0 │ 1  │ 4  │
├───┼────┼────┤
│ 9 │ 16 │ 25 │
└───┴────┴────┘



tensor     (b) 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌───┬────┬────┐
│ 0 │ 1  │ 4  │
├───┼────┼────┤
│ 9 │ 16 │ 25 │
└───┴────┴────┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You can use <code>torch.cat</code> to concatenate a sequence of tensors along a given dimension. See also <code>torch.stack</code>, another tensor joining op that is subtly different from <code>torch.cat</code></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span> <span class="p">[</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">],</span> <span class="n">dim</span> <span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="s2">"Y"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     Y 

    num. dims        2
    num. entries     18
    shape            [2 9]


┌───┬───┬───┬───┬───┬───┬───┬───┬───┐
│ 0 │ 1 │ 2 │ 0 │ 1 │ 2 │ 0 │ 1 │ 2 │
├───┼───┼───┼───┼───┼───┼───┼───┼───┤
│ 3 │ 4 │ 5 │ 3 │ 4 │ 5 │ 3 │ 4 │ 5 │
└───┴───┴───┴───┴───┴───┴───┴───┴───┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If you have a one-element tensor, for example obtained by aggregating all values of a given tensor into a single value, you can convert it to a Python numerical value using <code>item()</code>:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">agg</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">agg_item</span> <span class="o">=</span> <span class="n">agg</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> 
<span class="nb">print</span><span class="p">(</span><span class="n">agg_item</span><span class="p">,</span> <span class="s2">"</span><span class="se">\t</span><span class="s2">"</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">agg_item</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>15 	 &lt;class 'int'&gt;
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="2.2-...-one-dimensional-convolutions-of-scalar-valued-signals">
<a class="anchor" href="#2.2-...-one-dimensional-convolutions-of-scalar-valued-signals" aria-hidden="true"><span class="octicon octicon-link"></span></a><font color="CornflowerBlue">2.2 ... one-dimensional convolutions of scalar-valued signals</font><a class="anchor-link" href="#2.2-...-one-dimensional-convolutions-of-scalar-valued-signals"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We now consider one of the simplest possible settings for learning, in which the underlying domain is a one-dimensional grid, and where signals over the domain have only a single channel.</p>
<p>In this case, the signal domain is a group itself, the cyclic group of order $n$, 
$$
C_n = \langle \, a : a^n = 1 \, \rangle \equiv \{ \, 1, a, a^2, \dots, a^{n-1} \, \}.
$$
It is convenient to parametrize the group, and hence the grid, through the exponent of the generator 
$$
C_n \equiv \{ 0, 1, \dots, n -1 \}
$$
as this indexing is consistent with the way most python indexes vectors. In this setting, the group operation may be reinterpreted as addition modulo $n$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As the input domain is fixed, it feels natural to consider the GDL group $G$ to be $C_n$ as well.</p>
<p>We suppose that signals are scalar-valued, and are encoded in the natural basis, so that each $x \in \mathcal{X}(C_n, \mathbb{R})$ may be expressed as</p>
$$
\mathcal{X}(C_n,\mathbb{R}) = \{ x : C_n \to \mathbb{R} \} ,
$$<p>is finite dimensional, and each $x \in \mathcal{X}(C_n, \mathbb{R})$ may be expressed as 
$$
x = 
\left[ 
\begin{matrix}
x_0\\ 
\vdots\\
\,x_{n-1}\,
\end{matrix}
\right] 
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>With this basis, we can describe the representation $\rho$ of $G \equiv C_n$ concretely, as a matrix.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Given a vector $\theta = (\theta_0 , \dots, \theta_{n-1})$, recall the associated <font color="purple">_circulant matrix_</font> is the $n \times n$ matrix with entries 
$$
S(\theta) := \left( \, \theta_{ (u - v) \mod n} \right)_{ 0 \, \leq \,u,\,v \, \leq n-1 } 
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the specific case of $\theta_{+} := (0,1,0,\dots, 0)^T$, the associated circulant matrix, $S_{+} := S(\theta_{+})$ acts on vectors by shifting the entries of vectors to the right by one position, corresponding to addition by one, modulo $n$.</p>
<p>We call $S_+$ the <em><font color="purple">(right) shift operator</font></em>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>Lemma</strong> $\quad$
A matrix is circulant if and only if it commutes with $S_+$. Moreover, given any two vectors $\theta, \eta \in \mathbb{R}^n$, one has $S(\theta) S(\eta) = S(\eta) S(\theta)$.</p>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The importance of $S_+$ to the present discussion is that it generates a group isomorphic to the one-dimensional translation group $C_n$; the matrices $\{ I, S_+, S_+^2, \dots, S_+^{n-1} \}$ constitute a faithful representation of $C_n$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Circulant matrices are synonymous with discrete convolutions; given $x, \theta \in \mathcal{X}(\Omega,\mathbb{R}) \equiv \mathbb{R}^n$, their <em>convolution</em> $x \star \theta$ is defined by 
$$
( x \star \theta )_u := \sum_{v = 0}^{n-1} x_{v \mod n}\, \theta_{ (u-v) \mod n} \equiv S(\theta) x 
$$</p>
<p>Thus, the next corollary effectively follows from the much stronger theorem stated at the end of <a href="https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2021/10/21/GDL1.html#1.3-...-equivariance-in-neural-networks">section 1.3</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>Corollary</strong> $\quad$ Any $f : \mathcal{X}(C_n, \mathbb{R}) \to \mathcal{X}(C_n,\mathbb{R})$ which is linear and $C_n$-equivariant can be expressed (in the input coordinate system) as an $n \times n$ circulant matrix $S(\theta)$ for some vector $\theta$.</p>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<hr>
<h3 id="...example:-local-averaging-as-a-circulant-matrix">
<a class="anchor" href="#...example:-local-averaging-as-a-circulant-matrix" aria-hidden="true"><span class="octicon octicon-link"></span></a><font color="teal">...example: local averaging as a circulant matrix</font><a class="anchor-link" href="#...example:-local-averaging-as-a-circulant-matrix"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>$\quad$ Recall a <a href="https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2021/10/21/GDL1.html#...-example:-permutations-and-local-averaging">previous recipe</a> for an equivariant function $F= \Phi( X, A)$ using a local aggregation function $\varphi$.</p>
<p>In our present case of $\Omega \equiv G \equiv C_n$, we may write this local aggregation more concretely as
$$
\varphi ( x_u, X_{\textsf{nbhd}(u)} ) = \varphi( x_{u-1}, \, x_u, \, x_{u+1} ),
$$
with addition and subtraction in the indices above understood to be modulo $n$.</p>
<p>If in addition, we insist that $\varphi$ is linear, then it has the form 
$$
 \varphi( x_{u-1}, \, x_u, \, x_{u+1} ) = \theta_{-1} x_{u-1} + \theta_0 x_u + \theta_1 x_{u+1},
$$
and in this case we can express $F = \Phi (X, A )$ through the following matrix multiplication:
$$
\left[
\begin{matrix}
\theta_0 &amp; \theta_1 &amp; \text{ } &amp; \text{ } &amp; \theta_{-1} \\
\theta_{-1} &amp; \theta_0 &amp; \theta_1 &amp; \text{ } &amp;   \text{ } \\
\text{} &amp; \ddots &amp; \ddots &amp; \ddots &amp; \text{ } \\
\text{ } &amp; \text{ } &amp; \theta_{-1} &amp; \theta_0 &amp; \theta_1 \\
\theta_1 &amp; \text{ } &amp; \text{ } &amp; \theta_{-1} &amp; \theta_0 
\end{matrix} 
\right]
\left[
\begin{matrix}
x_0 \\
x_1 \\
\vdots \\
\,x_{n-2} \, \\
x_{n-1}  
\end{matrix}
\right]
$$
This multi-diagonal structure is often synonymous with the concept of weight sharing in ML literature.</p>
<hr>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="2.3-...-one-dimensional-convolutions-in-torch">
<a class="anchor" href="#2.3-...-one-dimensional-convolutions-in-torch" aria-hidden="true"><span class="octicon octicon-link"></span></a><font color="CornflowerBlue">2.3 ... one-dimensional convolutions in torch</font><a class="anchor-link" href="#2.3-...-one-dimensional-convolutions-in-torch"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The object in <code>torch</code> for executing convolutions of signals over $C_n$ is called <code>torch.nn.Conv1d</code>. We'll denote an instance of this object by $\tilde{B}$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h4 id="class-...-torch.nn.Conv1d">
<a class="anchor" href="#class-...-torch.nn.Conv1d" aria-hidden="true"><span class="octicon octicon-link"></span></a>class ... <code>torch.nn.Conv1d</code><a class="anchor-link" href="#class-...-torch.nn.Conv1d"> </a>
</h4>
<table>
<thead>
<tr>
<th>args</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>in_channels</code></td>
<td><code>out_channels</code></td>
<td><code>kernel_size</code></td>
<td><code>stride = 1</code></td>
</tr>
<tr>
<td><code>padding = 0</code></td>
<td><code>dilation = 1</code></td>
<td><code>groups = 1</code></td>
<td><code>bias = True</code></td>
</tr>
<tr>
<td><code>padding_mode = 'zeros'</code></td>
<td><code>device = None</code></td>
<td><code>dtype = None</code></td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let us now relate the shapes of the input and output to the parameters</p>
<table>
<thead>
<tr>
<th>input parameter</th>
<th>LaTeX symbol</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>in_channels</code></td>
<td>$\text{dim}(\mathcal{C})$</td>
</tr>
<tr>
<td><code>out_channels</code></td>
<td>$\text{dim}(\tilde{\mathcal{C}})$</td>
</tr>
<tr>
<td><code>kernel_size</code></td>
<td>$k$</td>
</tr>
<tr>
<td><code>stride</code></td>
<td>$\lambda$</td>
</tr>
<tr>
<td><code>padding</code></td>
<td>$\rho$</td>
</tr>
<tr>
<td><code>dilation</code></td>
<td>$\delta$</td>
</tr>
<tr>
<td><code>groups</code></td>
<td>$M$        </td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This class specifies a <a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html">one-dimensional convolution operation</a>.  Because of batching and the channel dimension, the input and output are $3$-tensors. We use $N$ to denote the batch size, and we will continue to use $n$ for the input length.</p>
<p>In general, $\mathcal{C}$ and $\mathcal{C}_1$ are the vector spaces of input channels and output channels respectively. In our current setting, assuming that signals $x$ are scalar signals over the cyclic group, one has $\textrm{dim}(\mathcal{C}) = 1$. The shape of the input tensor is thus</p>
$$
(N, 1, n)
$$<p>Even though we restrict ourselves to scalar-valued input signals, we will allow $\textrm{dim}(\tilde{\mathcal{C}}) &gt; 1$, and we will write $\tilde{n}$ to denote the length of the output, so that the shape of the output tensor is</p>
$$
(N, \textrm{dim}(\tilde{\mathcal{C}}), \tilde{n})
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For the moment, we will also take $N=1$. Thus, inputs can be though of effectively as vectors, which we can easily visualize. We will think of the output as a length-$\textrm{dim}(\tilde{\mathcal{C}})$ list of vectors (with scalar entries).</p>
<p>Our present goal is to try to understand how the number of learnable parameters depends on these shapes, as well as to visualize the effect of a convolutional layer in this simplest setting.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First, we remark that the relationship between $n_1$ and $n$, in terms of the input parameters, can be expressed as follows:</p>
$$
\tilde{n} = \left\lfloor \frac{ n + 2 \rho - \delta (k-1) -1 }{\lambda} + 1 \right\rfloor
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h3 id="method-...-compute_out_size">
<a class="anchor" href="#method-...-compute_out_size" aria-hidden="true"><span class="octicon octicon-link"></span></a>method ... <code>compute_out_size</code><a class="anchor-link" href="#method-...-compute_out_size"> </a>
</h3>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">compute_out_size</span><span class="p">(</span><span class="n">in_size</span><span class="p">,</span>
                     <span class="n">padding</span><span class="p">,</span>
                     <span class="n">dilation</span><span class="p">,</span>
                     <span class="n">kernel_size</span><span class="p">,</span>
                     <span class="n">stride</span><span class="p">):</span>
    <span class="n">numerator</span> <span class="o">=</span> <span class="n">in_size</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">padding</span> <span class="o">-</span> <span class="n">dilation</span> <span class="o">*</span> <span class="p">(</span> <span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

    <span class="n">arg_of_floor</span> <span class="o">=</span> <span class="p">(</span><span class="n">numerator</span> <span class="o">/</span> <span class="n">stride</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span> <span class="n">arg_of_floor</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<hr>
<h3>
<a class="anchor" href="#-example-concrete-layerh3divdivdivfor-now-we-will-instantiate-a-parameters-dictionary-with-dilation--1-and-padding--0-let-us-use-n--32-so-thattilden--leftlfloor-fracn-klambda--1-rightrfloor-equiv-leftlfloor-frac32-klambda--1-rightrfloorfor-our-example-we-specialize-to-kernel_size--4-and-stride--2-in-which-case-tilden--15------------odcp----one-dimensional-conv-parametersodcpin_size--32odcpin_channels--1odcppadding--0odcpdilation--1odcpkernel_size--4odcpstride--2odcpgroups--1odcpout_channels--16----------------------------------odcpout_size--compute_out_size----in_size--odcpin_size----padding--odcppadding----dilation--odcpdilation----kernel_size--odcpkernel_size----stride--odcpstride----print-odcpout_size---------150----one-of-the-important-free-parameters-we-can-choose-to-initialize-a-one-dimensional-convolutional-layer-is-the-number-of-out_channels-we-choose-out_channels--16-in-our-first-examplein-many-vision-related-cnns-the-first-layer-takes-in-a-grayscale-or-rgb-image-outputting-features-with-a-much-larger-number-of-channelsi-am-thinking-of-each-output-channel-as-a-separate-lint-roller-to-run-over-the-imagefrom-the-docs-the-parameter-groups-controls-the-connections-between-inputs-and-outputsboth-in_channels-and-out_channels-must-be-divisible-by-groupsat-groups--1-all-inputs-are-convolved-to-all-outputsat-groups--2-the-operation-becomes-equivalent-to-having-two-convolutional-layers-side-by-side-each-seeing-half-the-input-channels-and-producing-half-the-output-channels-and-both-subsequently-concatenatedat-groups--in_channels-each-input-channel-is-convolved-with-its-own-set-of-filters-of-size-out_channels--in_channelsby-taking-in_channels--1-we-must-have-groups--1-alsothe-learned-parameters-of-layer-tildeb-are-of-two-types-those-in-the-weight-tensor-tildetheta-and-those-in-the-bias-vector-tildeb-at-this-point-for-simplicity-we-set-tildeb-equiv-0-via-bias--false-so-that-the-only-learned-parameters-are-those-in-tildetheta------------from-torchnn-import-conv1d--------we-now-initialize-the-convolutional-layer------------b_tilde--conv1din_channels--odcpin_channels-----------------out_channels--odcpout_channels-----------------kernel_size--odcpkernel_size-----------------stride--odcpstride-----------------padding--odcppadding-----------------dilation--odcpdilation-----------------groups--odcpgroups-----------------bias--false------------------------let-us-initialize-a-few-possible-input-signals-x_1-x_2-x_3--------------------------x_1--torcharange032infox_1-x-1viz_tens-x_1-display_size--10-x_2--torchrand-32--infox_2-x-2viz_tens-x_2-display_size--10-x_3--torchrandn-32-infox_3-x-3viz_tens-x_3-display_size--10---------tensor-----x-1-----num-dims--------1----num-entries-----32----shape------------32-0--1--2--3--4--5--6--7--8--9--10--11--12--13--14--15--16--17--18--19--20--21--22--23--24--25--26--27--28--29--30--31-tensor-----x-2-----num-dims--------1----num-entries-----32----shape------------32-025--069--089--036--029--004--024--006--038--060--003--093--081--001--026--066--039--044--027--090--022--091--053--060--089--041--021--041--090--012--061--000-tensor-----x-3-----num-dims--------1----num-entries-----32----shape------------32--02--151--003--103--088---13---03--101---16---00---12--119---07--111--029--029---05--194---04---03--144---07--249---03--087--021--122---05---01---04---01---02-----to-apply-b_tilde-to-these-signals-we-must-augment-each-signal-with-batch-and-channel-dimensions--------------------------x_1--torchas_tensor-x_1-none-none---dtype-torchfloat32-x_2--torchas_tensor-x_2-none-none---dtype-torchfloat32-x_3--torchas_tensor-x_3-none-none---dtype-torchfloat32-------------each-output------------y_1--b_tilde-x_1-detachy_2--b_tilde-x_2-detachy_3--b_tilde-x_3-detachnparray-objsize-print-nparray-y_1size-------1-16-15--1-16-15--1-16-15----------------infoy_1-y-1----tensor-----y-1-----num-dims--------3----num-entries-----240----shape-------------1-16-15--0---0---1--06--04--00--06--18---1---0--14---0--00--10--13---2-----recall-that-in-general-one-hastildetheta-in-mathcalx--h-backslash-g--tildeh-mathcalc-otimes-tildemathcalc-but-let-us-reduce-this-further-based-on-the-assumptions-made-because-g-equiv-omega-equiv-c_n-subgroup-h-must-be-the-trivial-subgroup--e--moreover-having-assumed-that-dimmathcalc--1-one-also-has-mathcalc-otimes-tildemathcalc-cong-tildemathcalc-so-we-may-writetildetheta-in-mathcalx--g--tildeh---tildemathcalc-with-c_tilden-cong-g--tildeh24--two-dimensional-convolutions-in-torch-torchnnconv2d-the-argumentsin_channelsout_channelskernel_sizestride-quad-controls-the-stride-for-the-cross-correlation-a-single-number-or-a-tuplepadding-quad-controls-amount-of-padding-applied-to-the-input-it-can-either-be-a-string-valid-or-same-or-a-tuple-of-ints-giving-the-amount-of-implicit-padding-applied-on-both-sidesdilation-quad-controls-the-spacing-between-kernel-points-also-known-as-the-a-trous-algorithmgroups-quad-controls-connections-between-inputs-and-outputs-the-in_channels-and-out_channels-must-be-divisible-by-groups-for-exampleat-groups--1-all-inputs-are-convolved-to-all-outputsat-groups--2-the-operation-becomes-equivalent-to-having-two-conv-layers-side-by-side-each-seeing-half-the-input-channels-and-producing-half-the-output-channels-and-both-subsequently-concatenatedat-groups--in_channels-each-input-channel-is-convolved-with-its-own-set-of-filters-of-size-out_channels--in_channelsbiaspadding_modedevicedtypelet-us-now-relate-the-shapes-of-the-input-and-output-to-the-parametersinput-parameterlatex-symbolin_channelstextdimmathcalcout_channelstextdimmathcalc_1kernel_sizekstridelambdapaddingrhodilationdeltagroupsm--------additionally-we-use-n-for-the-batch-size-of-the-input-n-we-also-let-hw-denote-the-height-width-pair-describing-the-shape-of-the-input-signal-domaincorrespondingly-we-write-h_1-w_1-for-the-height-width-pair-describing-the-shape-of-the-output-signal-domainwe-remark-that-the-stride-can-be-either-integer-or-a-2-tuple-whose-coordinates-describe-the-vertical-and-horizontal-stride-respectively-we-still-write-lambda-for-the-stride-when-it-is-a-tuple-and-use-lambda_h-equiv-lambda0-and-lambda_w-equiv-lambda1-to-denote-its-first-and-second-coordinate-in-this-case-likewise-the-padding-and-kernel-size-may-be-2-tuples-as-well-and-we-use-similar-notation-to-denote-their-entriesthe-full-shape-of-the-input-to-the-layer-includes-the-batch-dimension-and-is-thusn-textdimmathcalc-h-w-while-the-shape-of-the-output-isn--textdimmathcalc_1-h_1-w_1-these-shapes-in-particular-the-spatial-dimensions-of-each-are-related-as-followsbeginalignh_1--leftlfloor-frac----h--2-rho_h---delta_h--k_h--1--1-lambda_hrightrfloor-w_1--leftlfloor-frac----w--2-rho_w---delta_w--k_w--1--1-lambda_wrightrfloorendalignin-particular-the-batch-size-does-not-have-any-bearing-on-how-the-shapes-of-tensors-transformthe-parameters-to-be-learned-are-the-weights-w1-and-biases-b1-these-are-both-tensor-objects-accessed-from-the-layer-as-conv2dweight-and-conv2dbias-the-shape-of-the-weight-tensor-istextrmshapew1-left--textdimmathcalc_1---textdimmathcalc-big-m--k_h-k_w-rightthe-tensor-w1-thus-hastextrmsizew1--textrmdimmathcalc_1-textrmdim-mathcalc-k_h-k_w-big-mscalar-entriesthere-is-always-the-question-of-how-to-initialize-weights-in-the-case-of-the-conv2d-class-the-weights-are-initialized-to-be-iid-textunif---sqrt-alpha_1-sqrtalpha_1--random-variables-wherealpha_1--frac-textrmdimmathcalc_1-textrmsizew1the-bias-tensor-is-a-much-smaller-object-we-havebeginaligntextrmshapeb1---textrmdimmathcalc_1------quad-textrmsizeb1--textrmdimmathcalc_1endaligndespite-this-we-use-the-same-initialization-with-mutual-independence-of-all-random-variables-in-discussion-for-the-bias-entries-as-we-did-for-the-weights-a-simple-cnn-we-consider-possibly-the-simplest-neural-network-that-we-can-construct-through-the-above-blueprint-suppose-we-have-a-binary-classification-problem-with-the-following-hypothesis-space-let-textsfh_1-denote-the-hypothesis-space-of-functions-f--mathcalx-c_n-mathbbr-to-01-of-the-formf--a-circ-p-circ-mathbfa-circ-b-where-the-components-of-f-arewhere-the-components-of-f-areb---quad-a-c_n-equivariant-function-to-be-learned-it-is-represented-as-a-circulant-matrix-mathbfctheta-where-theta-is-a-vector-theta-equiv-theta_0-dots-theta_n-1-whose-entries-theta_j-are-parameters-to-be-learned-mathbfa---quad-we-consider-the-relu-activation-function-a--mathbbr-to-mathbbr_geq-0-defined-by-aw--max0w-for-w-in-mathbbr-the-bold-face-mathbfa-denotes-the-entry-wise-action-of-this-function-on-a-given-vectorfor-y-equiv-y_1-dots--y_n---in-mathcalxc_n-mathbbr-which-we-imagine-as-the-output-of-bx-for-some-input-signal-x-we-have-mathbfa-y------max0y_1---dots--max0y_n--there-are-no-learned-parameters-in-this-layerp--quad-a-coarsening-operator-in-this-case-let-us-say-it-is-a-zero-padded-group-homomorphismp--c_n-to-c_n--d--for-some-divisor-d-mid-n-footnotezero-padding--and-let-us-say-that-it-operates-through-max-pooling-on-the-signal-over-the-pre-images-of-each-element-of-c_n--da--quad-a-global-pooling-layer-we-assume-this-has-the-form-of-a-fully-connected-layer-followed-by-a-softmax-specificallydiv-" aria-hidden="true"><span class="octicon octicon-link"></span></a><font color="teal">... example: concrete layer&lt;/h3&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For now, we will instantiate a parameters dictionary with <code>dilation = 1</code> and <code>padding = 0</code>. Let us use $n = 32$, so that</p>
$$
\tilde{n} = \left\lfloor \frac{n-k}{\lambda} + 1 \right\rfloor \equiv \left\lfloor \frac{32-k}{\lambda} + 1 \right\rfloor
$$<p>For our example, we specialize to <code>kernel_size = 4</code> and <code>stride = 2</code>, in which case $\tilde{n} = 15$.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">odcp</span> <span class="o">=</span> <span class="p">{}</span> <span class="c1"># one dimensional conv. parameters</span>

<span class="n">odcp</span><span class="p">[</span><span class="s2">"in_size"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">odcp</span><span class="p">[</span><span class="s2">"in_channels"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">odcp</span><span class="p">[</span><span class="s2">"padding"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">odcp</span><span class="p">[</span><span class="s2">"dilation"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">odcp</span><span class="p">[</span><span class="s2">"kernel_size"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">odcp</span><span class="p">[</span><span class="s2">"stride"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">odcp</span><span class="p">[</span><span class="s2">"groups"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">odcp</span><span class="p">[</span><span class="s2">"out_channels"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">16</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">odcp</span><span class="p">[</span><span class="s2">"out_size"</span><span class="p">]</span> <span class="o">=</span> <span class="n">compute_out_size</span><span class="p">(</span>
    <span class="n">in_size</span> <span class="o">=</span> <span class="n">odcp</span><span class="p">[</span><span class="s2">"in_size"</span><span class="p">],</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="n">odcp</span><span class="p">[</span><span class="s2">"padding"</span><span class="p">],</span>
    <span class="n">dilation</span> <span class="o">=</span> <span class="n">odcp</span><span class="p">[</span><span class="s2">"dilation"</span><span class="p">],</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">odcp</span><span class="p">[</span><span class="s2">"kernel_size"</span><span class="p">],</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">odcp</span><span class="p">[</span><span class="s2">"stride"</span><span class="p">]</span>
    <span class="p">)</span>

<span class="nb">print</span><span class="p">(</span> <span class="n">odcp</span><span class="p">[</span><span class="s2">"out_size"</span><span class="p">]</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>15.0
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>One of the important free parameters we can choose to initialize a one-dimensional convolutional layer is the number of <code>out_channels</code>. We choose <code>out_channels = 16</code> in our first example.</p>
<p>In many vision related CNNs, the first layer takes in a grayscale or RGB image, outputting features with a <em>much</em> larger number of channels.</p>
<blockquote>
<p>I am thinking of each output channel as a separate "lint roller" to run over the image.</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>From the <a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html">docs</a>, the parameter <code>groups</code> "controls the connections between inputs and outputs."</p>
<p>Both <code>in_channels</code> and <code>out_channels</code> must be divisible by <code>groups</code>:</p>
<ul>
<li>
<p>At <code>groups = 1</code>, all inputs are convolved to all outputs.</p>
</li>
<li>
<p>At <code>groups = 2</code>, the operation becomes equivalent to having two convolutional layers side by side, each seeing half the input channels and producing half the output channels, and both subsequently concatenated.</p>
</li>
<li>
<p>At <code>groups = in_channels</code>, each input channel is convolved with its own set of filters (of size <code>out_channels</code> / <code>in_channels</code>).</p>
</li>
</ul>
<p>By taking <code>in_channels = 1</code>, we must have <code>groups = 1</code> also.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The learned parameters of layer $\tilde{B}$ are of two types: those in the weight tensor $\tilde{\theta}$, and those in the bias vector $\tilde{b}$. At this point, for simplicity, we set $\tilde{b} \equiv 0$ via <code>bias = False</code>, so that the only learned parameters are those in $\tilde{\theta}$.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">Conv1d</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We now initialize the convolutional layer.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">B_tilde</span> <span class="o">=</span> <span class="n">Conv1d</span><span class="p">(</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">odcp</span><span class="p">[</span><span class="s2">"in_channels"</span><span class="p">],</span>
                 <span class="n">out_channels</span> <span class="o">=</span> <span class="n">odcp</span><span class="p">[</span><span class="s2">"out_channels"</span><span class="p">],</span>
                 <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">odcp</span><span class="p">[</span><span class="s2">"kernel_size"</span><span class="p">],</span>
                 <span class="n">stride</span> <span class="o">=</span> <span class="n">odcp</span><span class="p">[</span><span class="s2">"stride"</span><span class="p">],</span>
                 <span class="n">padding</span> <span class="o">=</span> <span class="n">odcp</span><span class="p">[</span><span class="s2">"padding"</span><span class="p">],</span>
                 <span class="n">dilation</span> <span class="o">=</span> <span class="n">odcp</span><span class="p">[</span><span class="s2">"dilation"</span><span class="p">],</span>
                 <span class="n">groups</span> <span class="o">=</span> <span class="n">odcp</span><span class="p">[</span><span class="s2">"groups"</span><span class="p">],</span>
                 <span class="n">bias</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let us initialize a few possible input signals: <code>X_1</code>, <code>X_2</code>, <code>X_3</code>.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">32</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">X_1</span><span class="p">,</span> <span class="s2">"X 1"</span><span class="p">)</span>
<span class="n">viz_tens</span><span class="p">(</span> <span class="n">X_1</span><span class="p">,</span> <span class="n">display_size</span> <span class="o">=</span> <span class="mi">10</span> <span class="p">)</span>

<span class="n">X_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span> <span class="p">(</span><span class="mi">32</span><span class="p">)</span> <span class="p">)</span> 
<span class="n">info</span><span class="p">(</span><span class="n">X_2</span><span class="p">,</span> <span class="s2">"X 2"</span><span class="p">)</span>
<span class="n">viz_tens</span><span class="p">(</span> <span class="n">X_2</span><span class="p">,</span> <span class="n">display_size</span> <span class="o">=</span> <span class="mi">10</span> <span class="p">)</span>

<span class="n">X_3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span> <span class="p">(</span><span class="mi">32</span><span class="p">)</span> <span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">X_3</span><span class="p">,</span> <span class="s2">"X 3"</span><span class="p">)</span>
<span class="n">viz_tens</span><span class="p">(</span> <span class="n">X_3</span><span class="p">,</span> <span class="n">display_size</span> <span class="o">=</span> <span class="mi">10</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     X 1 

    num. dims        1
    num. entries     32
    shape            [32]


┌───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┐
│ 0 │ 1 │ 2 │ 3 │ 4 │ 5 │ 6 │ 7 │ 8 │ 9 │ 10 │ 11 │ 12 │ 13 │ 14 │ 15 │ 16 │ 17 │ 18 │ 19 │ 20 │ 21 │ 22 │ 23 │ 24 │ 25 │ 26 │ 27 │ 28 │ 29 │ 30 │ 31 │
└───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┘



tensor     X 2 

    num. dims        1
    num. entries     32
    shape            [32]


┌──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┐
│ 0.25 │ 0.69 │ 0.89 │ 0.36 │ 0.29 │ 0.04 │ 0.24 │ 0.06 │ 0.38 │ 0.60 │ 0.03 │ 0.93 │ 0.81 │ 0.01 │ 0.26 │ 0.66 │ 0.39 │ 0.44 │ 0.27 │ 0.90 │ 0.22 │ 0.91 │ 0.53 │ 0.60 │ 0.89 │ 0.41 │ 0.21 │ 0.41 │ 0.90 │ 0.12 │ 0.61 │ 0.00 │
└──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┘



tensor     X 3 

    num. dims        1
    num. entries     32
    shape            [32]


┌──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┐
│ -0.2 │ 1.51 │ 0.03 │ 1.03 │ 0.88 │ -1.3 │ -0.3 │ 1.01 │ -1.6 │ -0.0 │ -1.2 │ 1.19 │ -0.7 │ 1.11 │ 0.29 │ 0.29 │ -0.5 │ 1.94 │ -0.4 │ -0.3 │ 1.44 │ -0.7 │ 2.49 │ -0.3 │ 0.87 │ 0.21 │ 1.22 │ -0.5 │ -0.1 │ -0.4 │ -0.1 │ -0.2 │
└──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┘



</pre>
</div>
</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAjwAAAAfCAYAAADjqBcrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAA30lEQVR4nO3bwQnDMBBFwSikDvffpVJBcsmKmMfMVfDZ48Pgtfd+AACUPf99AADAaYIHAMgTPABAnuABAPIEDwCQ9/r2uNb6+Reuqb/AJnbccu+NqR23nNuY2rnLxtSOW+69MbXjlnMbUzvXda1Pb77wAAB5ggcAyBM8AECe4AEA8gQPAJAneACAPMEDAOQJHgAgT/AAAHmCBwDIEzwAQJ7gAQDyBA8AkCd4AIA8wQMA5AkeACBP8AAAeWvv/e8bAACO8oUHAMgTPABAnuABAPIEDwCQJ3gAgDzBAwDkvQHJm2Q54H0SmgAAAABJRU5ErkJggg==%0A">
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAjwAAAAfCAYAAADjqBcrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABfElEQVR4nO3bIYpCYRSG4eMw0eROREEwic3sHjRaDS7AVYhbsFvsJnEDFqsrkDsrmCnnl4HD89QLHycovlyw13VdAABU9vXfBwAAfJrgAQDKEzwAQHmCBwAoT/AAAOV9//VwPp+n/8K12WyyExERMR6P0xvb7bbBJRGv1yu9cblcGlwSMRwO0xu32y29sVwu0xsREZPJJL3R6jP3fD7TG4/Ho8ElEaPRKL0xm83yh0TEer1Ob5zP5/RGv99Pb0RE3O/39MbxeMwfEm2+R6fTqcElEYfDIb0xGAzSG4vFIr0REbHf79Mb1+u1wSVtfs92u12DSyKm02l6o9Utq9UqvfF+v3u/PfOGBwAoT/AAAOUJHgCgPMEDAJQneACA8gQPAFCe4AEAyhM8AEB5ggcAKE/wAADlCR4AoDzBAwCUJ3gAgPIEDwBQnuABAMoTPABAeYIHACiv13Xdf98AAPBR3vAAAOUJHgCgPMEDAJQneACA8gQPAFCe4AEAyvsBRL83OchkoxIAAAAASUVORK5CYII=%0A">
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAjwAAAAfCAYAAADjqBcrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABdUlEQVR4nO3bsYkCYRCG4dnzAhO7MBPMxAoMTQ03tBADG9EWxBbMNDS3AsF8reAumV+E4XlS4WMwWF4WthuGIQAAKvv59gEAAJ8meACA8gQPAFCe4AEAyhM8AEB5v//9eDqd0p9wXa/X7EREROz3+/RGqy/S7vd7eqPV//J6vdIbfd+nNzabTXojIuJ4PKY3brdbg0sittttemM8Hje4JOLxeKQ35vN5g0siuq5Lb0yn0/TGbrdLb0REHA6H9MZkMmlwSZvnwuVyaXBJxGq1Sm8sl8v0xmw2S29EtHlGjUajBpe0+V9aPJ8iIs7nc3pjvV43uCTi+XymNxaLxZ8PKG94AIDyBA8AUJ7gAQDKEzwAQHmCBwAoT/AAAOUJHgCgPMEDAJQneACA8gQPAFCe4AEAyhM8AEB5ggcAKE/wAADlCR4AoDzBAwCUJ3gAgPK6YRi+fQMAwEd5wwMAlCd4AIDyBA8AUJ7gAQDKEzwAQHmCBwAo7w1j7jc5RgUSngAAAABJRU5ErkJggg==%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To apply <code>B_tilde</code> to these signals, we must augment each signal with batch and channel dimensions:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span> <span class="n">X_1</span><span class="p">[</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:</span> <span class="p">],</span> <span class="n">dtype</span><span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span> <span class="p">)</span>
<span class="n">X_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span> <span class="n">X_2</span><span class="p">[</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:</span> <span class="p">],</span> <span class="n">dtype</span><span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span> <span class="p">)</span>
<span class="n">X_3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span> <span class="n">X_3</span><span class="p">[</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:</span> <span class="p">],</span> <span class="n">dtype</span><span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Each output</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Y_1</span> <span class="o">=</span> <span class="n">B_tilde</span><span class="p">(</span> <span class="n">X_1</span> <span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="n">Y_2</span> <span class="o">=</span> <span class="n">B_tilde</span><span class="p">(</span> <span class="n">X_2</span> <span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="n">Y_3</span> <span class="o">=</span> <span class="n">B_tilde</span><span class="p">(</span> <span class="n">X_3</span> <span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

<span class="c1">#np.array( obj.size() )</span>

<span class="nb">print</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span> <span class="n">Y_1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="p">)</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[ 1 16 15] [ 1 16 15] [ 1 16 15]
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">info</span><span class="p">(</span><span class="n">Y_1</span><span class="p">,</span> <span class="s2">"Y 1"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     Y 1 

    num. dims        3
    num. entries     240
    shape            [ 1 16 15]


┌──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┐
│ [-0. │ [-0. │ [-1. │ [0.6 │ [0.4 │ [0.0 │ [0.6 │ [1.8 │ [-1. │ [-0. │ [1.4 │ [-0. │ [0.0 │ [1.0 │ [1.3 │ [-2. │
└──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Recall that, in general, one has</p>
$$
\tilde{\theta} \in \mathcal{X} ( H \backslash G / \tilde{H}, \mathcal{C} \otimes \tilde{\mathcal{C}} ),
$$<p>but let us reduce this further based on the assumptions made. Because $G \equiv \Omega \equiv C_n$, subgroup $H$ must be the trivial subgroup $\{ e \}$. Moreover, having assumed that $\dim{\mathcal{C}} = 1$, one also has $\mathcal{C} \otimes \tilde{\mathcal{C}} \cong \tilde{\mathcal{C}}$, so we may write</p>
$$
\tilde{\theta} \in \mathcal{X} ( G / \tilde{H} ) , \tilde{\mathcal{C}} ),
$$<p>with $C_{\tilde{n}} \cong G / \tilde{H}$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="2.4-...-two-dimensional-convolutions-in-torch">
<a class="anchor" href="#2.4-...-two-dimensional-convolutions-in-torch" aria-hidden="true"><span class="octicon octicon-link"></span></a><font color="CornflowerBlue">2.4 ... two-dimensional convolutions in torch</font><a class="anchor-link" href="#2.4-...-two-dimensional-convolutions-in-torch"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h4 id="torch.nn.Conv2d">
<a class="anchor" href="#torch.nn.Conv2d" aria-hidden="true"><span class="octicon octicon-link"></span></a><code>torch.nn.Conv2d</code><a class="anchor-link" href="#torch.nn.Conv2d"> </a>
</h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The arguments:</p>
<ul>
<li>
<p><code>in_channels</code></p>
</li>
<li>
<p><code>out_channels</code></p>
</li>
<li>
<p><code>kernel_size</code></p>
</li>
<li>
<p><code>stride</code> $\quad$ controls the stride for the cross-correlation, a single number or a tuple.</p>
</li>
<li>
<p><code>padding</code> $\quad$ controls amount of padding applied to the input. It can either be a string, <code>"valid"</code> or <code>"same"</code> or a tuple of ints giving the amount of implicit padding applied on both sides.</p>
</li>
<li>
<p><code>dilation</code> $\quad$ controls the spacing between kernel points; "also known as the a trous algorithm</p>
</li>
<li>
<p><code>groups</code> $\quad$ controls connections between inputs and outputs. The <code>in_channels</code> and <code>out_channels</code> must be divisible by <code>groups</code>. For example,</p>
<ul>
<li>
<p>At groups = 1, all inputs are convolved to all outputs</p>
</li>
<li>
<p>At groups = 2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated.</p>
</li>
<li>
<p>At groups = <code>in_channels</code>, each input channel is convolved with its own set of filters (of size <code>out_channels // in_channels</code>)</p>
</li>
</ul>
</li>
<li>
<p><code>bias</code></p>
</li>
<li>
<p><code>padding_mode</code>,</p>
</li>
<li>
<p><code>device</code>,</p>
</li>
<li>
<p><code>dtype</code></p>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let us now relate the shapes of the input and output to the parameters</p>
<table>
<thead>
<tr>
<th>input parameter</th>
<th>LaTeX symbol</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>in_channels</code></td>
<td>$\text{dim}(\mathcal{C})$</td>
</tr>
<tr>
<td><code>out_channels</code></td>
<td>$\text{dim}(\mathcal{C}_1)$</td>
</tr>
<tr>
<td><code>kernel_size</code></td>
<td>$k$</td>
</tr>
<tr>
<td><code>stride</code></td>
<td>$\lambda$</td>
</tr>
<tr>
<td><code>padding</code></td>
<td>$\rho$</td>
</tr>
<tr>
<td><code>dilation</code></td>
<td>$\delta$</td>
</tr>
<tr>
<td><code>groups</code></td>
<td>$M$        </td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Additionally, we use $N$ for the batch size of the input, <code>N</code>. We also let $(h,w)$ denote the height-width pair describing the shape of the input signal domain.</p>
<p>Correspondingly, we write $(h_1, w_1)$ for the height-width pair describing the shape of the output signal domain.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We remark that the stride can be either integer or a $2$-tuple, whose coordinates describe the vertical and horizontal stride respectively. We still write $\lambda$ for the stride when it is a tuple, and use $\lambda_h \equiv \lambda[0]$ and $\lambda_w \equiv \lambda[1]$ to denote its first and second coordinate, in this case. Likewise, the padding and kernel size may be $2$-tuples as well, and we use similar notation to denote their entries.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The full shape of the input to the layer includes the batch dimension, and is thus</p>
$$
(N, \text{dim}(\mathcal{C}), H, W) \,,
$$<p>while the shape of the output is</p>
$$
(N , \text{dim}(\mathcal{C}_1), H_1, W_1 )
$$<p>These shapes, in particular the spatial dimensions of each, are related as follows:</p>
<p>$\begin{align}
H_1 &amp;= \left\lfloor \frac{
    H + 2 \rho_h - \delta_h ( k_h -1) -1 }{\lambda_h}
\right\rfloor \\
W_1 &amp;= \left\lfloor \frac{
    W + 2 \rho_w - \delta_w ( k_w -1) -1 }{\lambda_w}
\right\rfloor
\end{align}$,</p>
<p>in particular, the batch size does not have any bearing on how the shapes of tensors transform.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The parameters to be learned are the weights $w^1$ and biases $b^1$. These are both <code>Tensor</code> objects, accessed from the layer as <code>Conv2d.weight</code> and <code>Conv2d.bias</code>. The shape of the weight tensor is</p>
$$
\textrm{shape}(w^1) =
\left( \, \text{dim}(\mathcal{C}_1),  \, \text{dim}(\mathcal{C}) \big/ M , k_h, k_w \right)
$$<p>The tensor $w^1$ thus has</p>
$$
\textrm{size}(w^1) = \textrm{dim}(\mathcal{C}_1) \textrm{dim} (\mathcal{C}) k_h k_w \big/ M
$$<p>scalar entries.</p>
<p>There is always the question of how to initialize weights. In the case of the <code>Conv2d</code> class, the weights are initialized to be i.i.d. $\text{Unif}( - \sqrt{ \alpha_1}, \sqrt{\alpha_1} )$ random variables, where</p>
$$
\alpha_1 := \frac{ \textrm{dim}(\mathcal{C}_1) }{\textrm{size}(w^1)}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The bias tensor is a much smaller object, we have</p>
<p>$
\begin{align}
\textrm{shape}(b^1) = (\, \textrm{dim}(\mathcal{C}_1  ) \,) \, , \quad \textrm{size}(b^1) = \textrm{dim}(\mathcal{C}_1)
\end{align}
$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Despite this, we use the same initialization (with mutual independence of all random variables in discussion) for the bias entries as we did for the weights.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="...-A-simple-CNN">
<a class="anchor" href="#...-A-simple-CNN" aria-hidden="true"><span class="octicon octicon-link"></span></a>... A simple CNN<a class="anchor-link" href="#...-A-simple-CNN"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We consider possibly the simplest neural network that we can construct through the above blueprint. Suppose we have a binary classification problem, with the following hypothesis space. Let $\textsf{H}_1$ denote the hypothesis space of functions $f : \mathcal{X}( C_n, \mathbb{R}) \to \{0,1\}$ of the form</p>
$$
f = A \circ P \circ \mathbf{a} \circ B \,,
$$<p>where the components of $f$ are</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>where the components of $f$ are</p>
<ul>
<li>
<p>$B$  : $\quad$ A $C_n$-equivariant function, to be learned. It is represented as a circulant matrix $\mathbf{C}(\theta)$, where $\theta$ is a vector $\theta \equiv (\theta_0, \dots, \theta_{n-1})$ whose entries $\theta_j$ are parameters to be learned.</p>
</li>
<li>
<p>$ \mathbf{a} $ : $\quad$ We consider the ReLU activation function, $a : \mathbb{R} \to \mathbb{R}_{\geq\, 0}$ defined by $a(w) = \max(0,w)$, for $w \in \mathbb{R}$. The bold-face $\mathbf{a}$ denotes the entry-wise action of this function on a given vector;for $y \equiv (\,y_1, \,\dots, \, y_n \, ) \in \mathcal{X}(C_n, \mathbb{R})$, which we imagine as the output of $B(x)$ for some input signal $x$, we have $\mathbf{a} (y ) = ( \,  \max(0,y_1), \,  \dots, \, \max(0,y_n) )$. There are no learned parameters in this layer.</p>
</li>
<li>
<p>$P$ : $\quad$ A coarsening operator. In this case, let us say it is a <em>zero-padded group homomorphism</em>.</p>
<p>$P : C_n \to C_{n / d }$ for some divisor $d \mid n$ \footnote{zero-padding} , and let us say that it operates through max-pooling on the signal, over the pre-images of each element of $C_{n / d}$.</p>
</li>
<li>
<p>$A$ : $\quad$ A global-pooling layer. We assume this has the form of a fully-connected layer, followed by a softmax. Specifically,</p>
</li>
</ul>

</div>
</div>
</div>
&lt;/div&gt;
 

</font>
</h3>
</div>
</div></div>
</div>


  </div><a class="u-url" href="/geometric-deep-learning/jupyter/2021/10/22/GDL2.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/geometric-deep-learning/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/geometric-deep-learning/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/geometric-deep-learning/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>...</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" title="fastai"><svg class="svg-icon grey"><use xlink:href="/geometric-deep-learning/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/geometric-deep-learning/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
