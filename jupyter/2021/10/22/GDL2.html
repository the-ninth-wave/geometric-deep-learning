<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>2 | geometric deep learning</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="2" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="GDL" />
<meta property="og:description" content="GDL" />
<link rel="canonical" href="https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2021/10/22/GDL2.html" />
<meta property="og:url" content="https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2021/10/22/GDL2.html" />
<meta property="og:site_name" content="geometric deep learning" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-10-22T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"GDL","url":"https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2021/10/22/GDL2.html","@type":"BlogPosting","headline":"2","dateModified":"2021-10-22T00:00:00-05:00","datePublished":"2021-10-22T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2021/10/22/GDL2.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/geometric-deep-learning/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://the-ninth-wave.github.io/geometric-deep-learning/feed.xml" title="geometric deep learning" /><link rel="shortcut icon" type="image/x-icon" href="/geometric-deep-learning/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/geometric-deep-learning/">geometric deep learning</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/geometric-deep-learning/about/">About Me</a><a class="page-link" href="/geometric-deep-learning/search/">Search</a><a class="page-link" href="/geometric-deep-learning/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">2</h1><p class="page-description">GDL</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-10-22T00:00:00-05:00" itemprop="datePublished">
        Oct 22, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      28 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/geometric-deep-learning/categories/#jupyter">jupyter</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/the-ninth-wave/geometric-deep-learning/tree/master/_notebooks/2021-10-22-GDL2.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/geometric-deep-learning/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/the-ninth-wave/geometric-deep-learning/master?filepath=_notebooks%2F2021-10-22-GDL2.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/geometric-deep-learning/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/the-ninth-wave/geometric-deep-learning/blob/master/_notebooks/2021-10-22-GDL2.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/geometric-deep-learning/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#geometric-deep-learning-models">geometric deep learning models </a>
<ul>
<li class="toc-entry toc-h2"><a href="#2.1-...-tensors-in-pytorch">2.1 ... tensors in pytorch </a>
<ul>
<li class="toc-entry toc-h3"><a href="#...-helper-classes-and-functions-">... helper classes and functions  </a>
<ul>
<li class="toc-entry toc-h4"><a href="#method-...-info">method ... info </a></li>
<li class="toc-entry toc-h4"><a href="#method-...-viz_tens">method ... viz_tens </a></li>
<li class="toc-entry toc-h4"><a href="#method-...-viz_tens_list">method ... viz_tens_list </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#...-initializing-tensors">... initializing tensors </a>
<ul>
<li class="toc-entry toc-h4"><a href="#...-1.---from-lists">... 1.   from lists </a></li>
<li class="toc-entry toc-h4"><a href="#...-2.-from-numpy">... 2. from numpy </a></li>
<li class="toc-entry toc-h4"><a href="#...-3.-(a)-ones-of-given-shape">... 3. (a) ones of given shape </a></li>
<li class="toc-entry toc-h4"><a href="#...-3.-(b)-zeros-of-given-shape">... 3. (b) zeros of given shape </a></li>
<li class="toc-entry toc-h4"><a href="#...-3.-(c)-i.i.d.-uniform-of-given-shape">... 3. (c) i.i.d. uniform of given shape </a></li>
<li class="toc-entry toc-h4"><a href="#...-3.-(d)-i.i.d.-standard-normal-of-given-shape">... 3. (d) i.i.d. standard normal of given shape </a></li>
<li class="toc-entry toc-h4"><a href="#...-3.-(f)-sequence">... 3. (f) sequence </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#numpy-objects-and-tensors">numpy objects and tensors </a></li>
<li class="toc-entry toc-h3"><a href="#...-tensor-operations">... tensor operations </a>
<ul>
<li class="toc-entry toc-h4"><a href="#...-adding-tensors">... adding tensors </a></li>
<li class="toc-entry toc-h4"><a href="#...-stacking-tensors">... stacking tensors </a></li>
<li class="toc-entry toc-h4"><a href="#...-in-place-operations">... in-place operations </a></li>
<li class="toc-entry toc-h4"><a href="#...-reshaping-tensors">... reshaping tensors </a></li>
<li class="toc-entry toc-h4"><a href="#...-transposing-tensors-(permuting-dimensions)">... transposing tensors (permuting dimensions) </a></li>
<li class="toc-entry toc-h4"><a href="#...-numpy-like-indexing-and-slicing">... numpy-like indexing and slicing </a></li>
<li class="toc-entry toc-h4"><a href="#...-other-operations">... other operations </a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#2.2-...-one-dimensional-convolutions-of-scalar-valued-signals">2.2 ... one-dimensional convolutions of scalar-valued signals </a>
<ul>
<li class="toc-entry toc-h3"><a href="#...example:-local-averaging-as-a-circulant-matrix">...example: local averaging as a circulant matrix </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#2.3-...-one-dimensional-convolutions-in-torch">2.3 ... one-dimensional convolutions in torch </a>
<ul>
<li class="toc-entry toc-h4"><a href="#class-...-torch.nn.Conv1d">class ... torch.nn.Conv1d </a></li>
<li class="toc-entry toc-h3"><a href="#method-...-compute_num_out_channels">method ... compute_num_out_channels </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#2.4-...-two-dimensional-convolutions-in-torch">2.4 ... two-dimensional convolutions in torch </a>
<ul>
<li class="toc-entry toc-h4"><a href="#torch.nn.Conv2d">torch.nn.Conv2d </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#...-A-simple-CNN">... A simple CNN </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-10-22-GDL2.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="geometric-deep-learning-models">
<a class="anchor" href="#geometric-deep-learning-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>geometric deep learning models<a class="anchor-link" href="#geometric-deep-learning-models"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="2.1-...-tensors-in-pytorch">
<a class="anchor" href="#2.1-...-tensors-in-pytorch" aria-hidden="true"><span class="octicon octicon-link"></span></a><font color="CornflowerBlue">2.1 ... tensors in pytorch</font><a class="anchor-link" href="#2.1-...-tensors-in-pytorch"> </a>
</h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">torch</span> <span class="o">--</span><span class="n">upgrade</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu111)
Collecting torch
  Downloading torch-1.10.0-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)
     |██████████████████████████████▎ | 834.1 MB 1.2 MB/s eta 0:00:39tcmalloc: large alloc 1147494400 bytes == 0x55bad2e38000 @  0x7f020d191615 0x55ba992c24cc 0x55ba993a247a 0x55ba992c52ed 0x55ba993b6e1d 0x55ba99338e99 0x55ba993339ee 0x55ba992c6bda 0x55ba99338d00 0x55ba993339ee 0x55ba992c6bda 0x55ba99335737 0x55ba993b7c66 0x55ba99334daf 0x55ba993b7c66 0x55ba99334daf 0x55ba993b7c66 0x55ba99334daf 0x55ba992c7039 0x55ba9930a409 0x55ba992c5c52 0x55ba99338c25 0x55ba993339ee 0x55ba992c6bda 0x55ba99335737 0x55ba993339ee 0x55ba992c6bda 0x55ba99334915 0x55ba992c6afa 0x55ba99334c0d 0x55ba993339ee
     |████████████████████████████████| 881.9 MB 19 kB/s 
Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)
Installing collected packages: torch
  Attempting uninstall: torch
    Found existing installation: torch 1.9.0+cu111
    Uninstalling torch-1.9.0+cu111:
      Successfully uninstalled torch-1.9.0+cu111
<span class="ansi-red-fg">ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
torchvision 0.10.0+cu111 requires torch==1.9.0, but you have torch 1.10.0 which is incompatible.
torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.10.0 which is incompatible.</span>
Successfully installed torch-1.10.0
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.utils.data</span> <span class="k">as</span> <span class="nn">data</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Using torch"</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Using torch 1.10.0+cu102
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="...-helper-classes-and-functions-">
<a class="anchor" href="#...-helper-classes-and-functions-" aria-hidden="true"><span class="octicon octicon-link"></span></a><font color="teal">... helper classes and functions </font><a class="anchor-link" href="#...-helper-classes-and-functions-"> </a>
</h3>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>pip install tabletext
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Collecting tabletext
  Downloading tabletext-0.1.tar.gz (6.1 kB)
Building wheels for collected packages: tabletext
  Building wheel for tabletext (setup.py) ... done
  Created wheel for tabletext: filename=tabletext-0.1-py3-none-any.whl size=6022 sha256=384237e663e1e0614b9290fcd435e0e01872c35657dbc01661714520ea433992
  Stored in directory: /root/.cache/pip/wheels/cc/ae/ab/697f6cd9887c63663da889f796c2c7ea280bc407b16f6fd081
Successfully built tabletext
Installing collected packages: tabletext
Successfully installed tabletext-0.1
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tabletext</span> <span class="kn">import</span> <span class="n">to_text</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h4 id="method-...-info">
<a class="anchor" href="#method-...-info" aria-hidden="true"><span class="octicon octicon-link"></span></a>method ... <code>info</code><a class="anchor-link" href="#method-...-info"> </a>
</h4>
<p><em>can currently handle 1- and 2-tensors</em></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">info</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>

    <span class="n">ty_tens</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">])</span> <span class="p">)</span>

    <span class="n">ty_np</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">])</span> <span class="p">)</span>

    <span class="n">delim</span> <span class="o">=</span> <span class="s2">"   "</span>

    <span class="k">if</span> <span class="n">obj</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>

        <span class="n">display_obj</span> <span class="o">=</span> <span class="n">obj</span><span class="p">[</span><span class="kc">None</span><span class="p">,:]</span>

    <span class="k">elif</span> <span class="n">obj</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>

        <span class="n">display_obj</span> <span class="o">=</span> <span class="n">obj</span>

    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span> <span class="o">==</span> <span class="n">ty_tens</span><span class="p">:</span>

        <span class="nb">print</span><span class="p">(</span> <span class="s2">"tensor"</span><span class="p">,</span> <span class="n">delim</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span> <span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span> <span class="n">delim</span><span class="p">,</span> <span class="s2">"num. dims   "</span><span class="p">,</span> <span class="n">delim</span><span class="p">,</span> <span class="n">obj</span><span class="o">.</span><span class="n">ndim</span> <span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span> <span class="n">delim</span><span class="p">,</span> <span class="s2">"num. entries"</span><span class="p">,</span> <span class="n">delim</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span> <span class="n">obj</span> <span class="p">)</span><span class="o">.</span><span class="n">size</span> <span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span> <span class="n">delim</span><span class="p">,</span> <span class="s2">"shape       "</span><span class="p">,</span> <span class="n">delim</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span> <span class="n">obj</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="p">)</span> <span class="p">)</span>

    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span> <span class="o">==</span> <span class="n">ty_np</span><span class="p">:</span>

        <span class="nb">print</span><span class="p">(</span> <span class="s2">"np array"</span><span class="p">,</span> <span class="n">delim</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span> <span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span> <span class="n">delim</span><span class="p">,</span> <span class="s2">"number of dimensions"</span><span class="p">,</span> <span class="n">obj</span><span class="o">.</span><span class="n">ndim</span> <span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span> <span class="n">delim</span><span class="p">,</span> <span class="s2">"number of entries"</span><span class="p">,</span> <span class="n">delim</span><span class="p">,</span> <span class="n">obj</span><span class="o">.</span><span class="n">size</span> <span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span> <span class="n">delim</span><span class="p">,</span> <span class="s2">"shape"</span><span class="p">,</span> <span class="n">delim</span><span class="p">,</span> <span class="n">obj</span><span class="o">.</span><span class="n">shape</span> <span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>

    <span class="n">display_list</span> <span class="o">=</span> <span class="n">display_obj</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="n">J</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span> <span class="n">display_list</span> <span class="p">)</span>
    <span class="n">K</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span> <span class="n">display_list</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">outer_list</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">J</span><span class="p">):</span>

        <span class="n">inner_list</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>

            <span class="n">inner_list</span> <span class="o">+=</span> <span class="p">[</span> <span class="nb">str</span><span class="p">(</span> <span class="n">display_list</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="p">)[:</span><span class="mi">4</span><span class="p">]</span> <span class="p">]</span>

        <span class="n">outer_list</span> <span class="o">+=</span> <span class="p">[</span> <span class="n">inner_list</span> <span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span> <span class="n">to_text</span><span class="p">(</span> <span class="n">outer_list</span> <span class="p">)</span> <span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n\n</span><span class="s2">"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h4 id="method-...-viz_tens">
<a class="anchor" href="#method-...-viz_tens" aria-hidden="true"><span class="octicon octicon-link"></span></a>method ... <code>viz_tens</code><a class="anchor-link" href="#method-...-viz_tens"> </a>
</h4>
<p>args</p>
<ul>
<li>
<p><code>tens</code></p>
</li>
<li>
<p><code>display_size</code></p>
</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">viz_tens</span><span class="p">(</span> <span class="n">tens</span><span class="p">,</span> <span class="n">display_size</span> <span class="o">=</span> <span class="mi">2</span> <span class="p">):</span>

    <span class="n">size</span> <span class="o">=</span> <span class="n">display_size</span>

    <span class="k">if</span> <span class="n">tens</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>

        <span class="n">display_obj</span> <span class="o">=</span> <span class="n">tens</span><span class="p">[</span><span class="kc">None</span><span class="p">,:]</span>

        <span class="n">display_obj</span> <span class="o">=</span> <span class="n">display_obj</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

    <span class="k">elif</span> <span class="n">tens</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>

        <span class="n">display_obj</span> <span class="o">=</span> <span class="n">tens</span>

        <span class="n">display_obj</span> <span class="o">=</span> <span class="n">display_obj</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

    <span class="n">pil_image</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">ToPILImage</span><span class="p">()(</span> <span class="n">display_obj</span> <span class="p">)</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">"RGB"</span><span class="p">)</span>

    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span> <span class="p">)</span>

    <span class="n">f_rows</span><span class="p">,</span> <span class="n">f_cols</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>

    <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span> <span class="n">f_rows</span><span class="p">,</span> <span class="n">f_cols</span><span class="p">,</span> <span class="mi">1</span> <span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span> <span class="n">left</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">bottom</span> <span class="o">=</span> <span class="kc">False</span> <span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">pil_image</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h4 id="method-...-viz_tens_list">
<a class="anchor" href="#method-...-viz_tens_list" aria-hidden="true"><span class="octicon octicon-link"></span></a>method ... <code>viz_tens_list</code><a class="anchor-link" href="#method-...-viz_tens_list"> </a>
</h4>
<p>args</p>
<ul>
<li>
<p><code>list_of_tensors</code></p>
</li>
<li>
<p><code>display_size = 6</code></p>
</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">viz_tens_list</span><span class="p">(</span> <span class="n">list_of_tensors</span><span class="p">,</span> <span class="n">display_size</span> <span class="o">=</span> <span class="mi">6</span> <span class="p">):</span>

    <span class="n">size</span> <span class="o">=</span> <span class="n">display_size</span>

    <span class="n">image_list</span> <span class="o">=</span> <span class="p">[</span> <span class="n">transforms</span><span class="o">.</span><span class="n">ToPILImage</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">"RGB"</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">list_of_tensors</span> <span class="p">]</span>

    <span class="n">tensor_list</span> <span class="o">=</span> <span class="p">[</span> <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()(</span><span class="n">image</span><span class="p">)</span> <span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">image_list</span><span class="p">]</span>

    <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span> <span class="n">tensor_list</span><span class="p">,</span> <span class="n">padding</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">pad_value</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="p">)</span>

    <span class="n">grid_pil</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">ToPILImage</span><span class="p">()(</span><span class="n">grid</span><span class="p">)</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">"RGB"</span><span class="p">)</span>

    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span> <span class="p">)</span>

    <span class="n">f_rows</span><span class="p">,</span> <span class="n">f_cols</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>

    <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span> <span class="n">f_rows</span><span class="p">,</span> <span class="n">f_cols</span><span class="p">,</span> <span class="mi">1</span> <span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span> <span class="n">left</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">bottom</span> <span class="o">=</span> <span class="kc">False</span> <span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">grid_pil</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="...-initializing-tensors">
<a class="anchor" href="#...-initializing-tensors" aria-hidden="true"><span class="octicon octicon-link"></span></a><font color="teal">... initializing tensors</font><a class="anchor-link" href="#...-initializing-tensors"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Tensors are similar to NumPy's <code>ndarrays</code>, except that tensors can run on GPUs or other hardware accelerators. Tensors and NumPy arrays can often share the same underlying memory, eliminating the need to copy data. Tensors are also optimized for automatic differentiation</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here are some ways they can be initialized in torch.</p>
<ol>
<li>
<p>from lists: $\quad$ <code>X_1 = torch.tensor( list_object )</code></p>
</li>
<li>
<p>from a numpy array $\quad$ <code>X_2 = torch.from_numpy( array_object )</code></p>
</li>
<li>
<p>with random or constant values, of a given shape. For example,</p>
<p>a. entries all ones: $\quad$ <code>X_3_a = torch.ones( shape )</code></p>
<p>b. entries all zeros $\quad$ <code>X_3_b = torch.zeros( shape )</code></p>
<p>c. entries are i.i.d. $\text{Unif}(0,1)$ $\quad$ <code>X_3_c = torch.rand( shape )</code></p>
<p>d. entries are i.i.d. standard normal $\quad$ <code>X_3_d = torch.randn( shape )</code></p>
<p>e. from values stored in memory $\quad$ <code>X_3_e = torch.Tensor( shape )</code></p>
<p>f. a list of consecutive integers between $N$ and $M$, inclusive, as tensor object $\quad$ <code>X_3_f = torch.arange(N,M)</code></p>
</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-1.---from-lists">
<a class="anchor" href="#...-1.---from-lists" aria-hidden="true"><span class="octicon octicon-link"></span></a>... 1.   from lists<a class="anchor-link" href="#...-1.---from-lists"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span> <span class="p">[</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span> <span class="p">]</span> <span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">X_1</span><span class="p">,</span> <span class="s2">"X1"</span><span class="p">)</span>
<span class="n">viz_tens</span><span class="p">(</span> <span class="mi">25</span> <span class="o">*</span> <span class="n">X_1</span> <span class="p">)</span> <span class="c1"># the factor there to help distinguish values</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     X1 

    num. dims        1
    num. entries     2
    shape            [2]


┌───┬───┐
│ 2 │ 3 │
└───┴───┘



</pre>
</div>
</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAABGCAYAAAAKCiBIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAA2klEQVR4nO3dsQ0CIBBAUTFOx/KMwRg4gXaGxP9ee80lP1dQMc45D3qetxfgDuGjhI8SPkr4KOGjXt+Ga62/fuvtvW+v8FNzzvFp5uKjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPmr4RrzJxUcJHyV8lPBRwkcJH/UGmigNh5tk3yoAAAAASUVORK5CYII=%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-2.-from-numpy">
<a class="anchor" href="#...-2.-from-numpy" aria-hidden="true"><span class="octicon octicon-link"></span></a>... 2. from numpy<a class="anchor-link" href="#...-2.-from-numpy"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span> <span class="p">[</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span> <span class="p">]</span> <span class="p">)</span> <span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">X_2</span><span class="p">,</span> <span class="s2">"'X two'"</span><span class="p">)</span>
<span class="n">viz_tens</span><span class="p">(</span> <span class="mi">25</span> <span class="o">*</span> <span class="n">X_2</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     'X two' 

    num. dims        1
    num. entries     3
    shape            [3]


┌───┬───┬───┐
│ 2 │ 3 │ 4 │
└───┴───┴───┘



</pre>
</div>
</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAAAzCAYAAABR5bw6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAsklEQVR4nO3csQ0DIRAAQWO5UwqmDMrgG7Cd/ks7k15y0uoCEsY550XP++4FuIfwUcJHCR8lfJTwUZ9/w7XW4956e++7V/jqiXvNOcevmYuPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4a/bJtcfJTwUcJHCR8lfJTwURc1fBBhoRLH+gAAAABJRU5ErkJggg==%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The next examples allow a shape to be provided as an argument.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">shape_dict</span> <span class="o">=</span> <span class="p">{}</span>

<span class="n">shape_dict</span><span class="p">[</span><span class="s2">"i"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span> <span class="mi">3</span> <span class="p">)</span>

<span class="n">shape_dict</span><span class="p">[</span><span class="s2">"ii"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span> <span class="p">)</span>

<span class="n">shape_dict</span><span class="p">[</span><span class="s2">"iii"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-3.-(a)-ones-of-given-shape">
<a class="anchor" href="#...-3.-(a)-ones-of-given-shape" aria-hidden="true"><span class="octicon octicon-link"></span></a>... 3. (a) ones of given shape<a class="anchor-link" href="#...-3.-(a)-ones-of-given-shape"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">shape_dict</span><span class="p">:</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">key</span>
    <span class="n">X_3_a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape_dict</span><span class="p">[</span><span class="n">s</span><span class="p">])</span>
    <span class="n">info</span><span class="p">(</span><span class="n">X_3_a</span><span class="p">,</span> <span class="s2">"'X three (a)'"</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s2">" "</span> <span class="o">+</span> <span class="n">s</span> <span class="o">+</span> <span class="s2">"'"</span> <span class="p">)</span>
    <span class="n">viz_tens</span><span class="p">(</span> <span class="mi">25</span> <span class="o">*</span> <span class="n">X_3_a</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     'X three (a) i' 

    num. dims        1
    num. entries     3
    shape            [3]


┌─────┬─────┬─────┐
│ 1.0 │ 1.0 │ 1.0 │
└─────┴─────┴─────┘



tensor     'X three (a) ii' 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌─────┬─────┬─────┐
│ 1.0 │ 1.0 │ 1.0 │
├─────┼─────┼─────┤
│ 1.0 │ 1.0 │ 1.0 │
└─────┴─────┴─────┘



tensor     'X three (a) iii' 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌─────┬─────┬─────┐
│ 1.0 │ 1.0 │ 1.0 │
├─────┼─────┼─────┤
│ 1.0 │ 1.0 │ 1.0 │
└─────┴─────┴─────┘



</pre>
</div>
</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAAAzCAYAAABR5bw6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAqElEQVR4nO3csQ2AMAwAQYIYIvtPly3CBNAi9Hetm0gvF24y9t4HPefXD+AbwkcJHyV8lPBRwkddb8O1llvvx+ac42lm46OEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svgo4aOEjxI+Svio4S/bJhsfJXyU8FHCRwkfJXzUDUneCmEkcikWAAAAAElFTkSuQmCC%0A">
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAA/UlEQVR4nO3WwQkDMQwAwTikCPdfnbvQVZB8TdiZrz6CRaA1My963rcX4A7ho4SPEj5K+KjPr+E5x8v/x/be69vMxUcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJH7Vm5vYOXODio4SPEj5K+Cjho4SPegDe4Qqr0aiDOwAAAABJRU5ErkJggg==%0A">
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAA/UlEQVR4nO3WwQkDMQwAwTikCPdfnbvQVZB8TdiZrz6CRaA1My963rcX4A7ho4SPEj5K+KjPr+E5x8v/x/be69vMxUcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJH7Vm5vYOXODio4SPEj5K+Cjho4SPegDe4Qqr0aiDOwAAAABJRU5ErkJggg==%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-3.-(b)-zeros-of-given-shape">
<a class="anchor" href="#...-3.-(b)-zeros-of-given-shape" aria-hidden="true"><span class="octicon octicon-link"></span></a>... 3. (b) zeros of given shape<a class="anchor-link" href="#...-3.-(b)-zeros-of-given-shape"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">shape_dict</span><span class="p">:</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">key</span>
    <span class="n">X_3_b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape_dict</span><span class="p">[</span><span class="n">s</span><span class="p">])</span>
    <span class="n">info</span><span class="p">(</span><span class="n">X_3_b</span><span class="p">,</span> <span class="s2">"'X three (a)'"</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s2">" "</span> <span class="o">+</span> <span class="n">s</span> <span class="o">+</span> <span class="s2">"'"</span> <span class="p">)</span>
    <span class="n">viz_tens</span><span class="p">(</span> <span class="mi">25</span> <span class="o">*</span> <span class="n">X_3_b</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     'X three (a) i' 

    num. dims        1
    num. entries     3
    shape            [3]


┌─────┬─────┬─────┐
│ 0.0 │ 0.0 │ 0.0 │
└─────┴─────┴─────┘



tensor     'X three (a) ii' 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌─────┬─────┬─────┐
│ 0.0 │ 0.0 │ 0.0 │
├─────┼─────┼─────┤
│ 0.0 │ 0.0 │ 0.0 │
└─────┴─────┴─────┘



tensor     'X three (a) iii' 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌─────┬─────┬─────┐
│ 0.0 │ 0.0 │ 0.0 │
├─────┼─────┼─────┤
│ 0.0 │ 0.0 │ 0.0 │
└─────┴─────┴─────┘



</pre>
</div>
</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAAAzCAYAAABR5bw6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAo0lEQVR4nO3cMQqAQAwAQSP+/8vxBdqK7Eyb5mBJkeZmdw96zq8fwDeEjxI+Svgo4aOEj7rehjPj1vux3Z2nmY2PEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho8Zftk02Pkr4KOGjhI8SPkr4qBuM0wphyOWPswAAAABJRU5ErkJggg==%0A">
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAA+ElEQVR4nO3WsQnEQAwAwdfj/luWK7DTw+xMqkSwCDS7+6Pnf3oBzhA+Svgo4aOEj7rehjPj5f+w3Z2nmYuPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5qdvf0Dhzg4qOEjxI+Svgo4aOEj7oBIeUKq3KMmaAAAAAASUVORK5CYII=%0A">
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAA+ElEQVR4nO3WsQnEQAwAwdfj/luWK7DTw+xMqkSwCDS7+6Pnf3oBzhA+Svgo4aOEj7rehjPj5f+w3Z2nmYuPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5qdvf0Dhzg4qOEjxI+Svgo4aOEj7oBIeUKq3KMmaAAAAAASUVORK5CYII=%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-3.-(c)-i.i.d.-uniform-of-given-shape">
<a class="anchor" href="#...-3.-(c)-i.i.d.-uniform-of-given-shape" aria-hidden="true"><span class="octicon octicon-link"></span></a>... 3. (c) i.i.d. uniform of given shape<a class="anchor-link" href="#...-3.-(c)-i.i.d.-uniform-of-given-shape"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">shape_dict</span><span class="p">:</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">key</span>
    <span class="n">X_3_c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">shape_dict</span><span class="p">[</span><span class="n">s</span><span class="p">])</span>
    <span class="n">info</span><span class="p">(</span><span class="n">X_3_c</span><span class="p">,</span> <span class="s2">"X 3 (c) "</span> <span class="o">+</span> <span class="s2">" "</span> <span class="o">+</span> <span class="n">s</span> <span class="p">)</span>
    <span class="n">viz_tens</span><span class="p">(</span> <span class="mi">25</span> <span class="o">*</span> <span class="n">X_3_c</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     X 3 (c)  i 

    num. dims        1
    num. entries     3
    shape            [3]


┌──────┬──────┬──────┐
│ 0.31 │ 0.62 │ 0.73 │
└──────┴──────┴──────┘



tensor     X 3 (c)  ii 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌──────┬──────┬──────┐
│ 0.43 │ 0.30 │ 0.77 │
├──────┼──────┼──────┤
│ 0.10 │ 0.81 │ 0.30 │
└──────┴──────┴──────┘



tensor     X 3 (c)  iii 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌──────┬──────┬──────┐
│ 0.50 │ 0.40 │ 0.56 │
├──────┼──────┼──────┤
│ 0.34 │ 0.86 │ 0.48 │
└──────┴──────┴──────┘



</pre>
</div>
</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAAAzCAYAAABR5bw6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAs0lEQVR4nO3csQ0DIRAAQWO5/07I6Ioi+AZsp//SzqSXnLS6gIRxznnR8757Ae4hfJTwUcJHCR8lfNTn33Dv/bi33lrr7hW+euJec87xa+bio4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+Cjho4SPEj5K+KjhL9smFx8lfJTwUcJHCR8lfNQFvK0QYWkLuqsAAAAASUVORK5CYII=%0A">
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABJ0lEQVR4nO3csW2EQBRFUWNteZRIDr3QAgmVjAvweuVNPJbuOekkT7r6EhHLGOODns/ZA5hD+Cjho4SPEj7q8erxvu9/98m/7/vsCU9d1zV7wjfbti0/vbn4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPurlz4+O4/irHb92nufsCU+t6zp7wltcfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfJTwUcJHCR8lfNQyxpi9gQlcfJTwUcJHCR8lfJTwUV/XbBXqvW4F4QAAAABJRU5ErkJggg==%0A">
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABK0lEQVR4nO3csW2EQBBAUWM5okOqu4gCKIUKaIMMx9bdke7J/710k5G+Rtpopuu6vuj5Hj0AYwgfJXyU8FHCR/3cPa7r+nFf/mVZRo/w0nmeo0d4Ms/z9O7NxkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBR091J023bPu740b7vo0d46fF4jB7hyXEcjh/xl/BRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV81O3VK/4vGx8lfJTwUcJHCR8lfNQvrVYap4dmmBYAAAAASUVORK5CYII=%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-3.-(d)-i.i.d.-standard-normal-of-given-shape">
<a class="anchor" href="#...-3.-(d)-i.i.d.-standard-normal-of-given-shape" aria-hidden="true"><span class="octicon octicon-link"></span></a>... 3. (d) i.i.d. standard normal of given shape<a class="anchor-link" href="#...-3.-(d)-i.i.d.-standard-normal-of-given-shape"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">shape_dict</span><span class="p">:</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">key</span>
    <span class="n">X_3_d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape_dict</span><span class="p">[</span><span class="n">s</span><span class="p">])</span>
    <span class="n">info</span><span class="p">(</span><span class="n">X_3_d</span><span class="p">,</span> <span class="s2">"X 3 (d)"</span> <span class="o">+</span> <span class="s2">" "</span> <span class="o">+</span> <span class="n">s</span> <span class="p">)</span>
    <span class="n">viz_tens</span><span class="p">(</span> <span class="mi">25</span> <span class="o">*</span> <span class="n">X_3_d</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     X 3 (d) i 

    num. dims        1
    num. entries     3
    shape            [3]


┌──────┬──────┬──────┐
│ 0.91 │ 0.84 │ -0.0 │
└──────┴──────┴──────┘



tensor     X 3 (d) ii 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌──────┬──────┬──────┐
│ 0.57 │ -0.9 │ -0.9 │
├──────┼──────┼──────┤
│ -0.2 │ -0.9 │ 1.85 │
└──────┴──────┴──────┘



tensor     X 3 (d) iii 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌──────┬──────┬──────┐
│ -0.2 │ 0.25 │ 1.49 │
├──────┼──────┼──────┤
│ 0.53 │ -0.1 │ 0.34 │
└──────┴──────┴──────┘



</pre>
</div>
</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAAAzCAYAAABR5bw6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAuElEQVR4nO3cuQ3DMBAAQdNwOyxHxaoA1UUX4CelgJ1JLzlgcQETjrXWg57n7gXYQ/go4aOEjxI+Svio17/heZ63e+sdx7F7ha+u69q9woc55/g1c/FRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV81PCXbZOLjxI+Svgo4aOEjxI+6g2sng1hoDkvEQAAAABJRU5ErkJggg==%0A">
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABK0lEQVR4nO3csW2EQBBAUWO5KZoiuK7IaIBK6IMUx9bdke7J/710k5G+Rtpopuu6vuj5Hj0AYwgfJXyU8FHCR/3cPS7L8nFf/nVdR4/w0rZto0d4Ms/z9O7NxkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBR091J0+M4Pu740ePxGD3CS+d5jh7hyb7vjh/xl/BRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV81O3VK/4vGx8lfJTwUcJHCR8lfNQvTl4ap2sIn/cAAAAASUVORK5CYII=%0A">
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABJ0lEQVR4nO3csWnEQBRFUctsqkTtqTU1MP2on3EBXi92smO456STPLh8UKRtzvlBz+fqAawhfJTwUcJHCR/1ePV4Xde/++QfY6ye8NRxHKsnfDPG2H56c/FRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV8lPBRwkcJHyV81MufH933/a4dv3ae5+oJT+37vnrCn7j4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4qG3OuXoDC7j4KOGjhI8SPkr4KOGjvgDNEhFhIv9kaAAAAABJRU5ErkJggg==%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-3.-(f)-sequence">
<a class="anchor" href="#...-3.-(f)-sequence" aria-hidden="true"><span class="octicon octicon-link"></span></a>... 3. (f) sequence<a class="anchor-link" href="#...-3.-(f)-sequence"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_3_f</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">)</span> 
<span class="n">info</span><span class="p">(</span><span class="n">X_3_f</span><span class="p">,</span> <span class="s2">"X 3 (f)"</span><span class="p">)</span>
<span class="n">viz_tens</span><span class="p">(</span> <span class="mi">25</span> <span class="o">*</span> <span class="n">X_3_f</span> <span class="p">)</span> <span class="c1"># note the modular arith. being performed automatically</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     X 3 (f) 

    num. dims        1
    num. entries     10
    shape            [10]


┌────┬────┬────┬────┬────┬────┬────┬────┬────┬────┐
│ 10 │ 11 │ 12 │ 13 │ 14 │ 15 │ 16 │ 17 │ 18 │ 19 │
└────┴────┴────┴────┴────┴────┴────┴────┴────┴────┘



</pre>
</div>
</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAAAZCAYAAAD30ppqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAAf0lEQVRoge3awQnAIBAAwVwIXP/t2cCVcakgwTzEwO58FRUWH4LR3Yd4zt0H0B6GhzI8lOGhDA9leKjrbTAzp996VTW96Ze5K9f+w9yVa48x4mnMGw9leCjDQxkeyvBQhocyPJThoQwPZXio8AcOkzceyvBQhocyPJThoQwPdQM4ZCIt7yfBxgAAAABJRU5ErkJggg==%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="numpy-objects-and-tensors">
<a class="anchor" href="#numpy-objects-and-tensors" aria-hidden="true"><span class="octicon octicon-link"></span></a><font color="teal">numpy objects and tensors</font><a class="anchor-link" href="#numpy-objects-and-tensors"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Tensors can be converted to numpy arrays, and numpy arrays back to tensors.
To transform a numpy array into a tensor, we can use the function <code>torch.from_numpy</code>, and we use <code>np.array</code> for the other direction.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The conversion of tensors to numpy require the tensor to be on the CPU, and not the GPU.</p>
<p>In case you have a tensor on GPU, you need to call <code>.cpu()</code> on the tensor beforehand.
Hence, you get a line like <code>np_arr = tensor.cpu().numpy()</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"t: </span><span class="si">{</span><span class="n">t</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"n: </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>t: tensor([1., 1., 1., 1., 1.])
n: [1. 1. 1. 1. 1.]
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">t</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"t: </span><span class="si">{</span><span class="n">t</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"n: </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>t: tensor([2., 2., 2., 2., 2.])
n: [2. 2. 2. 2. 2.]
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="...-tensor-operations">
<a class="anchor" href="#...-tensor-operations" aria-hidden="true"><span class="octicon octicon-link"></span></a><font color="teal">... tensor operations</font><a class="anchor-link" href="#...-tensor-operations"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Most operations existing in numpy also exist in PyTorch. A full list of operations can be found in the <a href="https://pytorch.org/docs/stable/tensors.html#">PyTorch documentation</a>.</p>
<ul>
<li>
<p>Each torch operation can be run on the GPU.</p>
</li>
<li>
<p>By default, tensors are created on the CPU. Unless we are using a package like <code>pytorch-lightning</code>, we need to explicitly move tensors to the GPU using the <code>.to</code> method, after checking GPU availability.</p>
</li>
<li>
<p>Copying large tensors across devices can be expensive in terms of time and memory.</p>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-adding-tensors">
<a class="anchor" href="#...-adding-tensors" aria-hidden="true"><span class="octicon octicon-link"></span></a>... adding tensors<a class="anchor-link" href="#...-adding-tensors"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_1</span><span class="p">,</span> <span class="n">X_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">viz_tens_list</span><span class="p">(</span> <span class="p">[</span> <span class="n">X_1</span><span class="p">,</span> <span class="n">X_2</span> <span class="p">]</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAV0AAAC1CAYAAAD86CzsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAC/UlEQVR4nO3asW3CYBhF0ThySY0YwwuwCROwCyPQMQ2VW0ahchYIoDTXKJzTfsVzdfUXHpZl+QKg8b32BwB8EtEFCIkuQEh0AUKiCxAaX9z92gDwd8Ojg5cuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAoXHtD1jTdrtN9263W7q32+3Svfv9nu6V5nlO9y6XS7p3OBzSvWma0r134qULEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoTGtT9gTdfrNd07nU7p3vF4TPf+s81mk+6dz+d0b7/fp3vTNKV778RLFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIDcuyPLs/PQLwq+HRwUsXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2A0PjiPiRfAfAhvHQBQqILEBJdgJDoAoREFyAkugChH2q5HfH36JPyAAAAAElFTkSuQmCC%0A">
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Y</span> <span class="o">=</span> <span class="n">X_1</span> <span class="o">+</span> <span class="n">X_2</span>
<span class="n">viz_tens</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> 
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH4AAABYCAYAAAAz1kOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABLElEQVR4nO3cu22EUBRFUWO5GWiFdohpk4yMOnAB89FEfpb2WulLjrR1JSKm+76/6PkePYAxhI8SPkr4KOGjft49Xtf17z75930fPeGpbdtGT3gwz/P06s3FRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FHCRwkfJXyU8FFvf350HMdf7fjYsiyjJzy1ruvoCQ/O83z55uKjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjhI8SPkr4KOGjpvu+R29gABcfJXyU8FHCRwkfJXzUL5y0FCs2rBfAAAAAAElFTkSuQmCC%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-stacking-tensors">
<a class="anchor" href="#...-stacking-tensors" aria-hidden="true"><span class="octicon octicon-link"></span></a>... stacking tensors<a class="anchor-link" href="#...-stacking-tensors"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_1</span><span class="p">,</span> <span class="n">X_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">15</span><span class="p">)</span>

<span class="n">info</span><span class="p">(</span><span class="n">X_1</span><span class="p">,</span> <span class="s2">"X 1"</span><span class="p">)</span>

<span class="n">info</span><span class="p">(</span><span class="n">X_2</span><span class="p">,</span> <span class="s2">"X 2"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     X 1 

    num. dims        1
    num. entries     5
    shape            [5]


┌───┬───┬───┬───┬───┐
│ 5 │ 6 │ 7 │ 8 │ 9 │
└───┴───┴───┴───┴───┘



tensor     X 2 

    num. dims        1
    num. entries     5
    shape            [5]


┌────┬────┬────┬────┬────┐
│ 10 │ 11 │ 12 │ 13 │ 14 │
└────┴────┴────┴────┴────┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">X_1</span><span class="p">,</span> <span class="n">X_2</span><span class="p">],</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="s2">"Y"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     Y 

    num. dims        2
    num. entries     10
    shape            [2 5]


┌────┬────┬────┬────┬────┐
│ 5  │ 6  │ 7  │ 8  │ 9  │
├────┼────┼────┼────┼────┤
│ 10 │ 11 │ 12 │ 13 │ 14 │
└────┴────┴────┴────┴────┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-in-place-operations">
<a class="anchor" href="#...-in-place-operations" aria-hidden="true"><span class="octicon octicon-link"></span></a>... in-place operations<a class="anchor-link" href="#...-in-place-operations"> </a>
</h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Operations that store the result into the operand are called <em>in-place</em>. They are  usually marked with a underscore postfix, e.g. "<code>add_</code>" instead of "<code>add</code>". The operation <code>X.copy_(Y)</code> will change <code>X</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Calling <code>x1 + x2</code> creates a new tensor containing the sum of the two inputs.
However, we can also use in-place operations that are applied directly on the memory of a tensor.
We therefore change the values of <code>x2</code> without the chance to re-accessing the values of <code>x2</code> before the operation.
An example is shown below:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_1</span><span class="p">,</span> <span class="n">X_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\t</span><span class="s2">"</span><span class="p">,</span><span class="s2">"before"</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">X_1</span><span class="p">,</span> <span class="s2">"'X one'"</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">X_2</span><span class="p">,</span> <span class="s2">"'X two'"</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n\n</span><span class="s2">"</span><span class="p">)</span>

<span class="n">X_2</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">X_1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\t</span><span class="s2">"</span><span class="p">,</span><span class="s2">"after"</span><span class="p">,</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">X_1</span><span class="p">,</span> <span class="s2">"'X one'"</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">X_2</span><span class="p">,</span> <span class="s2">"'X two'"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>	 before 

tensor     'X one' 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌──────┬──────┬──────┐
│ 0.59 │ 0.60 │ 0.57 │
├──────┼──────┼──────┤
│ 0.63 │ 0.25 │ 0.43 │
└──────┴──────┴──────┘



tensor     'X two' 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌──────┬──────┬──────┐
│ 0.97 │ 0.83 │ 0.48 │
├──────┼──────┼──────┤
│ 0.02 │ 0.52 │ 0.15 │
└──────┴──────┴──────┘






	 after 

tensor     'X one' 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌──────┬──────┬──────┐
│ 0.59 │ 0.60 │ 0.57 │
├──────┼──────┼──────┤
│ 0.63 │ 0.25 │ 0.43 │
└──────┴──────┴──────┘



tensor     'X two' 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌──────┬──────┬──────┐
│ 1.56 │ 1.44 │ 1.05 │
├──────┼──────┼──────┤
│ 0.66 │ 0.78 │ 0.59 │
└──────┴──────┴──────┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>In-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss of history. Hence, their use is discouraged.</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-reshaping-tensors">
<a class="anchor" href="#...-reshaping-tensors" aria-hidden="true"><span class="octicon octicon-link"></span></a>... reshaping tensors<a class="anchor-link" href="#...-reshaping-tensors"> </a>
</h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Another common operation aims at changing the shape of a tensor.
A tensor of size <code>(2,3)</code> can be re-organized to any other shape with the same number of elements (e.g. a tensor of size <code>(6)</code>, or <code>(3,2)</code>, ...).</p>
<p>In PyTorch, this reshaping operation is called <code>view</code>:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="s2">"X"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     X 

    num. dims        1
    num. entries     6
    shape            [6]


┌───┬───┬───┬───┬───┬───┐
│ 0 │ 1 │ 2 │ 3 │ 4 │ 5 │
└───┴───┴───┴───┴───┴───┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="s2">"X"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     X 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌───┬───┬───┐
│ 0 │ 1 │ 2 │
├───┼───┼───┤
│ 3 │ 4 │ 5 │
└───┴───┴───┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-transposing-tensors-(permuting-dimensions)">
<a class="anchor" href="#...-transposing-tensors-(permuting-dimensions)" aria-hidden="true"><span class="octicon octicon-link"></span></a>... transposing tensors (permuting dimensions)<a class="anchor-link" href="#...-transposing-tensors-(permuting-dimensions)"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="s2">"X with 0th and 1st dimensions permuted"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     X with 0th and 1st dimensions permuted 

    num. dims        2
    num. entries     6
    shape            [3 2]


┌───┬───┐
│ 0 │ 3 │
├───┼───┤
│ 1 │ 4 │
├───┼───┤
│ 2 │ 5 │
└───┴───┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span>
<span class="n">info</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="s2">"X transposed again"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     X transposed again 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌───┬───┬───┐
│ 0 │ 1 │ 2 │
├───┼───┼───┤
│ 3 │ 4 │ 5 │
└───┴───┴───┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-numpy-like-indexing-and-slicing">
<a class="anchor" href="#...-numpy-like-indexing-and-slicing" aria-hidden="true"><span class="octicon octicon-link"></span></a>... numpy-like indexing and slicing<a class="anchor-link" href="#...-numpy-like-indexing-and-slicing"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span> <span class="mi">4</span><span class="p">,</span><span class="mi">4</span> <span class="p">)</span>
<span class="n">info</span><span class="p">(</span> <span class="n">X</span><span class="p">,</span> <span class="s2">"X"</span> <span class="p">)</span>
<span class="n">info</span><span class="p">(</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">"first row of X"</span> <span class="p">)</span>
<span class="n">info</span><span class="p">(</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="s2">"first column of X"</span> <span class="p">)</span>
<span class="n">info</span><span class="p">(</span> <span class="n">X</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s2">"last column of X"</span> <span class="p">)</span>
<span class="n">info</span><span class="p">(</span> <span class="n">X</span><span class="p">[:</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s2">"First two rows, last column"</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span> <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">,</span> <span class="p">:],</span> <span class="s2">"Middle two rows"</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     X 

    num. dims        2
    num. entries     16
    shape            [4 4]


┌──────┬──────┬──────┬──────┐
│ 0.90 │ 0.19 │ 0.46 │ 0.38 │
├──────┼──────┼──────┼──────┤
│ 0.58 │ 0.97 │ 0.54 │ 0.78 │
├──────┼──────┼──────┼──────┤
│ 0.88 │ 0.90 │ 0.32 │ 0.38 │
├──────┼──────┼──────┼──────┤
│ 0.74 │ 0.36 │ 0.73 │ 0.39 │
└──────┴──────┴──────┴──────┘



tensor     first row of X 

    num. dims        1
    num. entries     4
    shape            [4]


┌──────┬──────┬──────┬──────┐
│ 0.90 │ 0.19 │ 0.46 │ 0.38 │
└──────┴──────┴──────┴──────┘



tensor     first column of X 

    num. dims        1
    num. entries     4
    shape            [4]


┌──────┬──────┬──────┬──────┐
│ 0.90 │ 0.58 │ 0.88 │ 0.74 │
└──────┴──────┴──────┴──────┘



tensor     last column of X 

    num. dims        1
    num. entries     4
    shape            [4]


┌──────┬──────┬──────┬──────┐
│ 0.38 │ 0.78 │ 0.38 │ 0.39 │
└──────┴──────┴──────┴──────┘



tensor     First two rows, last column 

    num. dims        1
    num. entries     2
    shape            [2]


┌──────┬──────┐
│ 0.38 │ 0.78 │
└──────┴──────┘



tensor     Middle two rows 

    num. dims        2
    num. entries     8
    shape            [2 4]


┌──────┬──────┬──────┬──────┐
│ 0.58 │ 0.97 │ 0.54 │ 0.78 │
├──────┼──────┼──────┼──────┤
│ 0.88 │ 0.90 │ 0.32 │ 0.38 │
└──────┴──────┴──────┴──────┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">info</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="s2">"modified X"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     modified X 

    num. dims        2
    num. entries     16
    shape            [4 4]


┌──────┬─────┬──────┬──────┐
│ 0.90 │ 0.0 │ 0.46 │ 0.38 │
├──────┼─────┼──────┼──────┤
│ 0.58 │ 0.0 │ 0.54 │ 0.78 │
├──────┼─────┼──────┼──────┤
│ 0.88 │ 0.0 │ 0.32 │ 0.38 │
├──────┼─────┼──────┼──────┤
│ 0.74 │ 0.0 │ 0.73 │ 0.39 │
└──────┴─────┴──────┴──────┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="...-other-operations">
<a class="anchor" href="#...-other-operations" aria-hidden="true"><span class="octicon octicon-link"></span></a>... other operations<a class="anchor-link" href="#...-other-operations"> </a>
</h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here are some ways to perform matrix multiplication:</p>
<ul>
<li>
<p><code>torch.matmul</code> $\quad$ Performs the matrix product over two tensors, where the specific behavior depends on the dimensions.
If both inputs are matrices (2-dimensional tensors), it performs the standard matrix product.
For higher dimensional inputs, the function supports broadcasting (for details see the <a href="https://pytorch.org/docs/stable/generated/torch.matmul.html?highlight=matmul#torch.matmul">documentation</a>).</p>
<p>It can also be written as <code>a @ b</code>, similar to numpy.</p>
</li>
<li>
<p><code>torch.mm</code> $\quad$ Performs the matrix product over two matrices, but doesn't support broadcasting (see <a href="https://pytorch.org/docs/stable/generated/torch.mm.html?highlight=torch%20mm#torch.mm">documentation</a>)</p>
</li>
<li>
<p><code>torch.bmm</code> $\quad$ Performs the matrix product with a support batch dimension. Let <code>T</code> be a tensor of shape <code>(b,  n, m)</code>, and <code>R</code> a tensor of shape <code>(b, m, p)</code>, the output tensor is of shape <code>(b, n , p)</code>, obtained by "entry-wise" matrix multiplication along the batch dimension.</p>
</li>
<li>
<p><code>torch.einsum</code> $\quad$ Performs matrix multiplications and more (i.e. sums of products) using the Einstein summation convention.</p>
</li>
</ul>
<p>Usually, we use <code>torch.matmul</code> or <code>torch.bmm</code>.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">9</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="n">info</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="s2">"X"</span><span class="p">)</span>

<span class="n">info</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="s2">"Y"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     X 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌───┬───┬───┐
│ 0 │ 1 │ 2 │
├───┼───┼───┤
│ 3 │ 4 │ 5 │
└───┴───┴───┘



tensor     Y 

    num. dims        2
    num. entries     9
    shape            [3 3]


┌───┬───┬───┐
│ 0 │ 1 │ 2 │
├───┼───┼───┤
│ 3 │ 4 │ 5 │
├───┼───┼───┤
│ 6 │ 7 │ 8 │
└───┴───┴───┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="s2">"Z"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     Z 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌────┬────┬────┐
│ 15 │ 18 │ 21 │
├────┼────┼────┤
│ 42 │ 54 │ 66 │
└────┴────┴────┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Given a tensor <code>X</code>, the tensors <code>Y_1</code>,<code>Y_2</code>, <code>Y_3</code> computed below all have the same value:</p>

<pre><code>Y_1 = X @ X.T
Y_2 = X.matmul(X.T)
Y_3 = torch.rand_like(X)
torch.matmul(X, X.T, out = Y_3)</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>On the other hand, <code>*</code> denotes the entrywise product of two tensors.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="s2">"X"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     X 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌───┬───┬───┐
│ 0 │ 1 │ 2 │
├───┼───┼───┤
│ 3 │ 4 │ 5 │
└───┴───┴───┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">info</span><span class="p">(</span> <span class="n">X</span> <span class="o">*</span> <span class="n">X</span><span class="p">,</span> <span class="s2">"(a)"</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span> <span class="n">X</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="s2">"(b)"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     (a) 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌───┬────┬────┐
│ 0 │ 1  │ 4  │
├───┼────┼────┤
│ 9 │ 16 │ 25 │
└───┴────┴────┘



tensor     (b) 

    num. dims        2
    num. entries     6
    shape            [2 3]


┌───┬────┬────┐
│ 0 │ 1  │ 4  │
├───┼────┼────┤
│ 9 │ 16 │ 25 │
└───┴────┴────┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You can use <code>torch.cat</code> to concatenate a sequence of tensors along a given dimension. See also <code>torch.stack</code>, another tensor joining op that is subtly different from <code>torch.cat</code></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span> <span class="p">[</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">],</span> <span class="n">dim</span> <span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">info</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="s2">"Y"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor     Y 

    num. dims        2
    num. entries     18
    shape            [2 9]


┌───┬───┬───┬───┬───┬───┬───┬───┬───┐
│ 0 │ 1 │ 2 │ 0 │ 1 │ 2 │ 0 │ 1 │ 2 │
├───┼───┼───┼───┼───┼───┼───┼───┼───┤
│ 3 │ 4 │ 5 │ 3 │ 4 │ 5 │ 3 │ 4 │ 5 │
└───┴───┴───┴───┴───┴───┴───┴───┴───┘



</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If you have a one-element tensor, for example obtained by aggregating all values of a given tensor into a single value, you can convert it to a Python numerical value using <code>item()</code>:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">agg</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">agg_item</span> <span class="o">=</span> <span class="n">agg</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> 
<span class="nb">print</span><span class="p">(</span><span class="n">agg_item</span><span class="p">,</span> <span class="s2">"</span><span class="se">\t</span><span class="s2">"</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">agg_item</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>15 	 &lt;class 'int'&gt;
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="2.2-...-one-dimensional-convolutions-of-scalar-valued-signals">
<a class="anchor" href="#2.2-...-one-dimensional-convolutions-of-scalar-valued-signals" aria-hidden="true"><span class="octicon octicon-link"></span></a><font color="CornflowerBlue">2.2 ... one-dimensional convolutions of scalar-valued signals</font><a class="anchor-link" href="#2.2-...-one-dimensional-convolutions-of-scalar-valued-signals"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We now consider one of the simplest possible settings for learning, in which the underlying domain is a one-dimensional grid, and where signals over the domain have only a single channel.</p>
<p>In this case, the signal domain is a group itself, the cyclic group of order $n$, 
$$
C_n = \langle \, a : a^n = 1 \, \rangle \equiv \{ \, 1, a, a^2, \dots, a^{n-1} \, \}.
$$
It is convenient to parametrize the group, and hence the grid, through the exponent of the generator 
$$
C_n \equiv \{ 0, 1, \dots, n -1 \}
$$
as this indexing is consistent with the way most python indexes vectors. In this setting, the group operation may be reinterpreted as addition modulo $n$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As the input domain is fixed, it feels natural to consider the GDL group $G$ to be $C_n$ as well.</p>
<p>We suppose that signals are scalar-valued, and are encoded in the natural basis, so that each $x \in \mathcal{X}(C_n, \mathbb{R})$ may be expressed as</p>
$$
\mathcal{X}(C_n,\mathbb{R}) = \{ x : C_n \to \mathbb{R} \} ,
$$<p>is finite dimensional, and each $x \in \mathcal{X}(C_n, \mathbb{R})$ may be expressed as 
$$
x = 
\left[ 
\begin{matrix}
x_0\\ 
\vdots\\
\,x_{n-1}\,
\end{matrix}
\right] 
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>With this basis, we can describe the representation $\rho$ of $G \equiv C_n$ concretely, as a matrix.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Given a vector $\theta = (\theta_0 , \dots, \theta_{n-1})$, recall the associated <font color="purple">_circulant matrix_</font> is the $n \times n$ matrix with entries 
$$
S(\theta) := \left( \, \theta_{ (u - v) \mod n} \right)_{ 0 \, \leq \,u,\,v \, \leq n-1 } 
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the specific case of $\theta_{+} := (0,1,0,\dots, 0)^T$, the associated circulant matrix, $S_{+} := S(\theta_{+})$ acts on vectors by shifting the entries of vectors to the right by one position, corresponding to addition by one, modulo $n$.</p>
<p>We call $S_+$ the <em><font color="purple">(right) shift operator</font></em>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>Lemma</strong> $\quad$
A matrix is circulant if and only if it commutes with $S_+$. Moreover, given any two vectors $\theta, \eta \in \mathbb{R}^n$, one has $S(\theta) S(\eta) = S(\eta) S(\theta)$.</p>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The importance of $S_+$ to the present discussion is that it generates a group isomorphic to the one-dimensional translation group $C_n$; the matrices $\{ I, S_+, S_+^2, \dots, S_+^{n-1} \}$ constitute a faithful representation of $C_n$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Circulant matrices are synonymous with discrete convolutions; given $x, \theta \in \mathcal{X}(\Omega,\mathbb{R}) \equiv \mathbb{R}^n$, their <em>convolution</em> $x \star \theta$ is defined by 
$$
( x \star \theta )_u := \sum_{v = 0}^{n-1} x_{v \mod n}\, \theta_{ (u-v) \mod n} \equiv S(\theta) x 
$$</p>
<p>Thus, the next corollary effectively follows from the much stronger theorem stated at the end of <a href="https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2021/10/21/GDL1.html#1.3-...-equivariance-in-neural-networks">section 1.3</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>Corollary</strong> $\quad$ Any $f : \mathcal{X}(C_n, \mathbb{R}) \to \mathcal{X}(C_n,\mathbb{R})$ which is linear and $C_n$-equivariant can be expressed (in the input coordinate system) as an $n \times n$ circulant matrix $S(\theta)$ for some vector $\theta$.</p>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<hr>
<h3 id="...example:-local-averaging-as-a-circulant-matrix">
<a class="anchor" href="#...example:-local-averaging-as-a-circulant-matrix" aria-hidden="true"><span class="octicon octicon-link"></span></a><font color="teal">...example: local averaging as a circulant matrix</font><a class="anchor-link" href="#...example:-local-averaging-as-a-circulant-matrix"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>$\quad$ Recall a <a href="https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2021/10/21/GDL1.html#...-example:-permutations-and-local-averaging">previous recipe</a> for an equivariant function $F= \Phi( X, A)$ using a local aggregation function $\varphi$.</p>
<p>In our present case of $\Omega \equiv G \equiv C_n$, we may write this local aggregation more concretely as
$$
\varphi ( x_u, X_{\textsf{nbhd}(u)} ) = \varphi( x_{u-1}, \, x_u, \, x_{u+1} ),
$$
with addition and subtraction in the indices above understood to be modulo $n$.</p>
<p>If in addition, we insist that $\varphi$ is linear, then it has the form 
$$
 \varphi( x_{u-1}, \, x_u, \, x_{u+1} ) = \theta_{-1} x_{u-1} + \theta_0 x_u + \theta_1 x_{u+1},
$$
and in this case we can express $F = \Phi (X, A )$ through the following matrix multiplication:
$$
\left[
\begin{matrix}
\theta_0 &amp; \theta_1 &amp; \text{ } &amp; \text{ } &amp; \theta_{-1} \\
\theta_{-1} &amp; \theta_0 &amp; \theta_1 &amp; \text{ } &amp;   \text{ } \\
\text{} &amp; \ddots &amp; \ddots &amp; \ddots &amp; \text{ } \\
\text{ } &amp; \text{ } &amp; \theta_{-1} &amp; \theta_0 &amp; \theta_1 \\
\theta_1 &amp; \text{ } &amp; \text{ } &amp; \theta_{-1} &amp; \theta_0 
\end{matrix} 
\right]
\left[
\begin{matrix}
x_0 \\
x_1 \\
\vdots \\
\,x_{n-2} \, \\
x_{n-1}  
\end{matrix}
\right]
$$
This multi-diagonal structure is often synonymous with the concept of weight sharing in ML literature.</p>
<hr>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="2.3-...-one-dimensional-convolutions-in-torch">
<a class="anchor" href="#2.3-...-one-dimensional-convolutions-in-torch" aria-hidden="true"><span class="octicon octicon-link"></span></a><font color="CornflowerBlue">2.3 ... one-dimensional convolutions in torch</font><a class="anchor-link" href="#2.3-...-one-dimensional-convolutions-in-torch"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The object in <code>torch</code> for executing convolutions of signals over $C_n$ is called <code>torch.nn.Conv1d</code>. We'll denote an instance of this object by $\tilde{B}$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h4 id="class-...-torch.nn.Conv1d">
<a class="anchor" href="#class-...-torch.nn.Conv1d" aria-hidden="true"><span class="octicon octicon-link"></span></a>class ... <code>torch.nn.Conv1d</code><a class="anchor-link" href="#class-...-torch.nn.Conv1d"> </a>
</h4>
<table>
<thead>
<tr>
<th>args</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>in_channels</code></td>
<td><code>out_channels</code></td>
<td><code>kernel_size</code></td>
<td><code>stride = 1</code></td>
</tr>
<tr>
<td><code>padding = 0</code></td>
<td><code>dilation = 1</code></td>
<td><code>groups = 1</code></td>
<td><code>bias = True</code></td>
</tr>
<tr>
<td><code>padding_mode = 'zeros'</code></td>
<td><code>device = None</code></td>
<td><code>dtype = None</code></td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let us now relate the shapes of the input and output to the parameters</p>
<table>
<thead>
<tr>
<th>input parameter</th>
<th>LaTeX symbol</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>in_channels</code></td>
<td>$\text{dim}(\mathcal{C})$</td>
</tr>
<tr>
<td><code>out_channels</code></td>
<td>$\text{dim}(\tilde{\mathcal{C}})$</td>
</tr>
<tr>
<td><code>kernel_size</code></td>
<td>$k$</td>
</tr>
<tr>
<td><code>stride</code></td>
<td>$\lambda$</td>
</tr>
<tr>
<td><code>padding</code></td>
<td>$\rho$</td>
</tr>
<tr>
<td><code>dilation</code></td>
<td>$\delta$</td>
</tr>
<tr>
<td><code>groups</code></td>
<td>$M$        </td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This class specifies a <a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html">one-dimensional convolution operation</a>.  Because of batching and the channel dimension, the input and output are $3$-tensors. We use $N$ to denote the batch size, and we will continue to use $n$ for the input length.</p>
<p>In general, $\mathcal{C}$ and $\mathcal{C}_1$ are the vector spaces of input channels and output channels respectively. In our current setting, assuming that signals $x$ are scalar signals over the cyclic group, one has $\textrm{dim}(\mathcal{C}) = 1$. The shape of the input tensor is thus</p>
$$
(N, 1, n)
$$<p>Even though we restrict ourselves to scalar-valued input signals, we will allow $\textrm{dim}(\tilde{\mathcal{C}}) &gt; 1$, and we will write $\tilde{n}$ to denote the length of the output, so that the shape of the output tensor is</p>
$$
(N, \textrm{dim}(\tilde{\mathcal{C}}), \tilde{n})
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For the moment, we will also take $N=1$. Thus, inputs can be though of effectively as vectors, which we can easily visualize. We will think of the output as a length-$\textrm{dim}(\tilde{\mathcal{C}})$ list of vectors (with scalar entries).</p>
<p>Our present goal is to try to understand how the number of learnable parameters depends on these shapes, as well as to visualize the effect of a convolutional layer in this simplest setting.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First, we remark that the relationship between $n_1$ and $n$, in terms of the input parameters, can be expressed as follows:</p>
$$
\tilde{n} = \left\lfloor \frac{ n + 2 \rho - \delta (k-1) -1 }{\lambda} + 1 \right\rfloor
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h3 id="method-...-compute_num_out_channels">
<a class="anchor" href="#method-...-compute_num_out_channels" aria-hidden="true"><span class="octicon octicon-link"></span></a>method ... <code>compute_num_out_channels</code><a class="anchor-link" href="#method-...-compute_num_out_channels"> </a>
</h3>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">compute_num_out_channels</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span>
                             <span class="n">padding</span><span class="p">,</span>
                             <span class="n">dilation</span><span class="p">,</span>
                             <span class="n">kernel_size</span><span class="p">,</span>
                             <span class="n">stride</span><span class="p">):</span>
    <span class="n">numerator</span> <span class="o">=</span> <span class="n">in_channels</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">padding</span> <span class="o">-</span> <span class="n">dilation</span> <span class="o">*</span> <span class="p">(</span> <span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

    <span class="n">arg_of_floor</span> <span class="o">=</span> <span class="p">(</span><span class="n">numerator</span> <span class="o">/</span> <span class="n">stride</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span> <span class="n">arg_of_floor</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For now, we will instantiate a parameters dictionary with <code>dilation = 1</code> and <code>padding = 0</code>. Let us use $n = 32$, equivalently <code>in_channels = 32</code>, so that</p>
$$
\tilde{n} = \left\lfloor \frac{n-k}{\lambda} + 1 \right\rfloor \equiv \left\lfloor \frac{32-k}{\lambda} + 1 \right\rfloor
$$<p>For our example, we specialize to <code>kernel_size = 4</code> and <code>stride = 2</code>, in which case $\tilde{n} = 15$.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">odcp</span> <span class="o">=</span> <span class="p">{}</span> <span class="c1"># one dimensional conv. parameters</span>

<span class="n">odcp</span><span class="p">[</span><span class="s2">"in_channels"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">odcp</span><span class="p">[</span><span class="s2">"padding"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">odcp</span><span class="p">[</span><span class="s2">"dilation"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">odcp</span><span class="p">[</span><span class="s2">"kernel_size"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">odcp</span><span class="p">[</span><span class="s2">"stride"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span> 
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">compute_num_out_channels</span><span class="p">(</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">odcp</span><span class="p">[</span><span class="s2">"in_channels"</span><span class="p">],</span>
                         <span class="n">padding</span> <span class="o">=</span> <span class="n">odcp</span><span class="p">[</span><span class="s2">"padding"</span><span class="p">],</span>
                         <span class="n">dilation</span> <span class="o">=</span> <span class="n">odcp</span><span class="p">[</span><span class="s2">"dilation"</span><span class="p">],</span>
                         <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">odcp</span><span class="p">[</span><span class="s2">"kernel_size"</span><span class="p">],</span>
                         <span class="n">stride</span> <span class="o">=</span> <span class="n">odcp</span><span class="p">[</span><span class="s2">"stride"</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>15.0</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The layer $\tilde{B}$ has two sets of learned parameters, these are respectively organized in the weight matrix $\tilde{\theta}$ and the bias vector $\tilde{b}$. For simplicity, at this point, we set $\tilde{b} \equiv 0$ through the option <code>bias = False</code>, so that the only learned parameters are those in the weight matrix.</p>
<p>Recall that one has</p>
$$
\tilde{\theta} \in \mathcal{X} ( H \backslash G / \tilde{H}, \mathcal{C} \otimes \tilde{\mathcal{C}} ),
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>but let us reduce this further based on the assumptions we've made. Because $G \equiv \Omega \equiv C_n$, one has that $H$ must be the trivial subgroup $\{ e \}$. Moreover, having assumed that $\dim{\mathcal{C}} = 1$, one also has $\mathcal{C} \otimes \tilde{\mathcal{C}} \cong \tilde{\mathcal{C}}$, so we may write</p>
$$
\tilde{\theta} \in \mathcal{X} ( G / \tilde{H} ) , \tilde{\mathcal{C}} ),
$$<p>with $C_{\tilde{n}} \cong G / \tilde{H}$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="2.4-...-two-dimensional-convolutions-in-torch">
<a class="anchor" href="#2.4-...-two-dimensional-convolutions-in-torch" aria-hidden="true"><span class="octicon octicon-link"></span></a><font color="CornflowerBlue">2.4 ... two-dimensional convolutions in torch</font><a class="anchor-link" href="#2.4-...-two-dimensional-convolutions-in-torch"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h4 id="torch.nn.Conv2d">
<a class="anchor" href="#torch.nn.Conv2d" aria-hidden="true"><span class="octicon octicon-link"></span></a><code>torch.nn.Conv2d</code><a class="anchor-link" href="#torch.nn.Conv2d"> </a>
</h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The arguments:</p>
<ul>
<li>
<p><code>in_channels</code></p>
</li>
<li>
<p><code>out_channels</code></p>
</li>
<li>
<p><code>kernel_size</code></p>
</li>
<li>
<p><code>stride</code> $\quad$ controls the stride for the cross-correlation, a single number or a tuple.</p>
</li>
<li>
<p><code>padding</code> $\quad$ controls amount of padding applied to the input. It can either be a string, <code>"valid"</code> or <code>"same"</code> or a tuple of ints giving the amount of implicit padding applied on both sides.</p>
</li>
<li>
<p><code>dilation</code> $\quad$ controls the spacing between kernel points; "also known as the a trous algorithm</p>
</li>
<li>
<p><code>groups</code> $\quad$ controls connections between inputs and outputs. The <code>in_channels</code> and <code>out_channels</code> must be divisible by <code>groups</code>. For example,</p>
<ul>
<li>
<p>At groups = 1, all inputs are convolved to all outputs</p>
</li>
<li>
<p>At groups = 2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated.</p>
</li>
<li>
<p>At groups = <code>in_channels</code>, each input channel is convolved with its own set of filters (of size <code>out_channels // in_channels</code>)</p>
</li>
</ul>
</li>
<li>
<p><code>bias</code></p>
</li>
<li>
<p><code>padding_mode</code>,</p>
</li>
<li>
<p><code>device</code>,</p>
</li>
<li>
<p><code>dtype</code></p>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let us now relate the shapes of the input and output to the parameters</p>
<table>
<thead>
<tr>
<th>input parameter</th>
<th>LaTeX symbol</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>in_channels</code></td>
<td>$\text{dim}(\mathcal{C})$</td>
</tr>
<tr>
<td><code>out_channels</code></td>
<td>$\text{dim}(\mathcal{C}_1)$</td>
</tr>
<tr>
<td><code>kernel_size</code></td>
<td>$k$</td>
</tr>
<tr>
<td><code>stride</code></td>
<td>$\lambda$</td>
</tr>
<tr>
<td><code>padding</code></td>
<td>$\rho$</td>
</tr>
<tr>
<td><code>dilation</code></td>
<td>$\delta$</td>
</tr>
<tr>
<td><code>groups</code></td>
<td>$M$        </td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Additionally, we use $N$ for the batch size of the input, <code>N</code>. We also let $(h,w)$ denote the height-width pair describing the shape of the input signal domain.</p>
<p>Correspondingly, we write $(h_1, w_1)$ for the height-width pair describing the shape of the output signal domain.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We remark that the stride can be either integer or a $2$-tuple, whose coordinates describe the vertical and horizontal stride respectively. We still write $\lambda$ for the stride when it is a tuple, and use $\lambda_h \equiv \lambda[0]$ and $\lambda_w \equiv \lambda[1]$ to denote its first and second coordinate, in this case. Likewise, the padding and kernel size may be $2$-tuples as well, and we use similar notation to denote their entries.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The full shape of the input to the layer includes the batch dimension, and is thus</p>
$$
(N, \text{dim}(\mathcal{C}), H, W) \,,
$$<p>while the shape of the output is</p>
$$
(N , \text{dim}(\mathcal{C}_1), H_1, W_1 )
$$<p>These shapes, in particular the spatial dimensions of each, are related as follows:</p>
<p>$\begin{align}
H_1 &amp;= \left\lfloor \frac{
    H + 2 \rho_h - \delta_h ( k_h -1) -1 }{\lambda_h}
\right\rfloor \\
W_1 &amp;= \left\lfloor \frac{
    W + 2 \rho_w - \delta_w ( k_w -1) -1 }{\lambda_w}
\right\rfloor
\end{align}$,</p>
<p>in particular, the batch size does not have any bearing on how the shapes of tensors transform.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The parameters to be learned are the weights $w^1$ and biases $b^1$. These are both <code>Tensor</code> objects, accessed from the layer as <code>Conv2d.weight</code> and <code>Conv2d.bias</code>. The shape of the weight tensor is</p>
$$
\textrm{shape}(w^1) =
\left( \, \text{dim}(\mathcal{C}_1),  \, \text{dim}(\mathcal{C}) \big/ M , k_h, k_w \right)
$$<p>The tensor $w^1$ thus has</p>
$$
\textrm{size}(w^1) = \textrm{dim}(\mathcal{C}_1) \textrm{dim} (\mathcal{C}) k_h k_w \big/ M
$$<p>scalar entries.</p>
<p>There is always the question of how to initialize weights. In the case of the <code>Conv2d</code> class, the weights are initialized to be i.i.d. $\text{Unif}( - \sqrt{ \alpha_1}, \sqrt{\alpha_1} )$ random variables, where</p>
$$
\alpha_1 := \frac{ \textrm{dim}(\mathcal{C}_1) }{\textrm{size}(w^1)}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The bias tensor is a much smaller object, we have</p>
<p>$
\begin{align}
\textrm{shape}(b^1) = (\, \textrm{dim}(\mathcal{C}_1  ) \,) \, , \quad \textrm{size}(b^1) = \textrm{dim}(\mathcal{C}_1)
\end{align}
$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Despite this, we use the same initialization (with mutual independence of all random variables in discussion) for the bias entries as we did for the weights.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="...-A-simple-CNN">
<a class="anchor" href="#...-A-simple-CNN" aria-hidden="true"><span class="octicon octicon-link"></span></a>... A simple CNN<a class="anchor-link" href="#...-A-simple-CNN"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We consider possibly the simplest neural network that we can construct through the above blueprint. Suppose we have a binary classification problem, with the following hypothesis space. Let $\textsf{H}_1$ denote the hypothesis space of functions $f : \mathcal{X}( C_n, \mathbb{R}) \to \{0,1\}$ of the form</p>
$$
f = A \circ P \circ \mathbf{a} \circ B \,,
$$<p>where the components of $f$ are</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>where the components of $f$ are</p>
<ul>
<li>
<p>$B$  : $\quad$ A $C_n$-equivariant function, to be learned. It is represented as a circulant matrix $\mathbf{C}(\theta)$, where $\theta$ is a vector $\theta \equiv (\theta_0, \dots, \theta_{n-1})$ whose entries $\theta_j$ are parameters to be learned.</p>
</li>
<li>
<p>$ \mathbf{a} $ : $\quad$ We consider the ReLU activation function, $a : \mathbb{R} \to \mathbb{R}_{\geq\, 0}$ defined by $a(w) = \max(0,w)$, for $w \in \mathbb{R}$. The bold-face $\mathbf{a}$ denotes the entry-wise action of this function on a given vector;for $y \equiv (\,y_1, \,\dots, \, y_n \, ) \in \mathcal{X}(C_n, \mathbb{R})$, which we imagine as the output of $B(x)$ for some input signal $x$, we have $\mathbf{a} (y ) = ( \,  \max(0,y_1), \,  \dots, \, \max(0,y_n) )$. There are no learned parameters in this layer.</p>
</li>
<li>
<p>$P$ : $\quad$ A coarsening operator. In this case, let us say it is a <em>zero-padded group homomorphism</em>.</p>
<p>$P : C_n \to C_{n / d }$ for some divisor $d \mid n$ \footnote{zero-padding} , and let us say that it operates through max-pooling on the signal, over the pre-images of each element of $C_{n / d}$.</p>
</li>
<li>
<p>$A$ : $\quad$ A global-pooling layer. We assume this has the form of a fully-connected layer, followed by a softmax. Specifically,</p>
</li>
</ul>

</div>
</div>
</div>
</div>



  </div><a class="u-url" href="/geometric-deep-learning/jupyter/2021/10/22/GDL2.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/geometric-deep-learning/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/geometric-deep-learning/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/geometric-deep-learning/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>...</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" title="fastai"><svg class="svg-icon grey"><use xlink:href="/geometric-deep-learning/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/geometric-deep-learning/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
