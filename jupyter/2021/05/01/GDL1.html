<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>1 | geometric deep learning</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="1" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="GDL1" />
<meta property="og:description" content="GDL1" />
<link rel="canonical" href="https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2021/05/01/GDL1.html" />
<meta property="og:url" content="https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2021/05/01/GDL1.html" />
<meta property="og:site_name" content="geometric deep learning" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-05-01T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"GDL1","url":"https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2021/05/01/GDL1.html","@type":"BlogPosting","headline":"1","dateModified":"2021-05-01T00:00:00-05:00","datePublished":"2021-05-01T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2021/05/01/GDL1.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/geometric-deep-learning/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://the-ninth-wave.github.io/geometric-deep-learning/feed.xml" title="geometric deep learning" /><link rel="shortcut icon" type="image/x-icon" href="/geometric-deep-learning/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/geometric-deep-learning/">geometric deep learning</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/geometric-deep-learning/about/">About Me</a><a class="page-link" href="/geometric-deep-learning/search/">Search</a><a class="page-link" href="/geometric-deep-learning/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">1</h1><p class="page-description">GDL1</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-05-01T00:00:00-05:00" itemprop="datePublished">
        May 1, 2021
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      8 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/geometric-deep-learning/categories/#jupyter">jupyter</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/the-ninth-wave/geometric-deep-learning/tree/master/_notebooks/2021-05-01-GDL1.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/geometric-deep-learning/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/the-ninth-wave/geometric-deep-learning/master?filepath=_notebooks%2F2021-05-01-GDL1.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/geometric-deep-learning/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/the-ninth-wave/geometric-deep-learning/blob/master/_notebooks/2021-05-01-GDL1.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/geometric-deep-learning/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#1.1-categories-and-groups">1.1 categories and groups </a>
<ul>
<li class="toc-entry toc-h3"><a href="#$\quad$-group-representations">$\quad$ group representations </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-05-01-GDL1.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Modern neural network (NN) design is built on two algorithmic principles: hierarchical feature learning ( concerning the architecture of the NN ), and learning by local gradient-descent driven by backpropagation ( concerning the learning dynamics undergone by the NN ).</p>
<p>An instance of training data is modeled as an element of some high-dimensional vector space, making a generic learning problem subject to the <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">curse of dimensionality</a>. Fortunately, most tasks of interest are not generic, inheriting regularities from the underlying low-dimensionality and structure of the physical world.</p>
<p>Exploiting known symmetries of a large system is a useful, classical remedy against the curse of dimensionality, and forms the basis of most physical theories. The notes [BBCV21] construct a blueprint for neural network architecture which incorporates these ``physical" priors, termed <em>geometric priors</em> throughout the notes. Importantly, this blueprint provides a unified perspective of the most successful neural network architectures.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="1.1-categories-and-groups">
<a class="anchor" href="#1.1-categories-and-groups" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>1.1 categories and groups</strong><a class="anchor-link" href="#1.1-categories-and-groups"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>Def</strong> $\quad$ A <em>graph</em> is a pair $\mathcal{G} = (\mathcal{V}, \mathcal{E})$, where $\mathcal{V}$ is a set whose elements are called <em>vertices</em>. The set $\mathcal{E}$ consists of <em>edges</em>, defined to be a multi-set of exactly two vertices in $\mathcal{V}$, not necessarily distinct.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>Def</strong> $\quad$ A <em>directed graph</em> is a pair of sets  $\mathcal{G} = (\mathcal{V}, \mathcal{A})$ of vertices and <em>arrows</em> (or <em>directed edges</em>). An <em>arrow</em> is an ordered pair of vertices.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>Def</strong> $\quad$ Consider an arrow $f$ of a directed graph $\mathcal{G} = ( \mathcal{V}, \mathcal{A})$, specifically $f \equiv (a,b) \in \mathcal{A}$, with $a,b \in \mathcal{V}$. The operations $\mathbb{dom}$ and $\mathbb{cod}$ act on the arrows $f \in \mathcal{A}$ via $\mathbb{dom}f = a,\, \mathbb{cod} f = b$, and are called the <em>domain</em> operation and <em>codomain</em> operation,  respectively.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p>$\vdots$</p>
<p>Given two arrows, $f$ and $g$ in some directed graph, we say that the ordered pair of arrows $(g,f)$ is a <em>composable pair</em> if $\mathbb{dom} g = \mathbb{cod} f$. 
Going forward, let us express the relations $a = \mathbb{dom} f$ and $b = \mathbb{cod} f$ more concisely via
$$
f : a \to b \, \quad \text { or equivalently, } \, \quad a \xrightarrow[\,]{ f} b  
$$</p>
<p>The next definition formalizes the behavior of a collection of structure-respecting maps between mathematical objects.</p>
<p>$\vdots$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>Def</strong> $\quad$ A <em>category</em> is a directed graph $\mathcal{C} = (\mathcal{O},\mathcal{A})$, whose vertices $\mathcal{O}$ we call <em>objects</em>, such that</p>
<ol>
<li>
<p>For each object $a \in \mathcal{O}$, there is a unique <em>identity</em> arrow $\textrm{id}_a \equiv \mathbf{1}_a : a \to a$, defined by the following property: for all arrows $f : b \to a$ and $g : a \to c$, composition with the identity arrow $\mathbf{1}_a $ gives
$$
\mathbf{1}_a \circ f = f \quad \text{ and } \quad g \circ \mathbf{1}_a = g
$$</p>
</li>
<li>
<p>For each composable pair $(g, f)$ of arrows, there is a unique arrow $g \circ f$ called their <em>composite</em>, with $g \circ f : \mathbb{dom} f \to \mathbb{cod} g$, such that the composition operation is associative. Namely, for given objects and arrows in the configuration $a \xrightarrow[\,]{ f} b \xrightarrow[\,]{ g} c \xrightarrow[\,]{ k} d$,
one always has the equality $k \circ (g \circ f) = (k \circ g ) \circ f$.</p>
</li>
</ol>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>$\vdots$</p>
<p>Given a category $\mathcal{C} = (\mathcal{O},\mathcal{A})$, let
$$
\mathbb{hom} (b,c) := \{ \, f : f \in \mathcal{A},  \, \mathbb{dom} f = b \in \mathcal{O}, \, \mathbb{cod} f = c \in \mathcal{O} \, \}
$$
denote the set of arrows from $b$ to $c$. Henceforth, we use the terms <em>morphism</em> and arrow interchangeably.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Groups are collections of symmetries. A <em>group</em> $G$ is a category $\mathcal{C} = (\mathcal{O}, \mathcal{A})$ with $\mathcal{O} = \{ o \}$ ( so that we may identify $G$ with the collection of arrows $\mathcal{A}$ ) such that each arrow has a unique inverse: for $g \in \mathcal{A}$, there is an arrow $h$ such that $g \circ h = h \circ g = \mathbf{1}_o$.</p>
<p>Each arrow $g \in \mathcal{A}$ thus has $\mathbb{dom} g = \mathbb{cod} g = o$. As remarked, the arrows $g \in \mathcal{A}$ correspond to group elements $g \in G$. The categorical interpretation suggests that the group \emph{acts} on some abstract object $o \in \mathcal{O}$. In the present context, we care how groups act on data, and how this action is represented to a computer.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="$\quad$-group-representations">
<a class="anchor" href="#%24%5Cquad%24-group-representations" aria-hidden="true"><span class="octicon octicon-link"></span></a>$\quad$ <em><strong>group representations</strong></em><a class="anchor-link" href="#%24%5Cquad%24-group-representations"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Linear representation theory allows us to study groups using linear algebra (<a href="https://wlou.blog/2018/06/22/a-first-impression-of-group-representations/">a source</a> ). We start by considering a function $\varphi : G \times V \to V$, where $G$ is a group, and where $V$ is a vector space over $\mathbb{R}$. This allows us to identify group elements $g$ with functions $\varphi(g, \cdot) : V \to V$ from the vector space to itself. When the map $\varphi$ is understood, or general ( as now ), we write $g.v$ in place of $\varphi(g,v)$, and we write $(g.)$ in place of $\varphi(g, \cdot)$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The "representatives" $(g.)$ of these group elements $g$ can be composed, and if this compositional structure is compatible with the original group operation, we say $\varphi$ is a <em>group action</em> on $V$. Specifically, $\varphi$ should satisfy $e.v = v$ for all $v \in V$, where $e$ denotes the identity element of $G$, and in general one has $(gh).v  = g.(h.v)$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The map $\varphi$ is <em>$\mathbb{R}$-linear</em> if it is compatible with the $\mathbb{R}$-vector space structure on $V$, i.e. additive and homogeneous. Specifically, if for all $v,w \in V$ and all scalars $\lambda \in \mathbb{R}$, one has $g.(v+w) = g.v + g.w$ and $g.(\lambda v) = \lambda g.v$.</p>
<p>$\vdots$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>Def</strong> $\quad$ 
An <em>$\mathbb{R}$-linear representation</em> of group $G$ over $\mathbb{R}$-vector space $V$ is an $\mathbb{R}$-linear group action on $V$.</p>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>$\vdots$</p>
<p>The next example illustrates how linear group representations arise naturally when considering group actions on data. As mentioned, we consider input data as members of some vector space $V$, which we may assume to be finite dimensional for any practical discussion. Specifically, we consider some finite, discrete domain $\Omega$, which may also have the structure of an undirected graph.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A \emph{\B{signal}} over $\Omega$ is a function $x : \Omega \to \mathbb{R}^s$, where $s$ is the number of <em>channels</em>. The vector space $\mathcal{X}(\Omega,\mathbb{R}^s)$ is defined to be the collection of all such signals, for given $\Omega$ and $s$.</p>
<p>$\vdots$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<hr>
<p><em><strong>Example</strong></em> $\quad$  Consider, for some $n \in \mathbb{N}$, a signal domain $\Omega = \mathbb{T}_n^2$, where $\mathbb{T}_n^2$ denotes the two-dimensional discrete torus of side-length $n$, namely $( \mathbb{Z} / n\mathbb{Z} )^2$. This domain has natural graph as well as group structures.</p>
<p>If we imagine each vertex of $\Omega$ to be a pixel, we can express an $n \times n$-pixel color (RGB) image as a signal $x : \Omega \to \mathbb{R}^3$, with the first, second and third coordinates of $\mathbb{R}^3$ reporting R, G and B values of a given pixel.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We make two observations:</p>
<ol>
<li>
<p>As a vector space, $\mathcal{X}(\Omega)$ is isomorphic to $\mathbb{R}^d$, with $d$ typically very large. In the above example, $d = 3n^2$, which is thirty-thousand for a $n \times n \equiv 100 \times 100$ pixel image.</p>
</li>
<li>
<p>Any group action on $\Omega$ induces a group action on $\mathcal{X}(\Omega)$.</p>
</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Expanding on the latter, consider a group action of $G$ on domain $\Omega$. As the torus $\Omega$ already has group structure, it is natural to think of it acting on itself through translations, i.e. we now additionally consider $G = \mathbb{T}_n^2$.</p>
<p>The action of $G \equiv \mathbb{T}_n^2$ on itself $\Omega \equiv \mathbb{T}_n^2$ induces a $G$-action on $\mathcal{X}(\Omega)$ as follows: for $g \in G$ signal $x \in \mathcal{X}(\Omega)$, the action $(g, x) \mapsto \mathbf{g}.x \in \mathcal{X}(\Omega)$  is defined pointwise at each $u \in \Omega$:
$$
(\mathbf{g}.x)(u) := x(g.\omega),
$$
where the bold $(\mathbf{g}.)$ is used to distinguish the action on signals from the action on the domain.</p>
<hr>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>$\vdots$</p>
<p>To summarize: any $G$-action on the domain $\Omega$ induces an $\mathbb{R}$-linear representation of $G$ over the vector space of signals on $\Omega$.</p>
<p>$\vdots$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<hr>
<p><em><strong>Example</strong></em> $\quad$  It seems like standard practice to encode the collection of classes associated to some ML classification problem as an orthonormal basis. These are given ( to the computer ) in the usual coordinate basis 
$$
e_1 \equiv (1, 0, \dots, 0),\, e_2 \equiv (0,1,\dots, 0),\, \dots, \, e_n \equiv (0,\dots, 0,1) \,,
$$
hence the nomenclature <em>one-hot</em>. In the preceding example, if one considers a one-hot encoding of the vertices of $\mathbb{T}_n^2$, we see that each signal is expressed with respect to this coordinate system, in the sense that $x = \sum_{j=1}^n x_j e_j$.</p>
<p>This kind of encoding is useful for considering general symmetries of the domain. For instance, if permuting node labels is a relevant symmetry, the action of the symmetric group $\frak{S}_n$ is naturally represented by $n \times n$ permutation matrices.</p>
<hr>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>$\vdots$</p>
<p>The following definition reformulates the notion of a signal over the nodes of some graph as <em>node features</em>.</p>
<p>$\vdots$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>Def</strong> $\quad$ We say a graph $\mathcal{G} = ( \mathcal{V}, \mathcal{E} )$ is equipped with \emph{\B{node features}} if for each $v  \in \mathcal{V}$, one has the additional data of an $s$-dimensional vector $x(v) \in \mathbb{R}^s$, called the <em>features</em> of node $v$.</p>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>$\vdots$</p>
<p>The term 'features' is compatible with the usage in ML, supposing that our input signal has domain some graph $\mathcal{G}$. In this case, we can think of a neural network as a sequence of node-layers built ``on top of" the graph $\mathcal{G}$. An input signal endows the first node layer of a NN with features, and the weights of the neural network propagate these through to node features on the nodes of the rest of the network. The features on the last layer of the network can be read off as the output of the NN function.</p>

</div>
</div>
</div>
</div>



  </div><a class="u-url" href="/geometric-deep-learning/jupyter/2021/05/01/GDL1.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/geometric-deep-learning/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/geometric-deep-learning/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/geometric-deep-learning/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>...</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" title="fastai"><svg class="svg-icon grey"><use xlink:href="/geometric-deep-learning/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/geometric-deep-learning/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
