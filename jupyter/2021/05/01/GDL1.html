<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>1 | geometric deep learning</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="1" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="GDL1" />
<meta property="og:description" content="GDL1" />
<link rel="canonical" href="https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2021/05/01/GDL1.html" />
<meta property="og:url" content="https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2021/05/01/GDL1.html" />
<meta property="og:site_name" content="geometric deep learning" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-05-01T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"GDL1","url":"https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2021/05/01/GDL1.html","@type":"BlogPosting","headline":"1","dateModified":"2021-05-01T00:00:00-05:00","datePublished":"2021-05-01T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2021/05/01/GDL1.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/geometric-deep-learning/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://the-ninth-wave.github.io/geometric-deep-learning/feed.xml" title="geometric deep learning" /><link rel="shortcut icon" type="image/x-icon" href="/geometric-deep-learning/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/geometric-deep-learning/">geometric deep learning</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/geometric-deep-learning/about/">About Me</a><a class="page-link" href="/geometric-deep-learning/search/">Search</a><a class="page-link" href="/geometric-deep-learning/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">1</h1><p class="page-description">GDL1</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-05-01T00:00:00-05:00" itemprop="datePublished">
        May 1, 2021
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      27 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/geometric-deep-learning/categories/#jupyter">jupyter</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/the-ninth-wave/geometric-deep-learning/tree/master/_notebooks/2021-05-01-GDL1.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/geometric-deep-learning/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/the-ninth-wave/geometric-deep-learning/master?filepath=_notebooks%2F2021-05-01-GDL1.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/geometric-deep-learning/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/the-ninth-wave/geometric-deep-learning/blob/master/_notebooks/2021-05-01-GDL1.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/geometric-deep-learning/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#1.1-$\quad$-Algebra">1.1 $\quad$ Algebra </a>
<ul>
<li class="toc-entry toc-h3"><a href="#$\quad$-_Graphs,-categories,-groups_">$\quad$ _Graphs, categories, groups_ </a></li>
<li class="toc-entry toc-h3"><a href="#$\quad$-_Group-representations_">$\quad$ _Group representations_ </a></li>
<li class="toc-entry toc-h3"><a href="#$\quad$-*Example:-shifts-of-RGB-images*">$\quad$ *Example: shifts of RGB images* </a></li>
<li class="toc-entry toc-h3"><a href="#$\quad$-_Example:-One-hot-encoding_">$\quad$ _Example: One-hot encoding_ </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#1.2-$\quad$-Equivariance">1.2 $\quad$ Equivariance </a>
<ul>
<li class="toc-entry toc-h3"><a href="#$\quad$-_Example:-permutation-invariance-and-equivariance-in-data_-">$\quad$ _Example: permutation invariance and equivariance in data_  </a></li>
<li class="toc-entry toc-h3"><a href="#$\quad$-_Example:-permutations-and-local-averaging_">$\quad$ _Example: permutations and local averaging_ </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#1.3-$\quad$-Equivariance-in-neural-networks">1.3 $\quad$ Equivariance in neural networks </a>
<ul>
<li class="toc-entry toc-h3"><a href="#$\quad\quad$-_Neural-networks_">$\quad\quad$ _Neural networks_ </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#1.4-$\quad$-Tensors">1.4 $\quad$ Tensors </a></li>
<li class="toc-entry toc-h2"><a href="#$1.5$-$\quad$-GDL-blueprint-(with-fixed-input-domain)">$1.5$ $\quad$ GDL blueprint (with fixed input domain) </a>
<ul>
<li class="toc-entry toc-h3"><a href="#$\quad$-Setup">$\quad$ Setup </a></li>
<li class="toc-entry toc-h3"><a href="#$\quad$-coarsening-operator">$\quad$ coarsening operator </a></li>
<li class="toc-entry toc-h3"><a href="#$\quad$-GDL-block">$\quad$ GDL block </a></li>
<li class="toc-entry toc-h3"><a href="#$\quad$-Hypothesis-space">$\quad$ Hypothesis space </a></li>
<li class="toc-entry toc-h3"><a href="#$\quad$-Discussion">$\quad$ Discussion </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-05-01-GDL1.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Modern neural network (NN) design is built on two algorithmic principles: hierarchical feature learning (this concerns the architecture of the NN), and learning by local gradient-descent driven by backpropagation (concerning the learning dynamics undergone by the NN).</p>
<p>An instance of training data is modeled as an element of some high-dimensional vector space, making a generic learning problem subject to the <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">curse of dimensionality</a>. Most tasks of interest are not generic; the data comes from the physical world, and so it inherits low-dimensional structure and symmetries. Requiring an NN to respect the symmetries of the data upon which it acts amounts to a prior.</p>
<p>The notes [BBCV21] construct a blueprint for neural network architecture incorporating these priors, termed <em>geometric priors</em> throughout the notes. Importantly, this blueprint provides a unified perspective of the most successful neural network architectures, at least at the level of the building blocks chosen for the network.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="1.1-$\quad$-Algebra">
<a class="anchor" href="#1.1-%24%5Cquad%24-Algebra" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.1 $\quad$ Algebra<a class="anchor-link" href="#1.1-%24%5Cquad%24-Algebra"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>References</em></p>
<ul>
<li>
<p><a href="https://arxiv.org/abs/2104.13478">Bronstein-Bruna-Cohen-Velicovic 2021</a>, <em>Geometric deep learning</em></p>
</li>
<li>
<p><a href="https://wlou.blog/2018/06/22/a-first-impression-of-group-representations/">A first impression of group representations</a></p>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="$\quad$-_Graphs,-categories,-groups_">
<a class="anchor" href="#%24%5Cquad%24-_Graphs,-categories,-groups_" aria-hidden="true"><span class="octicon octicon-link"></span></a>$\quad$ <font color="teal">_Graphs, categories, groups_</font><a class="anchor-link" href="#%24%5Cquad%24-_Graphs,-categories,-groups_"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>Def</strong> $\quad$ A <font color="purple">_graph_</font> is a pair $\mathcal{G} = (\mathcal{V}, \mathcal{E})$, where $\mathcal{V}$ is a set whose elements are called <font color="purple">_vertices_</font>. The set $\mathcal{E}$ consists of <font color="purple">_edges_</font>, defined to be a multi-set of exactly two vertices in $\mathcal{V}$, not necessarily distinct.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>Def</strong> $\quad$ A <font color="purple">_directed graph_</font> is a pair of sets  $\mathcal{G} = (\mathcal{V}, \mathcal{A})$ of vertices and <em>arrows</em> (or <em>directed edges</em>). An <font color="purple">_arrow_</font> is an ordered pair of vertices.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>Def</strong> $\quad$ Consider an arrow $f$ of a directed graph $\mathcal{G} = ( \mathcal{V}, \mathcal{A})$, specifically $f \equiv (a,b) \in \mathcal{A}$, with $a,b \in \mathcal{V}$. The operations $\textsf{dom}$ and $\textsf{cod}$ act on the arrows $f \in \mathcal{A}$ via 
$$
\textsf{dom}f = a,\, \textsf{cod} f = b\,,
$$ 
and are called the <font color="purple">_domain_</font> operation and <font color="purple">_codomain_</font> operation,  respectively.</p>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Given arrows, $f$ and $g$ of some directed graph, say that the ordered pair $(g,f)$ is a <font color="purple">_composable pair_</font> if $\textsf{dom} g = \textsf{cod} f$. 
Going forward, we express the relations $a = \textsf{dom} f$ and $b = \textsf{cod} f$ equivalently as
$$
f : a \to b \, \quad \text { or  } \, \quad a \xrightarrow[\,]{ f} b  
$$</p>
<p>The next definition formalizes the behavior of a collection of structure-respecting maps between mathematical objects.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>Def</strong> $\quad$ A <font color="purple">_category_</font> is a directed graph $\mathcal{C} = (\mathcal{O},\mathcal{A})$, whose vertices $\mathcal{O}$ we call <font color="purple">_objects_</font>, such that</p>
<ol>
<li>
<p>For each object $a \in \mathcal{O}$, there is a unique <font color="purple">_identity_</font> arrow $\textrm{id}_a \equiv \mathbf{1}_a : a \to a$, defined by the following property: for all arrows $f : b \to a$ and $g : a \to c$, composition with the identity arrow $\mathbf{1}_a $ gives
$$
\mathbf{1}_a \circ f = f \quad \text{ and } \quad g \circ \mathbf{1}_a = g
$$</p>
</li>
<li>
<p>For each composable pair $(g, f)$ of arrows, there is a unique arrow $g \circ f$ called their <font color="purple">_composite_</font>, with $g \circ f : \textsf{dom} f \to \textsf{cod} g$, such that the composition operation is associative. Namely, for given objects and arrows in the configuration 
$$a \xrightarrow[\,]{ f} b \xrightarrow[\,]{ g} c \xrightarrow[\,]{ k} d \,,
$$
one always has the equality $k \circ (g \circ f) = (k \circ g ) \circ f$.</p>
</li>
</ol>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Given a category $\mathcal{C} = (\mathcal{O},\mathcal{A})$, let
$$
\textsf{hom} (b,c) := \{ \, f : f \in \mathcal{A},  \, \textsf{dom} f = b \in \mathcal{O}, \, \textsf{cod} f = c \in \mathcal{O} \, \}
$$
denote the set of arrows from $b$ to $c$. Henceforth, we use the terms <font color="purple">_morphism_</font> and arrow interchangeably.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Groups are collections of symmetries. A <font color="purple">_group_</font> $G$ is a category $\mathcal{C} = (\mathcal{O}, \mathcal{A})$ with $\mathcal{O} = \{ o \}$ ( so that we may identify $G$ with the collection of arrows $\mathcal{A}$ ) such that each arrow has a unique inverse: for $g \in \mathcal{A}$, there is an arrow $h$ such that $g \circ h = h \circ g = \mathbf{1}_o$.</p>
<p>Each arrow $g \in \mathcal{A}$ thus has $\textsf{dom} g = \textsf{cod} g = o$. As remarked, the arrows $g \in \mathcal{A}$ correspond to group elements $g \in G$. The categorical interpretation suggests that the group acts on some abstract object $o \in \mathcal{O}$. In the present context, we care how groups act on data, and how this action is represented to a computer.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="$\quad$-_Group-representations_">
<a class="anchor" href="#%24%5Cquad%24-_Group-representations_" aria-hidden="true"><span class="octicon octicon-link"></span></a>$\quad$ <font color="teal">_Group representations_</font><a class="anchor-link" href="#%24%5Cquad%24-_Group-representations_"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Linear representation theory allows uses linear algebra to study groups.</p>
<p>Consider a function $\varphi : G \times V \to V$, where $G$ is a group, and where $V$ is a vector space over $\mathbb{R}$. Through this function, we can identify group elements $g$ with self-maps of $V$, namely $\varphi(g, \cdot) : V \to V$.</p>
<p>When the map $\varphi$ is understood, or left general, we write $g.v$ in place of $\varphi(g,v)$, and we write $(g.)$ in place of $\varphi(g, \cdot)$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The "representatives" $(g.)$ of these group elements $g$ can always be composed. If this compositional structure is compatible with the original group operation, we say $\varphi$ is a <font color="purple">_group action_</font> on $V$.</p>
<p>Specifically, $\varphi$ should satisfy $e.v = v$ for all $v \in V$, where $e$ denotes the identity element of $G$, and in general one has $(gh).v  = g.(h.v)$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The group action $\varphi$ is <font color="purple">_$\mathbb{R}$-linear_</font> if it is compatible with the $\mathbb{R}$-vector space structure on $V$, i.e. additive and homogeneous. Specifically, if for all $v,w \in V$ and all scalars $\lambda \in \mathbb{R}$, one has $g.(v+w) = g.v + g.w$ and $g.(\lambda v) = \lambda g.v$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>Def</strong> $\quad$ 
An <font color="purple">_$\mathbb{R}$-linear representation_</font> of group $G$ over $\mathbb{R}$-vector space $V$ is an $\mathbb{R}$-linear group action on $V$.</p>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The next example illustrates how linear group representations arise naturally when considering group actions on data.</p>
<p>Input datum are modeled as members of some vector space $\mathcal{X}$, defined with respect to a domain $\Omega$ and a smaller vector space of <font color="purple">_channels_</font>, $\mathcal{C}$. We may assume all vector spaces discussed to be finite dimensional.</p>
<p>Specifically, we consider the vector space of $\mathcal{C}$-valued signals over some finite, discrete domain $\Omega$, which perhaps has some graph structure. This vector space is denoted $\mathcal{X}(\Omega, \mathcal{C})$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<hr>
<h3 id="$\quad$-*Example:-shifts-of-RGB-images*">
<a class="anchor" href="#%24%5Cquad%24-*Example:-shifts-of-RGB-images*" aria-hidden="true"><span class="octicon octicon-link"></span></a>$\quad$ <font color="teal">*Example: shifts of RGB images*</font><a class="anchor-link" href="#%24%5Cquad%24-*Example:-shifts-of-RGB-images*"> </a>
</h3>
<p>$\quad$  Consider, for some $n \in \mathbb{N}$, a signal domain $\Omega = \mathbb{T}_n^2$, where $\mathbb{T}_n^2$ denotes the two-dimensional discrete torus of side-length $n$, namely $( \mathbb{Z} / n\mathbb{Z} )^2$. This domain has natural graph as well as group structures.</p>
<p>If we imagine each vertex of $\Omega$ to be a pixel, we can express an $n \times n$-pixel color (RGB) image as a signal $x : \Omega \to \mathbb{R}^3$, with the first, second and third coordinates of $\mathbb{R}^3$ reporting R, G and B values of a given pixel.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We make two observations:</p>
<ol>
<li>
<p>As a vector space, $\mathcal{X}(\Omega)$ is isomorphic to $\mathbb{R}^d$, with $d$ typically very large. In the above example, $d = 3n^2$, which is thirty-thousand for a $n \times n \equiv 100 \times 100$ pixel image.</p>
</li>
<li>
<p>Any group action on $\Omega$ induces a group action on $\mathcal{X}(\Omega)$.</p>
</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Expanding on the latter, consider a group action of $G$ on domain $\Omega$. As the torus $\Omega$ already has group structure, it is natural to think of it acting on itself through translations, i.e. we now additionally consider $G = \mathbb{T}_n^2$.</p>
<p>The action of $G \equiv \mathbb{T}_n^2$ on itself $\Omega \equiv \mathbb{T}_n^2$ induces a $G$-action on $\mathcal{X}(\Omega)$ as follows: for $g \in G$ signal $x \in \mathcal{X}(\Omega)$, the action $(g, x) \mapsto \mathbf{g}.x \in \mathcal{X}(\Omega)$  is defined pointwise at each $u \in \Omega$:
$$
(\mathbf{g}.x)(u) := x(g.\omega),
$$
where the bold $(\mathbf{g}.)$ is used to distinguish the action on signals from the action on the domain.</p>
<hr>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To summarize: any $G$-action on the domain $\Omega$ induces an $\mathbb{R}$-linear representation of $G$ over the vector space of signals on $\Omega$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<hr>
<h3 id="$\quad$-_Example:-One-hot-encoding_">
<a class="anchor" href="#%24%5Cquad%24-_Example:-One-hot-encoding_" aria-hidden="true"><span class="octicon octicon-link"></span></a>$\quad$ <font color="teal">_Example: One-hot encoding_</font><a class="anchor-link" href="#%24%5Cquad%24-_Example:-One-hot-encoding_"> </a>
</h3>
<p>$\quad$  It seems like standard practice to encode the collection of classes associated to some ML classification problem as an orthonormal basis. These are given ( to the computer ) in the usual coordinate basis 
$$
e_1 \equiv (1, 0, \dots, 0),\, e_2 \equiv (0,1,\dots, 0),\, \dots, \, e_n \equiv (0,\dots, 0,1) \,,
$$
hence the nomenclature <font color="purple">_one-hot_</font>. In the preceding example, if one considers a one-hot encoding of the vertices of $\mathbb{T}_n^2$, we see that each signal is expressed with respect to this coordinate system, in the sense that $x = \sum_{j=1}^n x_j e_j$.</p>
<p>This kind of encoding is useful for considering general symmetries of the domain. For instance, if permuting node labels is a relevant symmetry, the action of the symmetric group $\frak{S}_n$ is naturally represented by $n \times n$ permutation matrices.</p>
<hr>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The following definition reformulates the notion of a signal.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>Def</strong> $\quad$ A graph $\mathcal{G} = ( \mathcal{V}, \mathcal{E} )$ is said to be equipped with <font color="purple">_node features_</font> if for each $v  \in \mathcal{V}$, one has the additional data of an $s$-dimensional vector $x(v) \in \mathbb{R}^s$, called the <em>features</em> of node $v$.</p>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The term 'features' is compatible with the usage in ML, supposing that our input signal has domain some graph $\mathcal{G}$. In this case, we can think of a neural network as a sequence of node-layers built on top of the graph $\mathcal{G}$.</p>
<p>An input signal endows the first node layer of a NN with features, and the weights of the neural network propagate these through to node features on the nodes of the rest of the network. The features on the last layer of the network can be read off as the output of the NN function.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="1.2-$\quad$-Equivariance">
<a class="anchor" href="#1.2-%24%5Cquad%24-Equivariance" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.2 $\quad$ Equivariance<a class="anchor-link" href="#1.2-%24%5Cquad%24-Equivariance"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>References</em></p>
<ul>
<li>
<p><a href="https://arxiv.org/abs/2104.13478">Bronstein--Bruna--Cohen--Velicovic 2021</a>, <em>Geometric deep learning</em></p>
</li>
<li>
<p><a href="https://arxiv.org/abs/1812.09902">Maron--Ben-Hamu--Shamir--Lipman 2018</a>, <em>Invariant and equivariant graph networks</em></p>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We henceforth consider groups $G$ acting on some space of signals $\mathcal{X}(\Omega)$ through an action on some fixed signal domain $\Omega$.</p>
<p>The group action is encoded in a linear representation $\rho$, assumed to be described in a given input coordinate system, just as we would need to specify to a computer. Thus, if $\dim ( \mathcal{X} ) = n$, for each $g \in G$, the object $\rho(g)$ is an $n \times n$ matrix with real entries.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The learning dynamics occur in the <font color="purple">_hypothesis space_</font> $\textsf{H}$, a collection of functions 
$$
\textsf{H} \subset \{\, F : \mathcal{X}(\Omega) \to \mathcal{Y}\, \} ,
$$
where $\mathcal{Y}$ is a vector space of output channels. The point of the learning blueprint is to be able to choose $\textsf{H}$ compatible with the symmetries of input data. The blueprint describes $\textsf{H}$ explicitly, up to hyperparameters such as depth and layer widths.</p>
<p>A key aspect of this blueprint is that $F \in \textsf{H}$ should be expressed as a composition of functions, most of which are $G$-equivariant.</p>
<p>The requirement of $G$-equivariance constitutes a geometric prior. From this prior, one can derive the architecture of a generic CNN when $G$ corresponds to translations, and a family of generalizations for other domains and group actions.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>Def</strong> $\quad$ Let $\rho$ be a representation of group $G$ over $\mathcal{X}(\Omega)$, and let $\rho'$ be a representation of the same group over the $\mathbb{R}$-vector space $\mathcal{Y}$. A function $F: \mathcal{X}(\Omega) \to \mathcal{Y}$ is <font color="purple">_$G$-equivariant_</font> if for all $g \in G$ and all $x \in \mathcal{X}(\Omega)$, we have $\rho'(g) F(x) = F ( \rho(g) x )$.</p>
<p>We say $F$ is <font color="purple">_$G$-invariant_</font> if this holds when $\rho'$ is the trivial representation, which is to say $F ( \rho(g) x) = F(x)$ for all $g \in G$ and $x \in \mathcal{X}(\Omega)$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<hr>
<h3 id="$\quad$-_Example:-permutation-invariance-and-equivariance-in-data_-">
<a class="anchor" href="#%24%5Cquad%24-_Example:-permutation-invariance-and-equivariance-in-data_-" aria-hidden="true"><span class="octicon octicon-link"></span></a>$\quad$ <font color="teal">_Example: permutation invariance and equivariance in data_ </font><a class="anchor-link" href="#%24%5Cquad%24-_Example:-permutation-invariance-and-equivariance-in-data_-"> </a>
</h3>
<p>$\quad$ Suppose we are given either a set $\mathcal{V}$, or more generally a graph $\mathcal{G} = ( \mathcal{V}, \mathcal{E} )$, with $\# \mathcal{V} = n$ in either case. As discussed, a signal over $\mathcal{V} = \{ v_1, \dots, v_n \}$ can be thought of as a collection of node features $\{ \, x(v_1), \dots, x(v_n) \, \}$, with $x(v_j) \in \mathbb{R}^s$.</p>
<p>Let us stack the node features as rows of the $n \times s$ <font color="purple">_design matrix_</font></p>
$$
\mathbf{X} = 
\left[ 
\begin{matrix}
x_1\\ 
\vdots\\
x_n
\end{matrix}
\right] ,
$$<p>which is effectively the same object as signal $x$, provided the vertices are labeled as described. The action of $g \in \mathfrak{S}_n$ on this input data is naturally represented as an $n \times n$ permutation matrix, $\mathbf{P} \equiv \rho(g)$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>One standard way to construct a permutation invariant function $F$ in this setting is through the following recipe: a function $\psi$ is independently applied to every node's features, and $\varphi$ is applied on its sum-aggregated outputs.
$$
    F( \mathbf{X}) = \varphi \left( \, \sum_{j \, = \, 1}^n \psi(x_j) \, \right) .
$$ 
Such a function can be thought of as reporting some "global statistic" of signal $x$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Equivariance manifests even more naturally. Suppose we want to apply a function $F$ to the signal to "update" the node features to a set of <font color="purple">_latent_</font> node features.</p>
<p>This is the case in which the NN outputs an image segmentation mask; the underlying domain does not change, but the features at each node are updated to the extent that they may not even agree on the number of channels.</p>
<p>We can stack these latent features into another design matrix, $\mathbf{H} = F(\mathbf{X})$. The order of the rows of $\mathbf{H}$ should clearly be tied to the order of the rows of $\mathbf{X}$, i.e. permutation equivariant: for any permutation matrix $\mathbf{P}$, it holds that $F(\mathbf{P} \mathbf{X} ) = \mathbf{P} F(\mathbf{X})$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As a concrete example of a permutation equivariant function, consider a weight matrix $\theta \in \mathbb{R}^{s \, \times \, s'}$. This matrix can be used to map a length-$s$ feature vector at a given node to some new, updated feature vector with $s'$ channels.</p>
<p>Applying this matrix to each row of the design matrix is an example of a <font color="purple">_shared node-wise linear transform_</font>, and constitutes a linear, $\mathfrak{S}_n$-equivariant map. Note that, if one wishes to express this map in coordinates, it seems the correct object to consider is a $3$-tensor constructed as a stack of $n$ copies of $\theta$.</p>
<hr>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The above example considered signals over the nodes of a graph only, thus label permutation symmetry applies equally well, regardless of the graph structure ( or lack of structure ) underlying such functions.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the case that signals $x$ have a domain with graph structure, encoded by adjacency matrix $\mathbf{A}$, it feels right to work with a hypothesis space recognizing this structure.</p>
<p>This is to say that we wish to consider functions $F \in \textsf{H}$ with $F \equiv F( \mathbf{X}, \mathbf{A} )$. Such a function is (label) <font color="purple">_permutation invariant_</font> if $F( \mathbf{P} \mathbf{X},\, \mathbf{P} \mathbf{A} \mathbf{P}^{\textrm{T}} ) = F ( \mathbf{X}, \mathbf{A})$, and is <font color="purple">_permutation equivariant_</font> if
$$
F( \mathbf{P} \mathbf{X}, \mathbf{P} \mathbf{A} \mathbf{P}^T ) = \mathbf{P}F( \mathbf{X}, \mathbf{A})
$$
for any permutation matrix $\mathbf{P}$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>Rmk</strong> $\quad$ One can characterize linear $\mathfrak{S}_n$-equivariant functions on nodes.</p>
<blockquote>
<p><a href="https://arxiv.org/abs/2104.13478">BBCV 21</a> : $\quad$ "One can verify any such map can be written as a linear combination of two generators, the identity and the average. As observed by <a href="https://arxiv.org/abs/1812.09902">Maron et al. 2018</a>, any linear permutation equivariant $\mathbf{F}$ can be expressed as a linear combination of fifteen linear generators; remarkably, this family is independent of $n \equiv \# \mathcal{V}$."</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p>Amongst the generators discussed just above, the geometric learning blueprint "specifically advocates" for those that are also local, in the sense that the output on node $u$ directly depends on its neighboring nodes in the graph.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can formalize this constraint explicitly, by defining the <font color="purple">_$1$-hop neighborhood_</font> of node $u$ as
$$
\mathcal{N}(u) := \{ v : \{ u,v \} \in \mathcal{E} \} ,
$$
as well as the corresponding <font color="purple">_neighborhood features_</font>, 
$$
\mathbf{X}_{\mathcal{N}(u)} := \{ \!\{ \, x_v : v \in \mathcal{N}(u) \, \} \!\} ,
$$
which is a multiset, as indicated by double-brackets, as distinct nodes may be decorated with the same feature vector.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<hr>
<h3 id="$\quad$-_Example:-permutations-and-local-averaging_">
<a class="anchor" href="#%24%5Cquad%24-_Example:-permutations-and-local-averaging_" aria-hidden="true"><span class="octicon octicon-link"></span></a>$\quad$ <font color="teal">_Example: permutations and local averaging_</font><a class="anchor-link" href="#%24%5Cquad%24-_Example:-permutations-and-local-averaging_"> </a>
</h3>
<p>$\quad$ The node-wise linear transformation described in the previous example can be thought of as operating on $0$-hop neighborhoods of nodes. We generalize this with an example of a function operating on $1$-hop neighborhoods. Instead of a fixed map between feature spaces $\theta : \mathbb{R}^s \to \mathbb{R}^{s'}$, cloned to a pointwise map $\Theta$, we instead specify a <em>local</em> function 
$$
\varphi \equiv \varphi( \, x_u, \, \mathbf{X}_{\mathcal{N}(u)} \, )
$$</p>
<p>operating on the features of a node as well as those of its $1$-hop neighborhood.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We may construct a permutation equivariant function $\Phi$ by applying $\varphi$ to each $1$-hop neighborhood in isolation, and then collecting these into a new feature matrix.</p>
<p>As before, for $\mathcal{V} = \{ v_1, \dots, v_n \}$, we write $x_j$ in place of $x_{v(j)}$, and we similarly write $\mathcal{N}_j$ instead of $\mathcal{N}( v(j) )$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
\Phi ( \mathbf{X}, \mathbf{A} ) = 
\left[
\begin{matrix}
\varphi( \, x_1 , \, \mathbf{X}_{\mathcal{N}_1} \, ) \\
\varphi( \, x_2 , \, \mathbf{X}_{\mathcal{N}_2} \, ) \\
\vdots \\
\varphi( \, x_n , \, \mathbf{X}_{\mathcal{N}_n} \, )
\end{matrix}
\right]
$$<p>The permutation equivariance of $\Phi$ rests on the output of $\varphi$ being independent of the ordering of the nodes in $\mathcal{N}_u$. Thus, if $\varphi$ is permutation invariant ( a local averaging ) this property is satisfied.</p>
<hr>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p><a href="https://arxiv.org/abs/2104.13478">BBCV 21</a> : $\quad$ "The choice of $\varphi$ plays a crucial role in the expressive power of the learning scheme."</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>When considering signals $x \equiv \mathbf{X}$ over graphs, it is natural to consider a hypothesis space whose functions operate on the pair $( \mathbf{X}, \mathbf{A})$, where $\mathbf{A}$ is the adjacency matrix of the signal domain.</p>
<p>Thus, for such signals the domain naturally becomes part of the input.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The GDL blueprint distinguishes between two contexts:</p>
<ol>
<li>
<p>one in which the input domain is fixed, and</p>
</li>
<li>
<p>another in which the input domain varies from signal to signal.</p>
</li>
</ol>
<p>The preceding example demonstrates that, even in the former context, it can be essential that elements of $\textsf{H}$ treat the (fixed) domain as an argument.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>When the signal domain has geometric structure, it can often be leveraged to construct a <font color="purple">_coarsening operator_</font>, one of the components of a GDL block in the learning blueprint. Such an operator passes a signal $x \in \mathcal{X}(\Omega)$ to a signal $y \in \mathcal{X}( \Omega')$, where $\Omega'$ is a coarse-grained version of $\Omega$.</p>
<p>The domain may be fixed for each input, but this domain changes as the signal passes through the layers of the NN, and it is natural that the functions the NN is built out of should pass this data forward.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p><a href="https://arxiv.org/abs/2104.13478">BBCV 21</a> : $\quad$ "Due to their additional structure, graphs and grids, unlike sets, can be coarsened in a non-trivial way, giving rise to a variety of pooling operations... more precisely, we cannot define a non-trivial coarsening assuming set structure alone. There exist established approaches that infer topological structure from unordered sets, and those can admit non-trivial coarsening"</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="1.3-$\quad$-Equivariance-in-neural-networks">
<a class="anchor" href="#1.3-%24%5Cquad%24-Equivariance-in-neural-networks" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.3 $\quad$ Equivariance in neural networks<a class="anchor-link" href="#1.3-%24%5Cquad%24-Equivariance-in-neural-networks"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>References</em></p>
<ul>
<li><a href="https://arxiv.org/abs/1802.03690">Kondor-Trivedi 2018</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="$\quad\quad$-_Neural-networks_">
<a class="anchor" href="#%24%5Cquad%5Cquad%24-_Neural-networks_" aria-hidden="true"><span class="octicon octicon-link"></span></a>$\quad\quad$ <font color="teal">_Neural networks_</font><a class="anchor-link" href="#%24%5Cquad%5Cquad%24-_Neural-networks_"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The building blocks of neural networks are tensor objects. It is convenient for now to present a definition of a neural network without introducing the notion of a tensor.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>Def</strong> $\quad$ A <font color="purple">_feed-forward neural network_</font> 
$$
(\textsf{N}, \textsf{S}, b, w, a )  
$$ 
is a directed graph $(\textsf{N},\textsf{S})$, called the <font color="purple">_computation graph_</font>, whose vertices $\textsf{N}$ and edges $\textsf{S}$ are equipped with weights, respectively $b$ and $w$, along with an <font color="purple">_activation function_</font> $a$. For simplicity, we assume a single activation function $a$ is used across the network.  The objects should satisfy the following:</p>
<ul>
<li>
<p>The vertices $\textsf{N}$ of the computation graph, also called <font color="purple">_neurons_</font>, are partitioned into $L+1$ distinct <font color="purple">_layers_</font>, for some $L \in \mathbb{N}$. The collection of vertices in layer $\ell$ is denoted $\textsf{N}_\ell$. Layer $\textsf{N}_0$ is called the <font color="purple">_input layer_</font>, while $\textsf{N}_L$ is the <font color="purple">_output layer_</font>.</p>
</li>
<li>
<p>We write $\mathfrak{n}_i^\ell$ for the $i$th neuron in layer $\ell$. Each neuron is equipped with a <font color="purple">_bias_</font> parameter, $b_i^\ell$. The vector of biases for neurons in a given layer $\ell$ is denoted $b^\ell$, and the collection of all bias parameters across all layers is denoted $b$.</p>
</li>
<li>
<p>The edges $\textsf{S}$ of the computation graph, called <font color="purple">_synapses_</font>, only join neurons in consecutive layers, and importantly, the orientation of each edge always points towards the larger layer. This is why the network is "feed-forward."</p>
</li>
<li>
<p>This structure effectively partitions the synapses into layers, indexed by the largest layer among the two endpoints of the edges. We write $\textsf{S}_\ell$ for the $\ell$th layer of synapses, though we could also write $\textsf{S}_{\ell-1, \ell}$ to emphasize that the edges in $\textsf{S}_\ell$ join neurons in $\textsf{N}_{\ell-1}$ to those in $\textsf{N}_\ell$. The synapse in $\textsf{S}_\ell$ joining neuron $\mathfrak{n}_i^{\ell-1}$ to $\mathfrak{n}_j^\ell$ is denoted $\mathfrak{s}_{ij}^{\ell}$.</p>
</li>
<li>
<p>Each synapse $\mathfrak{s}_{ij}^{\ell}$ is equipped with a <font color="purple">_weight_</font> parameter, $w_{ij}^{\ell}$. The collection of weights associated to neurons in layer $\ell$ is denoted $w^\ell$, and the collection of all weights across all layers, $w$.</p>
</li>
<li>
<p>For all layers but the input layer, and given an input vector $x$, we also denote the <font color="purple">_activation at layer $\ell$_</font> and the <font color="purple">_activation at neuron $\mathfrak{n}_j^\ell$_</font> by $a^{\ell}$ and by $a_j^\ell$ respectively. The activations are defined inductively by
$$
a_j^\ell = a \left( \left( \sum_{i} w_{ij} a_i^{\ell-1} \right) + b_j^{\ell}\right) , 
$$
with the activation at the $0$th layer, $a^0$ is defined as some input signal $x$, and where the biases at the input layer are always set to $0$.</p>
</li>
</ul>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The parameter $L$, controlling the number of layers, is the <font color="purple">_depth_</font> of the network, while the <font color="purple">_width of layer $\ell$_</font> is the number of neurons in the layer.</p>
<p>We will often abuse notation slightly, and write $\textsf{N}$ in place of the quintuple $(\textsf{N}, \textsf{S}, b, w, a )$. Each feed-forward neural network $\textsf{N}$ of depth $L$ can be associated to a function $F_{\textsf{N}} \equiv a^L(\textsf{N})$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="1.4-$\quad$-Tensors">
<a class="anchor" href="#1.4-%24%5Cquad%24-Tensors" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.4 $\quad$ Tensors<a class="anchor-link" href="#1.4-%24%5Cquad%24-Tensors"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Consider a pair of orthonormal bases, the 'original basis' $(e_1, \dots, e_n)$ and the 'new basis' $(\tilde{e}_1, \dots, \tilde{e}_n )$. A signal $x \in \mathbb{R}^n$ can be expressed in either coordinate system</p>
\begin{align}
x &amp;= e_1 x^1 + \dots + e_nx^n\, \equiv \,e_jx^j \\
x &amp;= \tilde{e}_1 \tilde{x}^1 + \dots + \tilde{e}_n \tilde{x}^n\, \equiv\, \tilde{e}_j  \tilde{x}^j ,
\end{align}<p>where we have used the Einstein summation convention. We adopt this notation going forward.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can express the basis $( \tilde{e}_j )$ in terms of the $( e_j)$ via
$$
\tilde{e}_j = e_i \mathbf{S}^i_j \, ,
$$
where $\mathbf{S}$ is called the <em>direct transformation matrix</em> from original basis to the new. We let $\mathbf{T}$ denote $\mathbf{S}^{-1}$, and observe that the coordinate coefficients transform \emph{contravariantly}:
$$
\tilde{x}^j =  \mathbf{T}_i^j x^i\,.
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We assume that the underlying vector space is $\mathbb{R}^n$, and moreover, we equip it with the usual inner product ( in order to make sense of orthogonality ), and note that the Riesz representation theorem allows us to assign a given orthonormal basis $(e_j)$ of vector space $V$ to a canonical dual basis $(e^j)$ on the dual vector space $V^*$. Elements of $V^*$ are called dual vectors, or covectors. This canonical dual basis is given by</p>
\begin{align}
e^j &amp;= \langle e_j, \cdot \rangle \\
&amp;\equiv e_i  \, \delta^{ij} 
\end{align}<p>and thus,
$$
e_j = \delta_{ij} \, e^i. 
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The dual basis can be thought of as a `coordinate selector,' acting on a vector $x \in V$ by</p>
\begin{align}
e^j(x) &amp;= e^j \, e_k \, x^k \\
&amp;=x^k \, e^j \,e_k  \\
&amp;= \delta_k^j  \, x^k \\
&amp;= x^j.
\end{align}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let us consider how the objects 
$$
\delta_{ij} \, \equiv \, e_i \, e_j ,\quad  \delta_i^j  \, \equiv\, e_i \, e^k ,\quad  \delta^{ij} \,\equiv \, e^i \, e^j
$$
transform with the bases, i.e. how to express the following new basis objects
$$
\tilde{\delta}_{ij}, \quad \, \tilde{\delta}_i^j,  \quad \, \tilde{\delta}^{ij}
$$
in terms of the corresponding old, via the matrices $\mathbf{S}$ and $\mathbf{T}$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>One has</p>
\begin{align}
\tilde{\delta}_{ij} 
&amp;\equiv \tilde{e}_i\, \tilde{e}_j \\
&amp;= e_{\ell}\, \mathbf{S}^\ell_i  \, e_k  \,\mathbf{S}^k_j \\
&amp;=  \mathbf{S}^\ell_i \, \delta_{\ell k} \, \mathbf{S}^k_j 
\end{align}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Likewise, one has
$$
\tilde{\delta}_i^j \, = \, \mathbf{S}_i^\ell \, \delta_\ell^k \, \mathbf{T}_k^j \quad \text{ and } \quad \tilde{\delta}^{ij} \, = \, \mathbf{T}_\ell^i \, \delta^{\ell k } \, \mathbf{T}_k^j
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Given a change of basis $(e_j) \mapsto (\tilde{e}_j)$, the induced transformation $(e^j) \mapsto (\tilde{e}^j)$ is contravariant, which is consistent with the transformations described above:</p>
\begin{align}
\tilde{e}^j  
&amp;\equiv \tilde{e}_i \, \delta^{ij} \\
&amp;= e_\ell \, \mathbf{S}_i^\ell \, \mathbf{T}_\ell^i \, \delta^{\ell k} \, \mathbf{T}_k^j \\
&amp;= e_\ell \,  \delta^{\ell k } \, \mathbf{T}_k^{j} \\
&amp;=  \mathbf{T}_k^j e^k\,
\end{align}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>Def</strong> $\quad$  A <em>tensor of type</em>, or <em>valency</em> $(r,s)$ over vector space $V$ is a multilinear map 
$$
\mathbf{A} : \underbrace{V^* \times \, \dots \, \times V^*}_{ k \text{ copies }}  \,\times \, \underbrace{ V \times \, \dots \, \times V }_{ \ell \text{ copies }} \to \mathbb{R}
$$</p>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>When a basis $(e_j)$ is given, we can express $\mathbf{A}$ with respect to this coordinate system through a total of $n^{r+s}$ scalars, denoted generically by $\mathbf{A}_{ \quad j_1, \, \dots\, , \, j_s} ^ { i_1, \, \dots \, , \, i_r} $. The coordinate indices $i_1, \dots, i_r$ are the <em>contravariant indices</em>, while the $j_1, \dots, j_s$ are the <em>covariant indices</em>. They are so-named because of the transformation law of the entries under a change of basis $(e_j ) \mapsto (\tilde{e}_j)$ induces the linear transformation</p>
$$
\tilde{\mathbf{A}} _{ \quad j_1, \, \dots\, , \, j_s} ^ { i_1, \, \dots \, , \, i_r} 
=
\mathbf{T}_{k_1}^{i_1} \, \dots \, \mathbf{T}_{k_r}^{i_r} \, \mathbf{A} _{ \quad \ell_1, \, \dots\, , \, \ell_s} ^ { k_1, \, \dots \, , \, k_r} \, \mathbf{S}_{j_1}^{\ell_1}  \, \dots \, \mathbf{S}_{j_s}^{\ell_s} 
$$<p>Let $\mathcal{T}(r,s)$ denote the vector space of type-$(r,s)$ tensors over $V$. The dimension of this vector space is $n^{r+s}$, i.e. the number of scalar entries in the coordinate representation of such a tensor.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There are two operations on tensors of interest</p>
<ul>
<li>contraction</li>
<li>outer product</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="$1.5$-$\quad$-GDL-blueprint-(with-fixed-input-domain)">
<a class="anchor" href="#%241.5%24-%24%5Cquad%24-GDL-blueprint-(with-fixed-input-domain)" aria-hidden="true"><span class="octicon octicon-link"></span></a>$1.5$ $\quad$ GDL blueprint (with fixed input domain)<a class="anchor-link" href="#%241.5%24-%24%5Cquad%24-GDL-blueprint-(with-fixed-input-domain)"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="$\quad$-Setup">
<a class="anchor" href="#%24%5Cquad%24-Setup" aria-hidden="true"><span class="octicon octicon-link"></span></a>$\quad$ <em>Setup</em><a class="anchor-link" href="#%24%5Cquad%24-Setup"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our formal treatment of a classification problem requires the following objects:</p>
<ul>
<li>
<p>$\Omega$ : A graph with $n$ vertices. This is the domain of the signals considered.</p>
</li>
<li>
<p>$\mathcal{X}(\Omega,\mathbb{R}^s)$ :  The space of $s$-channel signals $x : \Omega \to \mathbb{R}^s$.</p>
</li>
<li>
<p>$(e_j)_{j\,=\,1}^{\,n}$ : A 'spatial' coordinate system for scalar-valued signals $\mathcal{X} \to \mathbb{R}$.</p>
</li>
<li>
<p>$(z_c)_{c \,=\,1}^{\,s}$ : A channel coordinate system.</p>
</li>
<li>
<p>$G$ : A group $G$ acting on $\Omega$, via $\Omega \ni u \mapsto g.u \in \Omega$. This induces the `pointwise' action on the space of signals $\mathcal{X}(\Omega,\mathbb{R}^s)$, denoted $x \mapsto \mathbf{g}.x$.</p>
</li>
<li>
<p>$\rho$ : The representation of the induced action of $G$, $x \mapsto \mathbf{g}.x$.</p>
</li>
</ul>
<p>The basis 
$$
\{ e_j \otimes z_c : 1 \leq j \leq n,\, 1 \leq c \leq s  \}
$$</p>
<p>is the one in which signals are expressed to a computer. Given $x \in \mathcal{X}(\Omega,\mathbb{R}^s)$. In terms of this basis, one can write $x$ as
$$
x = \mathbf{X}^{cj} e_j \otimes z_c ,
$$
Note that $\mathbf{X}$ is the so-called design matrix previously introduced.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>Let $g \in G$. In the basis $e_j \otimes z_c$, we can then identify $\rho(g)$ with a tensor... <em>(say more)</em></p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="$\quad$-coarsening-operator">
<a class="anchor" href="#%24%5Cquad%24-coarsening-operator" aria-hidden="true"><span class="octicon octicon-link"></span></a>$\quad$ <em>coarsening operator</em><a class="anchor-link" href="#%24%5Cquad%24-coarsening-operator"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let $\Omega$ and $\Omega'$ be domains, $G$ a symmetry group over $\Omega$, and write $\Omega' \prec \Omega$ if $\Omega'$ arises from $\Omega$ through some coarsening operator (presumably this coarsening operator needs to commute with the group action).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="$\quad$-GDL-block">
<a class="anchor" href="#%24%5Cquad%24-GDL-block" aria-hidden="true"><span class="octicon octicon-link"></span></a>$\quad$ <em>GDL block</em><a class="anchor-link" href="#%24%5Cquad%24-GDL-block"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ol>
<li><em>linear $G$-equivariant layer</em></li>
</ol>
$$
B : \mathcal{X}(\Omega, \mathcal{C}) \to \mathcal{X}(\Omega' , \mathcal{C}')
$$<p>
such that $B(g.x) = g.B(x)$ for all $g \in G$ and $x \in \mathcal{X}(\Omega, \mathcal{C})$.</p>
<ol>
<li>
<p><em>nonlinearity</em>, or <em>activation function</em> 
$$
a : \mathcal{C} \to \mathcal{C}'
$$ 
applied domain-pointwise as $(\mathbf{a}(x))(u) = a(x(u))$.</p>
</li>
<li>
<p><em>local pooling operator</em> or <em>coarsening operator</em> 
$$
P : \mathcal{X}(\Omega, \mathcal{C}) \to \mathcal{X}(\Omega', \mathcal{C})
$$ 
which gives us our notion $\Omega' \prec \Omega$.</p>
</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The last ingredient is a global pooling layer applied last, compositionally.</p>
<ol>
<li>
<em>$G$-invariant layer</em>, or <em>global pooling layer</em>
$$
A : \mathcal{X} (\Omega, \mathcal{C}) \to \mathcal{Y}
$$ 
satisfying $A(g.x) = A(x)$ for all $g \in G$ and $x \in \mathcal{X}(\Omega, \mathcal{C})$. </li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="$\quad$-Hypothesis-space">
<a class="anchor" href="#%24%5Cquad%24-Hypothesis-space" aria-hidden="true"><span class="octicon octicon-link"></span></a>$\quad$ <em>Hypothesis space</em><a class="anchor-link" href="#%24%5Cquad%24-Hypothesis-space"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>These objects can be used to define a class of $G$-invariant functions $f: \mathcal{X}(\Omega, \mathcal{C}) \to \mathcal{Y}$ of the form
$$
f = A \circ \mathbf{a}_J \circ B_J \circ P_{J-1} \circ \dots \circ P_1 \circ \mathbf{a}_1 \circ B_1 ,
$$
where the blocks are selected so that the output space of each block matches the input space of the next one. Different blocks may exploit different choices of symmetry groups $G$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="$\quad$-Discussion">
<a class="anchor" href="#%24%5Cquad%24-Discussion" aria-hidden="true"><span class="octicon octicon-link"></span></a>$\quad$ <em>Discussion</em><a class="anchor-link" href="#%24%5Cquad%24-Discussion"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Shift-invariance arises naturally in vision and pattern recognition. In this case, the desired function $f \in \textsf{H}$, typically implemented as a CNN, inputs an image and outputs the probability of the image to contain an object from a certain class. It is often reasonably assumed that the classification result should not be affected by the position of the object in the image, i.e., the function $f$ must be shift-invariant.</p>
<p>Multi-layer perceptrons lack this property, a reason why early (1970s) attempts to apply these architectures to pattern recognition problems failed. The development of NN architectures with local weight sharing, as epitomized by CNNs, was among other reasons motivated by the need for shift-invariant object classification.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A prototypical application requiring shift-equivariance is image segmentation, where the output of $f$ is a pixel-wise image mask. This segmentation mask must follow shifts in the input image. In this example, the domains of the input and output are the same, but since the input has three color channels while the output has \emph{one channel per class}, the representations $(\rho, \mathcal{X}(\Omega, \mathcal{C}) )$ and $(\rho', \mathcal{Y} \equiv \mathcal{X}(\Omega, \mathcal{C}'))$ are somewhat different.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>When $f$ is implemented as a CNN, it may be written as a composition of $L$ functions, where $L$ is determined by the depth and other hyperparameters:
$$
f = f_L \circ f_{L-1} \circ \dots \circ f_2 \circ f_1 .
$$</p>
<p>Examining the individual layer functions making up CNN, one finds they are not shift-invariant in general but rather shift-equivariant. The last function applied, namely $f_L$, is typically a ``global-pooling" function that is genuinely shift-invariant, causing $f$ to be shift-invariant, but to focus on this ignores the structure we will leverage for purposes of expressivity and regularity.</p>

</div>
</div>
</div>
</div>



  </div><a class="u-url" href="/geometric-deep-learning/jupyter/2021/05/01/GDL1.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/geometric-deep-learning/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/geometric-deep-learning/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/geometric-deep-learning/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>...</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" title="fastai"><svg class="svg-icon grey"><use xlink:href="/geometric-deep-learning/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/geometric-deep-learning/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
