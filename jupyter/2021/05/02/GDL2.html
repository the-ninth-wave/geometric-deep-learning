<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>2 | geometric deep learning</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="2" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="GDL" />
<meta property="og:description" content="GDL" />
<link rel="canonical" href="https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2021/05/02/GDL2.html" />
<meta property="og:url" content="https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2021/05/02/GDL2.html" />
<meta property="og:site_name" content="geometric deep learning" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-05-02T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"GDL","url":"https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2021/05/02/GDL2.html","@type":"BlogPosting","headline":"2","dateModified":"2021-05-02T00:00:00-05:00","datePublished":"2021-05-02T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://the-ninth-wave.github.io/geometric-deep-learning/jupyter/2021/05/02/GDL2.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/geometric-deep-learning/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://the-ninth-wave.github.io/geometric-deep-learning/feed.xml" title="geometric deep learning" /><link rel="shortcut icon" type="image/x-icon" href="/geometric-deep-learning/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/geometric-deep-learning/">geometric deep learning</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/geometric-deep-learning/about/">About Me</a><a class="page-link" href="/geometric-deep-learning/search/">Search</a><a class="page-link" href="/geometric-deep-learning/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">2</h1><p class="page-description">GDL</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-05-02T00:00:00-05:00" itemprop="datePublished">
        May 2, 2021
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      8 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/geometric-deep-learning/categories/#jupyter">jupyter</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/the-ninth-wave/geometric-deep-learning/tree/master/_notebooks/2021-05-02-GDL2.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/geometric-deep-learning/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/the-ninth-wave/geometric-deep-learning/master?filepath=_notebooks%2F2021-05-02-GDL2.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/geometric-deep-learning/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/the-ninth-wave/geometric-deep-learning/blob/master/_notebooks/2021-05-02-GDL2.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/geometric-deep-learning/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#1.2-$\dots$-equivariance">1.2 $\dots$ equivariance </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-05-02-GDL2.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="1.2-$\dots$-equivariance">
<a class="anchor" href="#1.2-%24%5Cdots%24-equivariance" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>1.2 $\dots$ equivariance</strong><a class="anchor-link" href="#1.2-%24%5Cdots%24-equivariance"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We henceforth consider groups $G$ acting on some space of signals $\mathcal{X}(\Omega)$ over a (fixed) signal domain $\Omega$. The group action is encoded in a linear representation $\rho$, assumed to be described in a given <em>input coordinate system</em>, just as we would need to specify to a computer. Thus, if $\dim ( \mathcal{X}(\Omega) ) = n$, for each $g \in G$, we have that $\rho(g)$ is an $n \times n$ matrix with real entries.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The learning dynamics occur in the <em>hypothesis space</em> $\textsf{H}$, a collection of functions 
$$
\textsf{H} \subset \{\, F : \mathcal{X}(\Omega) \to \mathcal{Y}\, \} ,
$$
where $\mathcal{Y}$ is some unspecified (context-specific) space of output signals. We describe $\textsf{H}$ explicitly in the "learning blueprint", up to hyperparameters such as depth and layer widths.</p>
<p>A key aspect of this blueprint is that $F \in \textsf{H}$ should be expressed as a composition of functions, most of which are $G$-equivariant. The requirement of $G$-equivariance constitutes a <em>geometric prior</em> from which one can derive the architecture of a generic CNN when $G$ corresponds to translations, and a family of generalizations for other domains and group actions.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>Def</strong> $\quad$ Let $\rho$ be a representation of group $G$ over $\mathcal{X}(\Omega)$, and let $\rho'$ be a representation of the same group over the $\mathbb{R}$-vector space $\mathcal{Y}$. A function $F: \mathcal{X}(\Omega) \to \mathcal{Y}$ is <em>$G$-equivariant</em> if for all $g \in G$ and all $x \in \mathcal{X}(\Omega)$, we have $\rho'(g) F(x) = F ( \rho(g) x )$. We say $F$ is <em>$G$-invariant</em> if this holds when $\rho'$ is the trivial representation, which is to say $F ( \rho(g) x) = F(x)$ for all $g \in G$ and $x \in \mathcal{X}(\Omega)$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<hr>
<p><strong>Example</strong> $\quad$ Suppose we are given either a set $\mathcal{V}$, or more generally a graph $\mathcal{G} = ( \mathcal{V}, \mathcal{E} )$, with $\# \mathcal{V} = n$ in either case. As discussed, a signal over $\mathcal{V} = \{ v_1, \dots, v_n \}$ can be thought of as a collection of node features $\{ \, x(v_1), \dots, x(v_n) \, \}$, with $x(v_j) \in \mathbb{R}^s$. We stack the node features as rows of the $n \times s$ <em>design matrix</em></p>
$$
\mathbf{X} = 
\left[ 
\begin{matrix}
x_1\\ 
\vdots\\
x_n
\end{matrix}
\right] ,
$$<p>which is effectively the same object as signal $x$, provided the vertices are labeled as described. The action of $g \in \mathfrak{S}_n$ on this input data is naturally represented as an $n \times n$ permutation matrix, $\mathbf{P} \equiv \rho(g)$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>One standard way to construct a permutation invariant function $F$ in this setting is through the following recipe: a function $\psi$ is independently applied to every node's features, and $\varphi$ is applied on its sum-aggregated outputs.
$$
    F( \mathbf{X}) = \varphi \left( \, \sum_{j \, = \, 1}^n \psi(x_j) \, \right) .
$$ 
Such a function can be thought of as reporting some "global statistic" of signal $x$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Equivariance manifests even more naturally. Suppose we want to apply a function $F$ to the signal to "update" the node features, producing a set of <em>latent</em> (node) features.</p>
<p>This is the case in which the NN outputs an image segmentation mask; the underlying domain does not change, but the features at each node are updated to the extent that they may not even agree on the number of channels.</p>
<p>We can stack these latent features into another design matrix, $\mathbf{H} = F(\mathbf{X})$. The order of the rows of $\mathbf{H}$ should clearly be tied to the order of the rows of $\mathbf{X}$, i.e. permutation equivariant: for any permutation matrix $\mathbf{P}$, it holds that $F(\mathbf{P} \mathbf{X} ) = \mathbf{P} F(\mathbf{X})$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As a concrete example of a permutation equivariant function, consider a weight matrix $\theta \in \mathbb{R}^{s \, \times \, s'}$. This matrix can be used to map a length-$s$ feature vector at a given node to some new, updated feature vector with $s'$-channels.</p>
<p>Applying this matrix to each row of the design matrix is an example of a <em>shared node-wise linear transform</em>, and constitutes a linear, $\mathfrak{S}_n$-equivariant map. Note that, if one wishes to express this map in coordinates, it seems the correct object to consider is a $3$-tensor, "$\Theta,$" constructed as a stack of $n$ copies of $\theta$.</p>
<hr>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The above example considered signals over the nodes of a graph only, thus label permutation symmetry applies equally well, regardless of the graph structure ( or lack of structure ) underlying such functions.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the case that signals $x$ have a domain with graph structure, encoded by adjacency matrix $\mathbf{A}$, it feels right to work with a hypothesis space recognizing this structure.</p>
<p>This is to say that we wish to consider functions $F \in \textsf{H}$ with $F \equiv F( \mathbf{X}, \mathbf{A} )$. Such a function is (label) <em>permutation invariant</em> if $F( \mathbf{P} \mathbf{X},\, \mathbf{P} \mathbf{A} \mathbf{P}^{\textrm{T}} ) = F ( \mathbf{X}, \mathbf{A})$, and is <em>permutation equivariant</em> if
$$
F( \mathbf{P} \mathbf{X}, \mathbf{P} \mathbf{A} \mathbf{P}^T ) = \mathbf{P}F( \mathbf{X}, \mathbf{A})
$$
for any permutation matrix $\mathbf{P}$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>Rmk</strong> $\quad$ One can characterize linear $\mathfrak{S}_n$-equivariant functions on nodes. From [BBCV21]:</p>
<blockquote>
<p>"One can verify any such map can be written as a linear combination of two generators, the identity and the average. As observed by Maron et al. 2018, any linear permutation equivariant $\mathbf{F}$ can be expressed as a linear combination of fifteen linear generators; remarkably, this family is independent of $n \equiv \# \mathcal{V}$."</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>Rmk</strong> $\quad$ Amongst the generators just discussed, the geometric learning blueprint "specifically advocates" for those that are also local, in the sense that the output on node $u$ directly depends on its neighboring nodes in the graph.</p>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can formalize this constraint explicitly, by defining the <em>$1$-hop neighborhood</em> of node $u$ as
$$
\mathcal{N}(u) := \{ v : \{ u,v \} \in \mathcal{E} \} ,
$$
as well as the corresponding \emph{\B{neighborhood features}}, 
$$
\mathbf{X}_{\mathcal{N}(u)} := \{ \{ \, x_v : v \in \mathcal{N}(u) \, \} \} ,
$$
which is a multiset ( indicated by double-brackets ) for the simple reason that distinct nodes may be decorated with the same feature vector.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<hr>
<p><strong>Example</strong> $\quad$ The node-wise linear transformation described in the previous example can be thought of as operating on `$0$-hop neighborhoods' of nodes. We generalize this with an example of a function operating on $1$-hop neighborhoods. Instead of a fixed map between feature spaces $\theta : \mathbb{R}^s \to \mathbb{R}^{s'}$, cloned to a pointwise map $\Theta$, we instead specify a \emph{local} function 
$$
\varphi \equiv \varphi( \, x_u, \, \mathbf{X}_{\mathcal{N}(u)} \, )
$$</p>
<p>operating on the features of a node as well as those of its $1$-hop neighborhood.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We may construct a permutation equivariant function $\Phi$ ( the analogue of $\Theta$ here, just as $\varphi$ corresponds to $\theta$ ) by applying $\varphi$ to each $1$-hop neighborhood in isolation, and then collecting these into a new feature matrix. As before, for $\mathcal{V} = \{ v_1, \dots, v_n \}$, we write $x_j$ in place of $x_{v(j)}$, and we similarly write $\mathcal{N}_j$ instead of $\mathcal{N}( v(j) )$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
\Phi ( \mathbf{X}, \mathbf{A} ) = 
\left[
\begin{matrix}
\varphi( \, x_1 , \, \mathbf{X}_{\mathcal{N}_1} \, ) \\
\varphi( \, x_2 , \, \mathbf{X}_{\mathcal{N}_2} \, ) \\
\vdots \\
\varphi( \, x_n , \, \mathbf{X}_{\mathcal{N}_n} \, )
\end{matrix}
\right]
$$<p>The permutation equivariance of $\Phi$ rests on the output of $\varphi$ being independent of the ordering of the nodes in $\mathcal{N}_u$. Thus, if $\varphi$ is permutation invariant ( a local averaging ) this property is satisfied.</p>
<hr>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>[BBCV21] :"The choice of $\varphi$ plays a crucial role in the expressive power of the learning scheme."</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>When considering signals $x \equiv \mathbf{X}$ over graphs, it is natural to consider a hypothesis space whose functions operates on the pair $( \mathbf{X}, \mathbf{A})$, where $\mathbf{A}$ is the adjacency matrix of the signal domain. Thus, for such signals the domain naturally becomes part of the input.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The GDL blueprint distinguishes between two contexts: one in which the input domain is fixed, and another in which the input domain varies from signal to signal. The preceding example demonstrates that, even in the former context, it can be essential that elements of $\textsf{H}$ treat the (fixed) domain as an argument.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>When the signal domain has geometric structure, it can often be leveraged to construct a <em>coarsening operator</em>, one of the components of a GDL block in the learning blueprint. Such an operator passes a signal $x \in \mathcal{X}(\Omega)$ to a signal $y \in \mathcal{X}( \Omega')$, where $\Omega'$ is a `coarsened version' of $\Omega$.</p>
<p>As the blueprint calls for a sequence of such blocks, we often wish to act on the latent signal $y$ by a pointwise non-linearity, followed by the action of an equivariant function on signals in $\mathcal{X}( \Omega')$.</p>
<p>The domain may be fixed for each input, but this domain changes as the signal passes through the layers of the NN, and it is natural that the functions the NN is built out of should pass this data forward.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote>
<p>[BBCV21] "Due to their additional structure, graphs and grids, unlike sets, can be coarsened in a non-trivial way, giving rise to a variety of pooling operations... more precisely, we cannot define a non-trivial coarsening assuming set structure alone. There exist established approaches that infer topological structure from unordered sets, and those can admit non-trivial coarsening"</p>
</blockquote>

</div>
</div>
</div>
</div>



  </div><a class="u-url" href="/geometric-deep-learning/jupyter/2021/05/02/GDL2.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/geometric-deep-learning/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/geometric-deep-learning/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/geometric-deep-learning/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>...</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" title="fastai"><svg class="svg-icon grey"><use xlink:href="/geometric-deep-learning/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/geometric-deep-learning/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
